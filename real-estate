
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "%pylab inline\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sklearn as sk\n",
    "import sklearn.tree as tree\n",
    "from IPython.display import Image \n",
    "import pydotplus\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# evaluate RFE for regression\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.model_selection import KFold\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aynı beslenen kolonları tekilleştirir:\n",
    "def drop_duplicate_columns(train_x, dummy_columns_prefix='DMY_'):\n",
    "    \"\"\"\n",
    "\n",
    "    :param train_x:\n",
    "    :param test_x:\n",
    "    :param dummy_columns_prefix:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    desc = train_x[[col for col in train_x.columns if not col.startswith(dummy_columns_prefix)]] \\\n",
    "        .describe()\n",
    "    desc = desc.loc[['mean', 'std'], :]\n",
    "    blacklist = []\n",
    "    duplicated_feat = {}\n",
    "    for i in range(0, len(desc.columns)):\n",
    "        if i % 250 == 0:  # this helps me understand how the loop is going\n",
    "            print(i)\n",
    "\n",
    "        col_1 = desc.columns[i]\n",
    "        if col_1 in blacklist:\n",
    "            continue\n",
    "\n",
    "        for col_2 in desc.columns[i + 1:]:\n",
    "            if desc[col_1].equals(desc[col_2]):\n",
    "                if col_1 not in duplicated_feat:\n",
    "                    duplicated_feat[col_1] = []\n",
    "                duplicated_feat[col_1].append(col_2)\n",
    "\n",
    "        blacklist = []\n",
    "        for sublist in [x for x in duplicated_feat.values()]:\n",
    "            for item in sublist:\n",
    "                blacklist.append(item)\n",
    "\n",
    "    train_x.drop(blacklist, axis=1, inplace=True)\n",
    "    #test_x.drop(blacklist, axis=1, inplace=True)\n",
    "    return blacklist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#değişkenler arasındaki korelasyona bakar, %80 ve üzerini eler, \n",
    "#hangisinin eleneceği tek değişkenli random forest ile belirlenir importance'ı düşük olan elenir:\n",
    "\n",
    "def eliminate_group_correlation(X_train, Y_train, corrmat=None, threshold=0.5, seed=100): #%80 de baklılabilir\n",
    "    \"\"\"\n",
    "\n",
    "    :param X_train:\n",
    "    :param X_test:\n",
    "    :param Y_train:\n",
    "    :param target_column:\n",
    "    :param corrmat:\n",
    "    :param threshold:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if corrmat is None:\n",
    "        corrmat = X_train.corr()\n",
    "    corrmat = corrmat.abs().unstack()  # absolute value of corr coef\n",
    "    corrmat = corrmat.sort_values(ascending=False)\n",
    "    corrmat = corrmat[corrmat >= threshold]\n",
    "    corrmat = corrmat[corrmat < 1]\n",
    "    corrmat = pd.DataFrame(corrmat).reset_index()\n",
    "    corrmat.columns = ['feature1', 'feature2', 'corr']\n",
    "\n",
    "    # find groups of correlated features\n",
    "    grouped_feature_ls = []\n",
    "    correlated_groups = []\n",
    "\n",
    "    for feature in corrmat.feature1.unique():\n",
    "        if feature not in grouped_feature_ls:\n",
    "            # find all features correlated to a single feature\n",
    "            correlated_block = corrmat[corrmat.feature1 == feature]\n",
    "            grouped_feature_ls = grouped_feature_ls + list(\n",
    "                correlated_block.feature2.unique()) + [feature]\n",
    "\n",
    "            # append the block of features to the list\n",
    "            correlated_groups.append(correlated_block)\n",
    "\n",
    "    count = 0\n",
    "    drop_features = []\n",
    "    for group in correlated_groups:\n",
    "        count += 1\n",
    "        features = list(group.feature1.unique()) + list(group.feature2.unique())\n",
    "        rf = RandomForestRegressor(n_estimators=100, random_state=seed)\n",
    "        rf.fit(X_train[features], Y_train)\n",
    "        keep = features[rf.feature_importances_.argmax()]\n",
    "        features.remove(keep)\n",
    "        drop_features += features\n",
    "        if count % 10 == 0:\n",
    "            print(f'\\rFeature Collinearity Elimination Progress: {count} / {len(correlated_groups)}', end='')\n",
    "    return drop_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tek değerli değişkenleri eler:\n",
    "\n",
    "def drop_constant(train_x):\n",
    "    \"\"\"\n",
    "\n",
    "    :param train_x:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    numerical = train_x.select_dtypes(['int64', 'float64']).columns\n",
    "    constant_numerical = [feat for feat in numerical if train_x[feat].std() == 0]\n",
    "    train_x.drop(constant_numerical, axis=1, inplace=True)\n",
    "    #test_x.drop(constant_numerical, axis=1, inplace=True)\n",
    "\n",
    "    categorical = train_x.select_dtypes(['O']).columns\n",
    "    constant_categorical = [feat for feat in categorical if len(train_x[feat].unique()) == 1]\n",
    "    train_x.drop(constant_categorical, axis=1, inplace=True)\n",
    "    #test_x.drop(constant_categorical, axis=1, inplace=True)\n",
    "    dropped_cache = constant_categorical + constant_numerical\n",
    "    return dropped_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data okuma:\n",
    "\n",
    "conn = pyodbc.connect('Driver={SQL Server};'\n",
    "                      'Server=DEVELOPMENT-01;'\n",
    "                      'Database=Erc_Project;'\n",
    "                      'Trusted_Connection=yes;')\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "data = pd.read_sql_query(('SELECT  * FROM [Erc_Project].[exp].[bireysel_vars_target]'), conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>Unique Number</th>\n",
       "      <th>Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ContactCode</td>\n",
       "      <td>451536</td>\n",
       "      <td>[44132683, 10914381, 1361520, 64712254, 244906...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RefDate</td>\n",
       "      <td>35</td>\n",
       "      <td>[2020-09-30, 2020-11-30, 2020-05-31, 2021-02-2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BusinessUnitCode</td>\n",
       "      <td>3</td>\n",
       "      <td>[40, 42, 41]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AssetManagementCode</td>\n",
       "      <td>73</td>\n",
       "      <td>[23, 2035, 6482, 8790, 4417, 7205, 5688, 5580,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ReachStatusCode</td>\n",
       "      <td>7</td>\n",
       "      <td>[0.0, 1.0, 3.0, 2.0, nan, 4.0, 6.0, 5.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>CanceledProtocolAmount_max_l12m</td>\n",
       "      <td>20287</td>\n",
       "      <td>[nan, 2400.18, 392.66, 2728.0, 2100.0, 2900.1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>FulledTotalProtocolAmount_max_l12m</td>\n",
       "      <td>2529</td>\n",
       "      <td>[nan, 1044.65, 1500.0, 800.0, 1804.0, 1700.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>LiveProtocolAmount_max_l12m</td>\n",
       "      <td>8303</td>\n",
       "      <td>[nan, 610.68, 2150.0, 19173.0, 1044.65, 2300.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>TotalProtocolAmount_max_l12m</td>\n",
       "      <td>13969</td>\n",
       "      <td>[nan, 2400.18, 2728.0, 1479.98, 610.68, 2900.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>target</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>759 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Features  Unique Number  \\\n",
       "0                           ContactCode         451536   \n",
       "1                               RefDate             35   \n",
       "2                      BusinessUnitCode              3   \n",
       "3                   AssetManagementCode             73   \n",
       "4                       ReachStatusCode              7   \n",
       "..                                  ...            ...   \n",
       "754     CanceledProtocolAmount_max_l12m          20287   \n",
       "755  FulledTotalProtocolAmount_max_l12m           2529   \n",
       "756         LiveProtocolAmount_max_l12m           8303   \n",
       "757        TotalProtocolAmount_max_l12m          13969   \n",
       "758                              target              2   \n",
       "\n",
       "                                                Values  \n",
       "0    [44132683, 10914381, 1361520, 64712254, 244906...  \n",
       "1    [2020-09-30, 2020-11-30, 2020-05-31, 2021-02-2...  \n",
       "2                                         [40, 42, 41]  \n",
       "3    [23, 2035, 6482, 8790, 4417, 7205, 5688, 5580,...  \n",
       "4             [0.0, 1.0, 3.0, 2.0, nan, 4.0, 6.0, 5.0]  \n",
       "..                                                 ...  \n",
       "754  [nan, 2400.18, 392.66, 2728.0, 2100.0, 2900.1,...  \n",
       "755  [nan, 1044.65, 1500.0, 800.0, 1804.0, 1700.0, ...  \n",
       "756  [nan, 610.68, 2150.0, 19173.0, 1044.65, 2300.0...  \n",
       "757  [nan, 2400.18, 2728.0, 1479.98, 610.68, 2900.1...  \n",
       "758                                             [0, 1]  \n",
       "\n",
       "[759 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#değişkenlerin içeriklerini gösterir:\n",
    "attFeatures = []\n",
    "for i in data.columns:\n",
    "    attFeatures.append([i, data[i].nunique(), data[i].drop_duplicates().values])\n",
    "pd.DataFrame(attFeatures, columns = ['Features', 'Unique Number', 'Values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:51:37\n",
      "30_05_2022\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "print(datetime.now().strftime(\"%H:%M:%S\"))\n",
    "print(datetime.today().strftime(\"%d_%m_%Y\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#değişkenlerin içeriklerini gösterir:\n",
    "xxx=pd.DataFrame(attFeatures, columns = ['Features', 'Unique Number', 'Values'])\n",
    "xxx.to_excel('Feature_details_'+str(datetime.today().strftime(\"%Y_%m_%d\"))+'.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['IsSalaryDeduction']=np.where(df1['IsSalaryDeduction']=='True',1,np.where(df1['IsSalaryDeduction']=='False',0,-999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['IsTCCitizen']=np.where(df1['IsTCCitizen']=='True',1,np.where(df1['IsTCCitizen']=='False',0,-999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.drop(columns=['CityName','Region','BirthPlace'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['CityCode'] = df1['CityCode'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReachStatusCode                        10327\n",
       "IncomeStatusCode                      157382\n",
       "FinancialStatusCode                   158491\n",
       "AggrementStatusCode                   156739\n",
       "FirstReachDate_monthdiff              147480\n",
       "                                       ...  \n",
       "BrokeProtocolAmount_max_l12m          609139\n",
       "CanceledProtocolAmount_max_l12m       582542\n",
       "FulledTotalProtocolAmount_max_l12m    615167\n",
       "LiveProtocolAmount_max_l12m           606946\n",
       "TotalProtocolAmount_max_l12m          594754\n",
       "Length: 629, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# missing values count:\n",
    "missing = df1.isnull().sum()\n",
    "missing = missing[missing>0]\n",
    "missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>ContactCode</th>\n",
       "      <th>RefDate</th>\n",
       "      <th>BusinessUnitCode</th>\n",
       "      <th>AssetManagementCode</th>\n",
       "      <th>ReachStatusCode</th>\n",
       "      <th>IncomeStatusCode</th>\n",
       "      <th>FinancialStatusCode</th>\n",
       "      <th>AggrementStatusCode</th>\n",
       "      <th>FirstReachDate_monthdiff</th>\n",
       "      <th>...</th>\n",
       "      <th>LiveProtocolCount_max_l12m</th>\n",
       "      <th>NewProtocolCount_max_l12m</th>\n",
       "      <th>ProtocolAmount_max_l12m</th>\n",
       "      <th>ActivatedProtocolAmount_max_l12m</th>\n",
       "      <th>BrokeProtocolAmount_max_l12m</th>\n",
       "      <th>CanceledProtocolAmount_max_l12m</th>\n",
       "      <th>FulledTotalProtocolAmount_max_l12m</th>\n",
       "      <th>LiveProtocolAmount_max_l12m</th>\n",
       "      <th>TotalProtocolAmount_max_l12m</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>44132683</td>\n",
       "      <td>2020-09-30</td>\n",
       "      <td>40</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10914381</td>\n",
       "      <td>2020-11-30</td>\n",
       "      <td>40</td>\n",
       "      <td>2035</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1361520</td>\n",
       "      <td>2020-05-31</td>\n",
       "      <td>42</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>64712254</td>\n",
       "      <td>2021-02-28</td>\n",
       "      <td>40</td>\n",
       "      <td>23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>24490610</td>\n",
       "      <td>2021-02-28</td>\n",
       "      <td>40</td>\n",
       "      <td>23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 757 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index ContactCode     RefDate  BusinessUnitCode  AssetManagementCode  \\\n",
       "0      0    44132683  2020-09-30                40                   23   \n",
       "1      1    10914381  2020-11-30                40                 2035   \n",
       "2      2     1361520  2020-05-31                42                   23   \n",
       "3      3    64712254  2021-02-28                40                   23   \n",
       "4      4    24490610  2021-02-28                40                   23   \n",
       "\n",
       "   ReachStatusCode  IncomeStatusCode  FinancialStatusCode  \\\n",
       "0              0.0               NaN                  NaN   \n",
       "1              1.0              12.0                 19.0   \n",
       "2              0.0              14.0                  1.0   \n",
       "3              1.0               NaN                  NaN   \n",
       "4              1.0               NaN                  NaN   \n",
       "\n",
       "   AggrementStatusCode  FirstReachDate_monthdiff  ...  \\\n",
       "0                  NaN                       NaN  ...   \n",
       "1                  2.0                      31.0  ...   \n",
       "2                 22.0                      41.0  ...   \n",
       "3                  NaN                       NaN  ...   \n",
       "4                  NaN                       NaN  ...   \n",
       "\n",
       "   LiveProtocolCount_max_l12m  NewProtocolCount_max_l12m  \\\n",
       "0                         NaN                        NaN   \n",
       "1                         NaN                        NaN   \n",
       "2                         NaN                        NaN   \n",
       "3                         NaN                        NaN   \n",
       "4                         NaN                        NaN   \n",
       "\n",
       "   ProtocolAmount_max_l12m  ActivatedProtocolAmount_max_l12m  \\\n",
       "0                      NaN                               NaN   \n",
       "1                      NaN                               NaN   \n",
       "2                      NaN                               NaN   \n",
       "3                      NaN                               NaN   \n",
       "4                      NaN                               NaN   \n",
       "\n",
       "   BrokeProtocolAmount_max_l12m  CanceledProtocolAmount_max_l12m  \\\n",
       "0                           NaN                              NaN   \n",
       "1                           NaN                              NaN   \n",
       "2                           NaN                              NaN   \n",
       "3                           NaN                              NaN   \n",
       "4                           NaN                              NaN   \n",
       "\n",
       "  FulledTotalProtocolAmount_max_l12m  LiveProtocolAmount_max_l12m  \\\n",
       "0                                NaN                          NaN   \n",
       "1                                NaN                          NaN   \n",
       "2                                NaN                          NaN   \n",
       "3                                NaN                          NaN   \n",
       "4                                NaN                          NaN   \n",
       "\n",
       "   TotalProtocolAmount_max_l12m  target  \n",
       "0                           NaN       0  \n",
       "1                           NaN       0  \n",
       "2                           NaN       0  \n",
       "3                           NaN       0  \n",
       "4                           NaN       0  \n",
       "\n",
       "[5 rows x 757 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manuel kolon silme:\n",
    "#df1 = df1.drop(columns=['ContactCode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dften target silme, y=target oluşturma:\n",
    "X_new = df1.drop(['target'],axis=1)\n",
    "y_new = df1['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#min date ile oot ayırma:\n",
    "#ardından train-test ayrılır:\n",
    "min_date = '2021-01-01'\n",
    "oot_index = X_new.loc[X_new['RefDate'] >= min_date].index.values\n",
    "dev_index = X_new.loc[X_new['RefDate'] < min_date].index.values  \n",
    "\n",
    "X_new = X_new.drop(columns=['RefDate'])\n",
    "\n",
    "X_dev = X_new.iloc[dev_index]\n",
    "y_dev = y_new.iloc[dev_index]\n",
    "oot_x = X_new.iloc[oot_index]\n",
    "oot_y = y_new.iloc[oot_index]\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(X_dev, y_dev, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_new = train_x.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tek değerli değişken silme:\n",
    "dc = drop_constant(train_x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "250\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "#aynı tutulan kolonları tekilleştirme:\n",
    "ddc = drop_duplicate_columns(train_x_new,  dummy_columns_prefix='DMY_') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data tipi obje olanları listeleme:\n",
    "k=[]\n",
    "for i in train_x_new.columns: \n",
    "    if(train_x_new[i].dtype == object):\n",
    "        k.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#objeler string -999 ile, numerikler sayı -999 ile doldurulur:\n",
    "for i in range(len(k)):\n",
    "    train_x_new[k[i]].fillna('-999',inplace=True)\n",
    "    train_x_new[k[i]]=train_x_new[k[i]].replace([np.nan],'-999')\n",
    "    train_x_new[k[i]]=train_x_new[k[i]].replace([np.inf,-np.inf],'-999')\n",
    "    train_x_new[k[i]]=train_x_new[k[i]].replace([np.inf,-np.inf],'-999')\n",
    "    train_x_new[k[i]]=train_x_new[k[i]].astype(str)\n",
    "\n",
    "train_x_new.fillna(-999,inplace=True)\n",
    "train_x_new=train_x_new.replace([np.nan],-999)\n",
    "train_x_new=train_x_new.replace([np.inf,-np.inf],-999)\n",
    "train_x_new=train_x_new.replace([np.inf,-np.inf],-999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test datası aynı işlemler:\n",
    "#data tipi obje olanları listeleme:\n",
    "k=[]\n",
    "for i in test_x.columns: \n",
    "    if(test_x[i].dtype == object):\n",
    "        k.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:6392: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return self._update_inplace(result)\n",
      "C:\\Users\\P-CEMR~1.KAS\\AppData\\Local\\Temp/ipykernel_15796/3853210864.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_x[k[i]]=test_x[k[i]].replace([np.nan],'-999')\n",
      "C:\\Users\\P-CEMR~1.KAS\\AppData\\Local\\Temp/ipykernel_15796/3853210864.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_x[k[i]]=test_x[k[i]].replace([np.inf,-np.inf],'-999')\n",
      "C:\\Users\\P-CEMR~1.KAS\\AppData\\Local\\Temp/ipykernel_15796/3853210864.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_x[k[i]]=test_x[k[i]].replace([np.inf,-np.inf],'-999')\n",
      "C:\\Users\\P-CEMR~1.KAS\\AppData\\Local\\Temp/ipykernel_15796/3853210864.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_x[k[i]]=test_x[k[i]].astype(str)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:5176: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().fillna(\n",
      "C:\\Users\\P-CEMR~1.KAS\\AppData\\Local\\Temp/ipykernel_15796/3853210864.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  oot_x[k[i]]=oot_x[k[i]].replace([np.nan],'-999')\n",
      "C:\\Users\\P-CEMR~1.KAS\\AppData\\Local\\Temp/ipykernel_15796/3853210864.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  oot_x[k[i]]=oot_x[k[i]].replace([np.inf,-np.inf],'-999')\n",
      "C:\\Users\\P-CEMR~1.KAS\\AppData\\Local\\Temp/ipykernel_15796/3853210864.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  oot_x[k[i]]=oot_x[k[i]].replace([np.inf,-np.inf],'-999')\n",
      "C:\\Users\\P-CEMR~1.KAS\\AppData\\Local\\Temp/ipykernel_15796/3853210864.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  oot_x[k[i]]=oot_x[k[i]].astype(str)\n"
     ]
    }
   ],
   "source": [
    "#test datası aynı işlemler:\n",
    "#objeler string 9999 ile, numerikler sayı 9999 ile doldurulur:\n",
    "for i in range(len(k)):\n",
    "    test_x[k[i]].fillna('-999',inplace=True)\n",
    "    test_x[k[i]]=test_x[k[i]].replace([np.nan],'-999')\n",
    "    test_x[k[i]]=test_x[k[i]].replace([np.inf,-np.inf],'-999')\n",
    "    test_x[k[i]]=test_x[k[i]].replace([np.inf,-np.inf],'-999')\n",
    "    test_x[k[i]]=test_x[k[i]].astype(str)\n",
    "\n",
    "test_x.fillna(-999,inplace=True)\n",
    "test_x=test_x.replace([np.nan],-999)\n",
    "test_x=test_x.replace([np.inf,-np.inf],-999)\n",
    "test_x=test_x.replace([np.inf,-np.inf],-999)\n",
    "\n",
    "#oot için aynı işlemler:\n",
    "for i in range(len(k)):\n",
    "    oot_x[k[i]].fillna('-999',inplace=True)\n",
    "    oot_x[k[i]]=oot_x[k[i]].replace([np.nan],'-999')\n",
    "    oot_x[k[i]]=oot_x[k[i]].replace([np.inf,-np.inf],'-999')\n",
    "    oot_x[k[i]]=oot_x[k[i]].replace([np.inf,-np.inf],'-999')\n",
    "    oot_x[k[i]]=oot_x[k[i]].astype(str)\n",
    "\n",
    "oot_x.fillna(-999,inplace=True)\n",
    "oot_x=oot_x.replace([np.nan],-999)\n",
    "oot_x=oot_x.replace([np.inf,-np.inf],-999)\n",
    "oot_x=oot_x.replace([np.inf,-np.inf],-999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time:  15:01:04\n"
     ]
    }
   ],
   "source": [
    "print(\"start time: \",datetime.now().strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Collinearity Elimination Progress: 70 / 78"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['index', 'ContactCode', 'BusinessUnitCode', 'AssetManagementCode',\n",
       "       'ReachStatusCode', 'FirstReachDate_monthdiff',\n",
       "       'FollowingStartDate_monthdiff', 'Sensibility', 'IsTCCitizen', 'Age',\n",
       "       ...\n",
       "       'ProtocolAmount_max_l6m', 'LiveProtocolAmount_max_l6m',\n",
       "       'BrokeProtocolAmount_min_l12m', 'TotalProtocolAmount_min_l12m',\n",
       "       'ActivatedProtocolAmount_avg_l12m',\n",
       "       'FulledTotalProtocolAmount_avg_l12m', 'LiveProtocolCount_max_l12m',\n",
       "       'ProtocolAmount_max_l12m', 'CanceledProtocolAmount_max_l12m',\n",
       "       'LiveProtocolAmount_max_l12m'],\n",
       "      dtype='object', length=134)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#corr elemesi:\n",
    "\n",
    "elc = eliminate_group_correlation(train_x_new, train_y, corrmat=None, threshold=0.8, seed=100) #136\n",
    "train_x_new.drop(elc, axis=1, inplace=True)\n",
    "train_x_new.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end time:  20:21:10\n"
     ]
    }
   ],
   "source": [
    "print(\"end time: \",datetime.now().strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train_x_new.columns).to_excel('bireysel_model_variables_'+str(datetime.today().strftime(\"%Y_%m_%d\"))+'.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vars=pd.read_excel('bireysel_model_variables_'+str(datetime.today().strftime(\"%Y_%m_%d\"))+'.xlsx')\n",
    "model_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vars[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=train_x[model_vars[0]]\n",
    "train_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#değişkenlerin içeriklerini gösterir:\n",
    "attFeatures2 = []\n",
    "for i in train_x.columns:\n",
    "    attFeatures2.append([i, train_x[i].nunique(), train_x[i].drop_duplicates().values])\n",
    "pd.DataFrame(attFeatures2, columns = ['Features', 'Unique Number', 'Values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(attFeatures2).to_excel(\"train_col_Details.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alttaki iki kolon için refdateten fark hesaplanacak\n",
    "\n",
    "\n",
    "#import datetime\n",
    "#date1=data['RefDate'][0]\n",
    "#date2=str(data['min_acqdate_ever'][0])\n",
    "#mdate1 = datetime.datetime.strptime(date1, \"%Y-%m-%d\").date()\n",
    "#rdate1 = datetime.datetime.strptime(date2, \"%Y-%m-%d\").date()\n",
    "#delta =  (mdate1 - rdate1).days\n",
    "#print (delta)\n",
    "#train_x['min_acqdate_ever']\n",
    "#train_x['min_acqdate_open']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elenecek değişkenleri listeler\n",
    "elimination_list = dc+ddc+elc\n",
    "elimination_list=pd.DataFrame(elimination_list)\n",
    "elimination_list['elimination'] = ''\n",
    "elimination_list['elimination'][:(len(dc)-1)]='Constant'\n",
    "elimination_list['elimination'][len(dc):(len(dc)+len(ddc)-1)]='Duplicate'\n",
    "elimination_list['elimination'][(len(dc)+len(ddc)):]='Correlation'\n",
    "\n",
    "#elenen değişkenler raporlanabilir\n",
    "elimination_list.to_excel('elimination_list.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elenmesi istenmeyen ama elenen değişkenler mauel olarak eklenir:\n",
    "\n",
    "#train_x = pd.merge(train_x_new,train_x[['APP_ID',\n",
    "#'RT_KKB_CC_O_TOT_LIM',\n",
    "#'RT_KKB_CC_MAX_O_C_LIM_L1Y',\n",
    "#'RT_CUST_TENURE',\n",
    "#'RT_CRE_TENURE',\n",
    "#'RT_EDUCATION',\n",
    "#'RT_MMZC_TOT_CASH_LIM_LM',\n",
    "#'RT_KKB_OD_O_MAX_LIM',\n",
    "#'RT_KKB_O_TOT_CC_AMT',\n",
    "#'VAR21',\n",
    "#'RT_KKB_CC_2ND_MAX_O_C_LIM_L1Y',\n",
    "#'RT_KKB_CC_O_TOT_RISK',\n",
    "#'RT_KKB_INST_O_TOT_GPL_AMT',\n",
    "#'RT_KKB_OD_O_TOT_RISK',\n",
    "#'RT_KKB_SCORE',\n",
    "#'VAR20',\n",
    "#'VAR22',\n",
    "#'VAR23',\n",
    "#'VAR31',\n",
    "#'VAR68',\n",
    "#'VAR95'                                        \n",
    "#]],on='APP_ID', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#id kolonunu silme:\n",
    "#train_x = train_x.drop(columns=['APP_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_x['IsSalaryDeduction']=np.where(train_x['IsSalaryDeduction']=='True',1,np.where(train_x['IsSalaryDeduction']=='False',0,-999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_x['IsTCCitizen']=np.where(train_x['IsTCCitizen']=='True',1,np.where(train_x['IsTCCitizen']=='False',0,-999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_x['IsSalaryDeduction']=np.where(test_x['IsSalaryDeduction']=='True',1,np.where(test_x['IsSalaryDeduction']=='False',0,-999))\n",
    "#oot_x['IsSalaryDeduction']=np.where(oot_x['IsSalaryDeduction']=='True',1,np.where(oot_x['IsSalaryDeduction']=='False',0,-999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_x['IsTCCitizen']=np.where(test_x['IsTCCitizen']=='True',1,np.where(test_x['IsTCCitizen']=='False',0,-999))\n",
    "#oot_x['IsTCCitizen']=np.where(oot_x['IsTCCitizen']=='True',1,np.where(oot_x['IsTCCitizen']=='False',0,-999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_x = train_x.drop(columns=['CityName','Region','BirthPlace'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_x = test_x.drop(columns=['CityName','Region','BirthPlace'])\n",
    "#oot_x = oot_x.drop(columns=['CityName','Region','BirthPlace'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_x['CityCode'] = train_x['CityCode'].astype(int)\n",
    "#test_x['CityCode'] = test_x['CityCode'].astype(int)\n",
    "#oot_x['CityCode'] = oot_x['CityCode'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eklenen kolonlar olduğu için yeni obje gelenleri listeler:\n",
    "k=[]\n",
    "for i in train_x.columns: \n",
    "    if(train_x[i].dtype == object):\n",
    "        k.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#null replacement:\n",
    "\n",
    "for i in range(len(k)):\n",
    "    train_x[k[i]].fillna('-999',inplace=True)\n",
    "    train_x[k[i]]=train_x[k[i]].replace([np.nan],'-999')\n",
    "    train_x[k[i]]=train_x[k[i]].replace([np.inf,-np.inf],'-999')\n",
    "    train_x[k[i]]=train_x[k[i]].replace([np.inf,-np.inf],'-999')\n",
    "    train_x[k[i]]=train_x[k[i]].astype(str)\n",
    "\n",
    "train_x.fillna(-999,inplace=True)\n",
    "train_x=train_x.replace([np.nan],-999)\n",
    "train_x=train_x.replace([np.inf,-np.inf],-999)\n",
    "train_x=train_x.replace([np.inf,-np.inf],-999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding:\n",
    "labelEncoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obje değişkenler için _L suffix ekler ve label encoding yapar:\n",
    "for i in range(len(k)):\n",
    "    train_x[k[i]+'_L'] = labelEncoder.fit_transform(train_x[k[i]])\n",
    "    \n",
    "for i in range(len(k)):\n",
    "    test_x[k[i]+'_L'] = labelEncoder.fit_transform(test_x[k[i]])\n",
    "    \n",
    "for i in range(len(k)):\n",
    "    oot_x[k[i]+'_L'] = labelEncoder.fit_transform(oot_x[k[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obje kolonları siler:\n",
    "train_x.drop(k, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot encoding: \n",
    "#train_x = pd.get_dummies(train_x, columns=['RT_OCCUPATION','RT_SECTOR_TYPE_DESC','RT_WORKING_TYPE_DESC','RT_EMPLOYMENT_TYPE','RT_CUSTOMER_TYPE' ])    \n",
    "#test_x = pd.get_dummies(test_x, columns=['RT_OCCUPATION','RT_SECTOR_TYPE_DESC','RT_WORKING_TYPE_DESC','RT_EMPLOYMENT_TYPE','RT_CUSTOMER_TYPE' ]) \n",
    "#oot_x = pd.get_dummies(oot_x, columns=['RT_OCCUPATION','RT_SECTOR_TYPE_DESC','RT_WORKING_TYPE_DESC','RT_EMPLOYMENT_TYPE','RT_CUSTOMER_TYPE' ]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kolon isimlerini değiştirme:\n",
    "#train_x.rename(columns={'RT_SECTOR_TYPE_DESC_-9999.0':'RT_SECTOR_TYPE_DESC_9999',\n",
    "#                        'RT_WORKING_TYPE_DESC_-9999.0':'RT_WORKING_TYPE_DESC_9999'}, inplace = True) \n",
    "#test_x.rename(columns={'RT_SECTOR_TYPE_DESC_-9999.0':'RT_SECTOR_TYPE_DESC_9999.0',\n",
    "#                        'RT_WORKING_TYPE_DESC_-9999.0':'RT_WORKING_TYPE_DESC_9999'}, inplace = True) \n",
    "#oot_x.rename(columns={'RT_SECTOR_TYPE_DESC_-9999.0':'RT_SECTOR_TYPE_DESC_9999.0',\n",
    "#                        'RT_WORKING_TYPE_DESC_-9999.0':'RT_WORKING_TYPE_DESC_9999'}, inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kolon isimlerini listeler:\n",
    "pd.DataFrame(train_x.columns).to_excel('bireysel_final_shortlist.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kaldırılacak:\n",
    "oot_x['RT_OCCUPATION_4.0'] = 0 \n",
    "oot_x['RT_OCCUPATION_75.0'] = 0\n",
    "oot_x['RT_SECTOR_TYPE_DESC_9999'] = 0\n",
    "test_x['RT_SECTOR_TYPE_DESC_9999'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kaldırılacak:\n",
    "oot_x[['RT_EMPLOYMENT_TYPE_5.0', 'RT_EMPLOYMENT_TYPE_10.0', \n",
    "       'RT_EMPLOYMENT_TYPE_11.0', 'RT_EMPLOYMENT_TYPE_13.0', \n",
    "       'RT_EMPLOYMENT_TYPE_17.0', 'RT_EMPLOYMENT_TYPE_20.0', \n",
    "       'RT_EMPLOYMENT_TYPE_26.0', 'RT_EMPLOYMENT_TYPE_27.0', \n",
    "       'RT_EMPLOYMENT_TYPE_31.0', 'RT_EMPLOYMENT_TYPE_33.0', \n",
    "       'RT_EMPLOYMENT_TYPE_34.0', 'RT_EMPLOYMENT_TYPE_35.0', \n",
    "       'RT_EMPLOYMENT_TYPE_41.0', 'RT_EMPLOYMENT_TYPE_43.0', \n",
    "       'RT_EMPLOYMENT_TYPE_48.0', 'RT_EMPLOYMENT_TYPE_51.0', \n",
    "       'RT_EMPLOYMENT_TYPE_52.0', 'RT_EMPLOYMENT_TYPE_55.0']] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oot_x_new = oot_x[train_x.columns] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_x[['RT_EMPLOYMENT_TYPE_10.0', 'RT_EMPLOYMENT_TYPE_13.0', \n",
    "#        'RT_EMPLOYMENT_TYPE_31.0', 'RT_EMPLOYMENT_TYPE_33.0', \n",
    "#        'RT_EMPLOYMENT_TYPE_34.0', 'RT_EMPLOYMENT_TYPE_35.0', \n",
    "#        'RT_EMPLOYMENT_TYPE_36.0', 'RT_EMPLOYMENT_TYPE_48.0', \n",
    "#        'RT_EMPLOYMENT_TYPE_52.0', 'RT_EMPLOYMENT_TYPE_55.0']] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_new = test_x[train_x.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgs = [sk.ensemble.RandomForestRegressor(n_jobs=-1), sk.ensemble.GradientBoostingRegressor(),\n",
    "      XGBRegressor()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=train_x.drop(columns=['min_acqdate_open','min_acqdate_ever'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hangi model daha iyi sonuç verir: (metrik:MAE) (cross_val_score'un scoring değiştirilecek)\n",
    "nfolds = 2\n",
    "kf = KFold(n_splits=nfolds,random_state=0,shuffle=True)\n",
    "bestmae = 9999999999\n",
    "bestrg = \"\"\n",
    "for rg in rgs:\n",
    "    mae = (sk.model_selection.cross_val_score(rg,train_x,train_y,cv=kf,n_jobs=-1,scoring='neg_mean_absolute_error').mean())\n",
    "    print (str(rg) + ' ' + str(mae))\n",
    "    if  -mae < bestmae:\n",
    "        bestrg = rg\n",
    "        bestmae = -mae\n",
    "        \n",
    "\n",
    "print('***********************************************')\n",
    "print ('Best is... ' + str(bestrg) + ' ' + str(-bestmae) + str(pprint(bestrg.get_params())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rkareye göre : hangi model daha iyi sonuç verir: (metrik:MAE) (cross_val_score'un scoring değiştirilecek)\n",
    "nfolds = 2\n",
    "kf = KFold(n_splits=nfolds,random_state=0,shuffle=True)\n",
    "worst_r = -9999999999\n",
    "bestrg = \"\"\n",
    "for rg in rgs:\n",
    "    r_square = (sk.model_selection.cross_val_score(rg,train_x,train_y,cv=kf,n_jobs=-1,scoring='r2').mean())\n",
    "    print (str(rg) + ' ' + str(mae))\n",
    "    if  r_square > worst_r:\n",
    "        bestrg = rg\n",
    "        bestmae = r_square\n",
    "        \n",
    "\n",
    "print('***********************************************')\n",
    "print ('Best is... ' + str(bestrg) + ' ' + str(-bestmae) + str(pprint(bestrg.get_params())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#en iyi modeli fit etme\n",
    "model=bestrg.fit(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importanceları çıkartır:\n",
    "sorted(zip((model.feature_importances_)*100,train_x.columns),reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final modeli fit etme:\n",
    "#rg = XGBRegressor()\n",
    "#nfolds = 10\n",
    "#kf = KFold(n_splits=nfolds,random_state=0,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_xgb = rg.fit(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = (sk.model_selection.cross_val_score(rg,train_x,train_y,cv=kf,n_jobs=-1,scoring='neg_mean_absolute_error').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [5, 10, 30],\n",
    "    'max_features': ['auto'],\n",
    "    'min_samples_leaf': [100,200,300,500],\n",
    "    'min_samples_split': [2 ,4, 8],\n",
    "    'n_estimators': [100, 200, 300]\n",
    "}\n",
    "# Create a based model\n",
    "rf = RandomForestRegressor()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(train_x, train_y_new)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#değişecek gini:\n",
    "def evaluate(model, test_features, test_labels):\n",
    "    predictions = model.predict(test_features)\n",
    "    errors = abs(predictions - test_labels)\n",
    "    mape = 100 * np.mean(errors / test_labels)\n",
    "    accuracy = 100 - mape\n",
    "    print('Model Performance')\n",
    "    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "    print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "    \n",
    "    return accuracy\n",
    "base_model = RandomForestRegressor(n_estimators = 10, random_state = 42)\n",
    "base_model.fit(train_x, train_y_new)\n",
    "base_accuracy = evaluate(base_model, test_x[train_x.columns], test_y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "best_grid = grid_search.best_estimator_\n",
    "grid_accuracy = evaluate(best_grid, test_x[train_x.columns], test_y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#oot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_accuracy = evaluate(best_grid, oot_x_new, oot_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_accuracy = evaluate(best_grid, train_x, train_y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bundan sonrası yok:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_new = test_y['target_w_inf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfolds = 10\n",
    "kf = KFold(n_splits=nfolds,random_state=0,shuffle=True)\n",
    "bestmae = 9999999999\n",
    "bestrg = \"\"\n",
    "for rg in rgs:\n",
    "    mae = (sk.model_selection.cross_val_score(rg,test_x,test_y_new,cv=kf,n_jobs=-1,scoring='neg_mean_absolute_error').mean())\n",
    "    print (str(rg) + ' ' + str(mae))\n",
    "    if  -mae < bestmae:\n",
    "        bestrg = rg\n",
    "        bestmae = -mae\n",
    "        \n",
    "\n",
    "print('***********************************************')\n",
    "print ('Best is... ' + str(bestrg) + ' ' + str(-bestmae) + 'Parameters currently in use:\\n' + str(pprint(bestrg.get_params())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##OOT\n",
    "nfolds = 10\n",
    "kf = KFold(n_splits=nfolds,random_state=0,shuffle=True)\n",
    "bestmae = 9999999999\n",
    "bestrg = \"\"\n",
    "for rg in rgs:\n",
    "    mae = (sk.model_selection.cross_val_score(rg,oot_x_new,oot_y,cv=kf,n_jobs=-1,scoring='neg_mean_absolute_error').mean())\n",
    "    print (str(rg) + ' ' + str(mae))\n",
    "    if  -mae < bestmae:\n",
    "        bestrg = rg\n",
    "        bestmae = -mae\n",
    "        \n",
    "\n",
    "print('***********************************************')\n",
    "print ('Best is... ' + str(bestrg) + ' ' + str(-bestmae) + 'Parameters currently in use:\\n' + str(pprint(bestrg.get_params())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset\n",
    "\n",
    "# create pipeline\n",
    "rfe = RFE(estimator=RandomForestRegressor(n_jobs=-1), n_features_to_select=100, step=1) ##seçilecek feature adetini revize et(100,75,50)\n",
    "model = RandomForestRegressor()\n",
    "pipeline = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "# evaluate model\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(pipeline, train_x, train_y_new, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n",
    "# report performance\n",
    "print('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
    "\n",
    "rfe.support_\n",
    "# summarize all features\n",
    "#for i in range(train_x.shape[1]):\n",
    "#print('Column: %d, Selected %s, Rank: %.3f' % (i, rfe.support_[i], rfe.ranking_[i])) ##bunu kontrol et\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline\n",
    "rfe = RFE(estimator=RandomForestRegressor(), n_features_to_select=5)\n",
    "model = RandomForestRegressor()\n",
    "pipeline = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "# evaluate model\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(pipeline, train_x, train_y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n",
    "# report performance\n",
    "print('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
****
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "%pylab inline\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sklearn as sk\n",
    "import sklearn.tree as tree\n",
    "from IPython.display import Image \n",
    "import pydotplus\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# evaluate RFE for regression\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.model_selection import KFold\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aynı beslenen kolonları tekilleştirir:\n",
    "def drop_duplicate_columns(train_x, dummy_columns_prefix='DMY_'):\n",
    "    \"\"\"\n",
    "\n",
    "    :param train_x:\n",
    "    :param test_x:\n",
    "    :param dummy_columns_prefix:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    desc = train_x[[col for col in train_x.columns if not col.startswith(dummy_columns_prefix)]] \\\n",
    "        .describe()\n",
    "    desc = desc.loc[['mean', 'std'], :]\n",
    "    blacklist = []\n",
    "    duplicated_feat = {}\n",
    "    for i in range(0, len(desc.columns)):\n",
    "        if i % 250 == 0:  # this helps me understand how the loop is going\n",
    "            print(i)\n",
    "\n",
    "        col_1 = desc.columns[i]\n",
    "        if col_1 in blacklist:\n",
    "            continue\n",
    "\n",
    "        for col_2 in desc.columns[i + 1:]:\n",
    "            if desc[col_1].equals(desc[col_2]):\n",
    "                if col_1 not in duplicated_feat:\n",
    "                    duplicated_feat[col_1] = []\n",
    "                duplicated_feat[col_1].append(col_2)\n",
    "\n",
    "        blacklist = []\n",
    "        for sublist in [x for x in duplicated_feat.values()]:\n",
    "            for item in sublist:\n",
    "                blacklist.append(item)\n",
    "\n",
    "    train_x.drop(blacklist, axis=1, inplace=True)\n",
    "    #test_x.drop(blacklist, axis=1, inplace=True)\n",
    "    return blacklist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#değişkenler arasındaki korelasyona bakar, %80 ve üzerini eler, \n",
    "#hangisinin eleneceği tek değişkenli random forest ile belirlenir importance'ı düşük olan elenir:\n",
    "\n",
    "def eliminate_group_correlation(X_train, Y_train, corrmat=None, threshold=0.5, seed=100): #%80 de baklılabilir\n",
    "    \"\"\"\n",
    "\n",
    "    :param X_train:\n",
    "    :param X_test:\n",
    "    :param Y_train:\n",
    "    :param target_column:\n",
    "    :param corrmat:\n",
    "    :param threshold:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if corrmat is None:\n",
    "        corrmat = X_train.corr()\n",
    "    corrmat = corrmat.abs().unstack()  # absolute value of corr coef\n",
    "    corrmat = corrmat.sort_values(ascending=False)\n",
    "    corrmat = corrmat[corrmat >= threshold]\n",
    "    corrmat = corrmat[corrmat < 1]\n",
    "    corrmat = pd.DataFrame(corrmat).reset_index()\n",
    "    corrmat.columns = ['feature1', 'feature2', 'corr']\n",
    "\n",
    "    # find groups of correlated features\n",
    "    grouped_feature_ls = []\n",
    "    correlated_groups = []\n",
    "\n",
    "    for feature in corrmat.feature1.unique():\n",
    "        if feature not in grouped_feature_ls:\n",
    "            # find all features correlated to a single feature\n",
    "            correlated_block = corrmat[corrmat.feature1 == feature]\n",
    "            grouped_feature_ls = grouped_feature_ls + list(\n",
    "                correlated_block.feature2.unique()) + [feature]\n",
    "\n",
    "            # append the block of features to the list\n",
    "            correlated_groups.append(correlated_block)\n",
    "\n",
    "    count = 0\n",
    "    drop_features = []\n",
    "    for group in correlated_groups:\n",
    "        count += 1\n",
    "        features = list(group.feature1.unique()) + list(group.feature2.unique())\n",
    "        rf = RandomForestRegressor(n_estimators=100, random_state=seed)\n",
    "        rf.fit(X_train[features], Y_train)\n",
    "        keep = features[rf.feature_importances_.argmax()]\n",
    "        features.remove(keep)\n",
    "        drop_features += features\n",
    "        if count % 10 == 0:\n",
    "            print(f'\\rFeature Collinearity Elimination Progress: {count} / {len(correlated_groups)}', end='')\n",
    "    return drop_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tek değerli değişkenleri eler:\n",
    "\n",
    "def drop_constant(train_x):\n",
    "    \"\"\"\n",
    "\n",
    "    :param train_x:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    numerical = train_x.select_dtypes(['int64', 'float64']).columns\n",
    "    constant_numerical = [feat for feat in numerical if train_x[feat].std() == 0]\n",
    "    train_x.drop(constant_numerical, axis=1, inplace=True)\n",
    "    #test_x.drop(constant_numerical, axis=1, inplace=True)\n",
    "\n",
    "    categorical = train_x.select_dtypes(['O']).columns\n",
    "    constant_categorical = [feat for feat in categorical if len(train_x[feat].unique()) == 1]\n",
    "    train_x.drop(constant_categorical, axis=1, inplace=True)\n",
    "    #test_x.drop(constant_categorical, axis=1, inplace=True)\n",
    "    dropped_cache = constant_categorical + constant_numerical\n",
    "    return dropped_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data okuma:\n",
    "\n",
    "conn = pyodbc.connect('Driver={SQL Server};'\n",
    "                      'Server=DEVELOPMENT-01;'\n",
    "                      'Database=Erc_Project;'\n",
    "                      'Trusted_Connection=yes;')\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "data = pd.read_sql_query(('SELECT  * FROM [Erc_Project].[exp].[ticari_vars_target]'), conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>Unique Number</th>\n",
       "      <th>Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>contactcode</td>\n",
       "      <td>45502</td>\n",
       "      <td>[38433022, 68510340, 9555221, 75038344, 795108...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>refdate</td>\n",
       "      <td>35</td>\n",
       "      <td>[2019-06-30, 2019-09-30, 2020-02-29, 2021-06-3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BusinessUnitCode</td>\n",
       "      <td>5</td>\n",
       "      <td>[22, 21, 20, 12, 29]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AssetManagementCode</td>\n",
       "      <td>95</td>\n",
       "      <td>[23, 5580, 718, 4246, 7205, 6482, 5643, 5688, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ReachStatusCode</td>\n",
       "      <td>5</td>\n",
       "      <td>[1.0, 3.0, 0.0, 2.0, nan, 4.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>CanceledProtocolAmount_max_l12m</td>\n",
       "      <td>779</td>\n",
       "      <td>[nan, 4500.0, 30123.7, 1662.75, 20000.0, 25000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>FulledTotalProtocolAmount_max_l12m</td>\n",
       "      <td>198</td>\n",
       "      <td>[nan, 3063.0, 30000.0, 1499.0, 2225.0, 1500.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>LiveProtocolAmount_max_l12m</td>\n",
       "      <td>317</td>\n",
       "      <td>[nan, 25642.0, 11937.0, 11481.8, 12500.0, 3003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>TotalProtocolAmount_max_l12m</td>\n",
       "      <td>538</td>\n",
       "      <td>[nan, 30123.7, 834.04, 30000.0, 5851.7, 35000....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>target</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>674 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Features  Unique Number  \\\n",
       "0                           contactcode          45502   \n",
       "1                               refdate             35   \n",
       "2                      BusinessUnitCode              5   \n",
       "3                   AssetManagementCode             95   \n",
       "4                       ReachStatusCode              5   \n",
       "..                                  ...            ...   \n",
       "669     CanceledProtocolAmount_max_l12m            779   \n",
       "670  FulledTotalProtocolAmount_max_l12m            198   \n",
       "671         LiveProtocolAmount_max_l12m            317   \n",
       "672        TotalProtocolAmount_max_l12m            538   \n",
       "673                              target              2   \n",
       "\n",
       "                                                Values  \n",
       "0    [38433022, 68510340, 9555221, 75038344, 795108...  \n",
       "1    [2019-06-30, 2019-09-30, 2020-02-29, 2021-06-3...  \n",
       "2                                 [22, 21, 20, 12, 29]  \n",
       "3    [23, 5580, 718, 4246, 7205, 6482, 5643, 5688, ...  \n",
       "4                       [1.0, 3.0, 0.0, 2.0, nan, 4.0]  \n",
       "..                                                 ...  \n",
       "669  [nan, 4500.0, 30123.7, 1662.75, 20000.0, 25000...  \n",
       "670  [nan, 3063.0, 30000.0, 1499.0, 2225.0, 1500.0,...  \n",
       "671  [nan, 25642.0, 11937.0, 11481.8, 12500.0, 3003...  \n",
       "672  [nan, 30123.7, 834.04, 30000.0, 5851.7, 35000....  \n",
       "673                                             [0, 1]  \n",
       "\n",
       "[674 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#değişkenlerin içeriklerini gösterir:\n",
    "attFeatures = []\n",
    "for i in data.columns:\n",
    "    attFeatures.append([i, data[i].nunique(), data[i].drop_duplicates().values])\n",
    "pd.DataFrame(attFeatures, columns = ['Features', 'Unique Number', 'Values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:17:14\n",
      "02_06_2022\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "print(datetime.now().strftime(\"%H:%M:%S\"))\n",
    "print(datetime.today().strftime(\"%d_%m_%Y\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#değişkenlerin içeriklerini gösterir:\n",
    "xxx=pd.DataFrame(attFeatures, columns = ['Features', 'Unique Number', 'Values'])\n",
    "xxx.to_excel('Ticari_Feature_details_'+str(datetime.today().strftime(\"%Y_%m_%d\"))+'.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['IsSalaryDeduction']=np.where(df1['IsSalaryDeduction']=='True',1,np.where(df1['IsSalaryDeduction']=='False',0,-999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['IsTCCitizen']=np.where(df1['IsTCCitizen']=='True',1,np.where(df1['IsTCCitizen']=='False',0,-999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1 = df1.drop(columns=['CityName','Region','BirthPlace'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['CityCode'] = df1['CityCode'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReachStatusCode                        2102\n",
       "IncomeStatusCode                      14586\n",
       "FinancialStatusCode                   14614\n",
       "AggrementStatusCode                   14665\n",
       "FirstReachDate_monthdiff              14591\n",
       "                                      ...  \n",
       "BrokeProtocolAmount_max_l12m          56314\n",
       "CanceledProtocolAmount_max_l12m       55398\n",
       "FulledTotalProtocolAmount_max_l12m    56393\n",
       "LiveProtocolAmount_max_l12m           56221\n",
       "TotalProtocolAmount_max_l12m          55795\n",
       "Length: 589, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# missing values count:\n",
    "missing = df1.isnull().sum()\n",
    "missing = missing[missing>0]\n",
    "missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>contactcode</th>\n",
       "      <th>refdate</th>\n",
       "      <th>BusinessUnitCode</th>\n",
       "      <th>AssetManagementCode</th>\n",
       "      <th>ReachStatusCode</th>\n",
       "      <th>IncomeStatusCode</th>\n",
       "      <th>FinancialStatusCode</th>\n",
       "      <th>AggrementStatusCode</th>\n",
       "      <th>FirstReachDate_monthdiff</th>\n",
       "      <th>...</th>\n",
       "      <th>LiveProtocolCount_max_l12m</th>\n",
       "      <th>NewProtocolCount_max_l12m</th>\n",
       "      <th>ProtocolAmount_max_l12m</th>\n",
       "      <th>ActivatedProtocolAmount_max_l12m</th>\n",
       "      <th>BrokeProtocolAmount_max_l12m</th>\n",
       "      <th>CanceledProtocolAmount_max_l12m</th>\n",
       "      <th>FulledTotalProtocolAmount_max_l12m</th>\n",
       "      <th>LiveProtocolAmount_max_l12m</th>\n",
       "      <th>TotalProtocolAmount_max_l12m</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>38433022</td>\n",
       "      <td>2019-06-30</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>68510340</td>\n",
       "      <td>2019-09-30</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>9555221</td>\n",
       "      <td>2020-02-29</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>75038344</td>\n",
       "      <td>2021-06-30</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7951080</td>\n",
       "      <td>2019-10-31</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 675 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index contactcode     refdate  BusinessUnitCode  AssetManagementCode  \\\n",
       "0      0    38433022  2019-06-30                22                   23   \n",
       "1      1    68510340  2019-09-30                22                   23   \n",
       "2      2     9555221  2020-02-29                21                   23   \n",
       "3      3    75038344  2021-06-30                21                   23   \n",
       "4      4     7951080  2019-10-31                22                   23   \n",
       "\n",
       "   ReachStatusCode  IncomeStatusCode  FinancialStatusCode  \\\n",
       "0              1.0              24.0                 19.0   \n",
       "1              1.0              10.0                 13.0   \n",
       "2              3.0              16.0                  9.0   \n",
       "3              0.0               NaN                  NaN   \n",
       "4              3.0              10.0                  1.0   \n",
       "\n",
       "   AggrementStatusCode  FirstReachDate_monthdiff  ...  \\\n",
       "0                 22.0                       8.0  ...   \n",
       "1                 22.0                      18.0  ...   \n",
       "2                 22.0                      29.0  ...   \n",
       "3                  NaN                       NaN  ...   \n",
       "4                 22.0                      25.0  ...   \n",
       "\n",
       "   LiveProtocolCount_max_l12m  NewProtocolCount_max_l12m  \\\n",
       "0                         NaN                        NaN   \n",
       "1                         NaN                        NaN   \n",
       "2                         NaN                        NaN   \n",
       "3                         NaN                        NaN   \n",
       "4                         NaN                        NaN   \n",
       "\n",
       "   ProtocolAmount_max_l12m  ActivatedProtocolAmount_max_l12m  \\\n",
       "0                      NaN                               NaN   \n",
       "1                      NaN                               NaN   \n",
       "2                      NaN                               NaN   \n",
       "3                      NaN                               NaN   \n",
       "4                      NaN                               NaN   \n",
       "\n",
       "   BrokeProtocolAmount_max_l12m  CanceledProtocolAmount_max_l12m  \\\n",
       "0                           NaN                              NaN   \n",
       "1                           NaN                              NaN   \n",
       "2                           NaN                              NaN   \n",
       "3                           NaN                              NaN   \n",
       "4                           NaN                              NaN   \n",
       "\n",
       "  FulledTotalProtocolAmount_max_l12m  LiveProtocolAmount_max_l12m  \\\n",
       "0                                NaN                          NaN   \n",
       "1                                NaN                          NaN   \n",
       "2                                NaN                          NaN   \n",
       "3                                NaN                          NaN   \n",
       "4                                NaN                          NaN   \n",
       "\n",
       "   TotalProtocolAmount_max_l12m  target  \n",
       "0                           NaN       0  \n",
       "1                           NaN       0  \n",
       "2                           NaN       0  \n",
       "3                           NaN       1  \n",
       "4                           NaN       0  \n",
       "\n",
       "[5 rows x 675 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manuel kolon silme:\n",
    "df1 = df1.drop(columns=['contacttype_seg'])\n",
    "df1 = df1.drop(columns=['ContactTypeName'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dften target silme, y=target oluşturma:\n",
    "X_new = df1.drop(['target'],axis=1)\n",
    "y_new = df1['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#min date ile oot ayırma:\n",
    "#ardından train-test ayrılır:\n",
    "min_date = '2021-01-01'\n",
    "oot_index = X_new.loc[X_new['refdate'] >= min_date].index.values\n",
    "dev_index = X_new.loc[X_new['refdate'] < min_date].index.values  \n",
    "\n",
    "X_new = X_new.drop(columns=['refdate'])\n",
    "\n",
    "X_dev = X_new.iloc[dev_index]\n",
    "y_dev = y_new.iloc[dev_index]\n",
    "oot_x = X_new.iloc[oot_index]\n",
    "oot_y = y_new.iloc[oot_index]\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(X_dev, y_dev, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_new = train_x.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tek değerli değişken silme:\n",
    "dc = drop_constant(train_x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "250\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "#aynı tutulan kolonları tekilleştirme:\n",
    "ddc = drop_duplicate_columns(train_x_new,  dummy_columns_prefix='DMY_') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['contactcode']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data tipi obje olanları listeleme:\n",
    "k=[]\n",
    "for i in train_x_new.columns: \n",
    "    if(train_x_new[i].dtype == object):\n",
    "        k.append(i)\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#objeler string -999 ile, numerikler sayı -999 ile doldurulur:\n",
    "for i in range(len(k)):\n",
    "    train_x_new[k[i]].fillna('-999',inplace=True)\n",
    "    train_x_new[k[i]]=train_x_new[k[i]].replace([np.nan],'-999')\n",
    "    train_x_new[k[i]]=train_x_new[k[i]].replace([np.inf,-np.inf],'-999')\n",
    "    train_x_new[k[i]]=train_x_new[k[i]].replace([np.inf,-np.inf],'-999')\n",
    "    train_x_new[k[i]]=train_x_new[k[i]].astype(str)\n",
    "\n",
    "train_x_new.fillna(-999,inplace=True)\n",
    "train_x_new=train_x_new.replace([np.nan],-999)\n",
    "train_x_new=train_x_new.replace([np.inf,-np.inf],-999)\n",
    "train_x_new=train_x_new.replace([np.inf,-np.inf],-999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['contactcode']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test datası aynı işlemler:\n",
    "#data tipi obje olanları listeleme:\n",
    "k=[]\n",
    "for i in test_x.columns: \n",
    "    if(test_x[i].dtype == object):\n",
    "        k.append(i)\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:6392: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return self._update_inplace(result)\n",
      "C:\\Users\\P-CEMR~1.KAS\\AppData\\Local\\Temp/ipykernel_10392/3853210864.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_x[k[i]]=test_x[k[i]].replace([np.nan],'-999')\n",
      "C:\\Users\\P-CEMR~1.KAS\\AppData\\Local\\Temp/ipykernel_10392/3853210864.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_x[k[i]]=test_x[k[i]].replace([np.inf,-np.inf],'-999')\n",
      "C:\\Users\\P-CEMR~1.KAS\\AppData\\Local\\Temp/ipykernel_10392/3853210864.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_x[k[i]]=test_x[k[i]].replace([np.inf,-np.inf],'-999')\n",
      "C:\\Users\\P-CEMR~1.KAS\\AppData\\Local\\Temp/ipykernel_10392/3853210864.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_x[k[i]]=test_x[k[i]].astype(str)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:5176: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().fillna(\n",
      "C:\\Users\\P-CEMR~1.KAS\\AppData\\Local\\Temp/ipykernel_10392/3853210864.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  oot_x[k[i]]=oot_x[k[i]].replace([np.nan],'-999')\n",
      "C:\\Users\\P-CEMR~1.KAS\\AppData\\Local\\Temp/ipykernel_10392/3853210864.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  oot_x[k[i]]=oot_x[k[i]].replace([np.inf,-np.inf],'-999')\n",
      "C:\\Users\\P-CEMR~1.KAS\\AppData\\Local\\Temp/ipykernel_10392/3853210864.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  oot_x[k[i]]=oot_x[k[i]].replace([np.inf,-np.inf],'-999')\n",
      "C:\\Users\\P-CEMR~1.KAS\\AppData\\Local\\Temp/ipykernel_10392/3853210864.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  oot_x[k[i]]=oot_x[k[i]].astype(str)\n"
     ]
    }
   ],
   "source": [
    "#test datası aynı işlemler:\n",
    "#objeler string 9999 ile, numerikler sayı 9999 ile doldurulur:\n",
    "for i in range(len(k)):\n",
    "    test_x[k[i]].fillna('-999',inplace=True)\n",
    "    test_x[k[i]]=test_x[k[i]].replace([np.nan],'-999')\n",
    "    test_x[k[i]]=test_x[k[i]].replace([np.inf,-np.inf],'-999')\n",
    "    test_x[k[i]]=test_x[k[i]].replace([np.inf,-np.inf],'-999')\n",
    "    test_x[k[i]]=test_x[k[i]].astype(str)\n",
    "\n",
    "test_x.fillna(-999,inplace=True)\n",
    "test_x=test_x.replace([np.nan],-999)\n",
    "test_x=test_x.replace([np.inf,-np.inf],-999)\n",
    "test_x=test_x.replace([np.inf,-np.inf],-999)\n",
    "\n",
    "#oot için aynı işlemler:\n",
    "for i in range(len(k)):\n",
    "    oot_x[k[i]].fillna('-999',inplace=True)\n",
    "    oot_x[k[i]]=oot_x[k[i]].replace([np.nan],'-999')\n",
    "    oot_x[k[i]]=oot_x[k[i]].replace([np.inf,-np.inf],'-999')\n",
    "    oot_x[k[i]]=oot_x[k[i]].replace([np.inf,-np.inf],'-999')\n",
    "    oot_x[k[i]]=oot_x[k[i]].astype(str)\n",
    "\n",
    "oot_x.fillna(-999,inplace=True)\n",
    "oot_x=oot_x.replace([np.nan],-999)\n",
    "oot_x=oot_x.replace([np.inf,-np.inf],-999)\n",
    "oot_x=oot_x.replace([np.inf,-np.inf],-999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time:  16:17:40\n"
     ]
    }
   ],
   "source": [
    "print(\"start time: \",datetime.now().strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Collinearity Elimination Progress: 60 / 61"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['index', 'contactcode', 'BusinessUnitCode', 'AssetManagementCode',\n",
       "       'ReachStatusCode', 'FinancialStatusCode', 'FirstReachDate_monthdiff',\n",
       "       'FollowingStartDate_monthdiff', 'Sensibility', 'IsTCCitizen',\n",
       "       ...\n",
       "       'LiveProtocolCount_max_l6m', 'FulledTotalProtocolAmount_max_l6m',\n",
       "       'TotalProtocolAmount_max_l6m', 'BrokeProtocolAmount_min_l12m',\n",
       "       'TotalProtocolAmount_min_l12m', 'ProtocolAmount_avg_l12m',\n",
       "       'CanceledProtocolAmount_avg_l12m', 'LiveProtocolCount_max_l12m',\n",
       "       'ActivatedProtocolAmount_max_l12m', 'LiveProtocolAmount_max_l12m'],\n",
       "      dtype='object', length=121)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#corr elemesi:\n",
    "\n",
    "elc = eliminate_group_correlation(train_x_new, train_y, corrmat=None, threshold=0.8, seed=100) #136\n",
    "train_x_new.drop(elc, axis=1, inplace=True)\n",
    "train_x_new.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end time:  16:34:52\n"
     ]
    }
   ],
   "source": [
    "print(\"end time: \",datetime.now().strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train_x_new.columns).to_excel('ticari_model_variables_'+str(datetime.today().strftime(\"%Y_%m_%d\"))+'.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>index</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>contactcode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>BusinessUnitCode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>AssetManagementCode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ReachStatusCode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>116</td>\n",
       "      <td>ProtocolAmount_avg_l12m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>117</td>\n",
       "      <td>CanceledProtocolAmount_avg_l12m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>118</td>\n",
       "      <td>LiveProtocolCount_max_l12m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>119</td>\n",
       "      <td>ActivatedProtocolAmount_max_l12m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>120</td>\n",
       "      <td>LiveProtocolAmount_max_l12m</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>121 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                 0\n",
       "0             0                             index\n",
       "1             1                       contactcode\n",
       "2             2                  BusinessUnitCode\n",
       "3             3               AssetManagementCode\n",
       "4             4                   ReachStatusCode\n",
       "..          ...                               ...\n",
       "116         116           ProtocolAmount_avg_l12m\n",
       "117         117   CanceledProtocolAmount_avg_l12m\n",
       "118         118        LiveProtocolCount_max_l12m\n",
       "119         119  ActivatedProtocolAmount_max_l12m\n",
       "120         120       LiveProtocolAmount_max_l12m\n",
       "\n",
       "[121 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_vars=pd.read_excel('ticari_model_variables_'+str(datetime.today().strftime(\"%Y_%m_%d\"))+'.xlsx')\n",
    "model_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>contactcode</th>\n",
       "      <th>BusinessUnitCode</th>\n",
       "      <th>AssetManagementCode</th>\n",
       "      <th>ReachStatusCode</th>\n",
       "      <th>FinancialStatusCode</th>\n",
       "      <th>FirstReachDate_monthdiff</th>\n",
       "      <th>FollowingStartDate_monthdiff</th>\n",
       "      <th>Sensibility</th>\n",
       "      <th>IsTCCitizen</th>\n",
       "      <th>...</th>\n",
       "      <th>LiveProtocolCount_max_l6m</th>\n",
       "      <th>FulledTotalProtocolAmount_max_l6m</th>\n",
       "      <th>TotalProtocolAmount_max_l6m</th>\n",
       "      <th>BrokeProtocolAmount_min_l12m</th>\n",
       "      <th>TotalProtocolAmount_min_l12m</th>\n",
       "      <th>ProtocolAmount_avg_l12m</th>\n",
       "      <th>CanceledProtocolAmount_avg_l12m</th>\n",
       "      <th>LiveProtocolCount_max_l12m</th>\n",
       "      <th>ActivatedProtocolAmount_max_l12m</th>\n",
       "      <th>LiveProtocolAmount_max_l12m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17020</th>\n",
       "      <td>17020</td>\n",
       "      <td>14583529</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-999</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37465</th>\n",
       "      <td>37465</td>\n",
       "      <td>11398561</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>3.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-999</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44875</th>\n",
       "      <td>44875</td>\n",
       "      <td>64813000</td>\n",
       "      <td>21</td>\n",
       "      <td>5580</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-999</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10118</th>\n",
       "      <td>10118</td>\n",
       "      <td>2204640</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-999</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43731</th>\n",
       "      <td>43731</td>\n",
       "      <td>373253</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-999</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index contactcode  BusinessUnitCode  AssetManagementCode  \\\n",
       "17020  17020    14583529                22                   23   \n",
       "37465  37465    11398561                21                   23   \n",
       "44875  44875    64813000                21                 5580   \n",
       "10118  10118     2204640                20                   23   \n",
       "43731  43731      373253                22                   23   \n",
       "\n",
       "       ReachStatusCode  FinancialStatusCode  FirstReachDate_monthdiff  \\\n",
       "17020              3.0                  1.0                      34.0   \n",
       "37465              3.0                 19.0                      12.0   \n",
       "44875              2.0                  1.0                      67.0   \n",
       "10118              1.0                  NaN                       NaN   \n",
       "43731              0.0                 19.0                     104.0   \n",
       "\n",
       "       FollowingStartDate_monthdiff  Sensibility  IsTCCitizen  ...  \\\n",
       "17020                           NaN          0.0         -999  ...   \n",
       "37465                           NaN          0.0         -999  ...   \n",
       "44875                           NaN          0.0         -999  ...   \n",
       "10118                           NaN          0.0         -999  ...   \n",
       "43731                           NaN          0.0         -999  ...   \n",
       "\n",
       "       LiveProtocolCount_max_l6m  FulledTotalProtocolAmount_max_l6m  \\\n",
       "17020                        NaN                                NaN   \n",
       "37465                        NaN                                NaN   \n",
       "44875                        NaN                                NaN   \n",
       "10118                        NaN                                NaN   \n",
       "43731                        NaN                                NaN   \n",
       "\n",
       "       TotalProtocolAmount_max_l6m  BrokeProtocolAmount_min_l12m  \\\n",
       "17020                          NaN                           NaN   \n",
       "37465                          NaN                           NaN   \n",
       "44875                          NaN                           NaN   \n",
       "10118                          NaN                           NaN   \n",
       "43731                          NaN                           NaN   \n",
       "\n",
       "       TotalProtocolAmount_min_l12m  ProtocolAmount_avg_l12m  \\\n",
       "17020                           NaN                      NaN   \n",
       "37465                           NaN                      NaN   \n",
       "44875                           NaN                      NaN   \n",
       "10118                           NaN                      NaN   \n",
       "43731                           NaN                      NaN   \n",
       "\n",
       "       CanceledProtocolAmount_avg_l12m  LiveProtocolCount_max_l12m  \\\n",
       "17020                              NaN                         NaN   \n",
       "37465                              NaN                         NaN   \n",
       "44875                              NaN                         NaN   \n",
       "10118                              NaN                         NaN   \n",
       "43731                              NaN                         NaN   \n",
       "\n",
       "       ActivatedProtocolAmount_max_l12m  LiveProtocolAmount_max_l12m  \n",
       "17020                               NaN                          NaN  \n",
       "37465                               NaN                          NaN  \n",
       "44875                               NaN                          NaN  \n",
       "10118                               NaN                          NaN  \n",
       "43731                               NaN                          NaN  \n",
       "\n",
       "[5 rows x 121 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x=train_x[model_vars[0]]\n",
    "train_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>contactcode</th>\n",
       "      <th>BusinessUnitCode</th>\n",
       "      <th>AssetManagementCode</th>\n",
       "      <th>ReachStatusCode</th>\n",
       "      <th>FinancialStatusCode</th>\n",
       "      <th>FirstReachDate_monthdiff</th>\n",
       "      <th>FollowingStartDate_monthdiff</th>\n",
       "      <th>Sensibility</th>\n",
       "      <th>IsTCCitizen</th>\n",
       "      <th>...</th>\n",
       "      <th>LiveProtocolCount_max_l6m</th>\n",
       "      <th>FulledTotalProtocolAmount_max_l6m</th>\n",
       "      <th>TotalProtocolAmount_max_l6m</th>\n",
       "      <th>BrokeProtocolAmount_min_l12m</th>\n",
       "      <th>TotalProtocolAmount_min_l12m</th>\n",
       "      <th>ProtocolAmount_avg_l12m</th>\n",
       "      <th>CanceledProtocolAmount_avg_l12m</th>\n",
       "      <th>LiveProtocolCount_max_l12m</th>\n",
       "      <th>ActivatedProtocolAmount_max_l12m</th>\n",
       "      <th>LiveProtocolAmount_max_l12m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17020</th>\n",
       "      <td>17020</td>\n",
       "      <td>14583529</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-999</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37465</th>\n",
       "      <td>37465</td>\n",
       "      <td>11398561</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>3.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-999</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44875</th>\n",
       "      <td>44875</td>\n",
       "      <td>64813000</td>\n",
       "      <td>21</td>\n",
       "      <td>5580</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-999</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10118</th>\n",
       "      <td>10118</td>\n",
       "      <td>2204640</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-999</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43731</th>\n",
       "      <td>43731</td>\n",
       "      <td>373253</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-999</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index contactcode  BusinessUnitCode  AssetManagementCode  \\\n",
       "17020  17020    14583529                22                   23   \n",
       "37465  37465    11398561                21                   23   \n",
       "44875  44875    64813000                21                 5580   \n",
       "10118  10118     2204640                20                   23   \n",
       "43731  43731      373253                22                   23   \n",
       "\n",
       "       ReachStatusCode  FinancialStatusCode  FirstReachDate_monthdiff  \\\n",
       "17020              3.0                  1.0                      34.0   \n",
       "37465              3.0                 19.0                      12.0   \n",
       "44875              2.0                  1.0                      67.0   \n",
       "10118              1.0                  NaN                       NaN   \n",
       "43731              0.0                 19.0                     104.0   \n",
       "\n",
       "       FollowingStartDate_monthdiff  Sensibility  IsTCCitizen  ...  \\\n",
       "17020                           NaN          0.0         -999  ...   \n",
       "37465                           NaN          0.0         -999  ...   \n",
       "44875                           NaN          0.0         -999  ...   \n",
       "10118                           NaN          0.0         -999  ...   \n",
       "43731                           NaN          0.0         -999  ...   \n",
       "\n",
       "       LiveProtocolCount_max_l6m  FulledTotalProtocolAmount_max_l6m  \\\n",
       "17020                        NaN                                NaN   \n",
       "37465                        NaN                                NaN   \n",
       "44875                        NaN                                NaN   \n",
       "10118                        NaN                                NaN   \n",
       "43731                        NaN                                NaN   \n",
       "\n",
       "       TotalProtocolAmount_max_l6m  BrokeProtocolAmount_min_l12m  \\\n",
       "17020                          NaN                           NaN   \n",
       "37465                          NaN                           NaN   \n",
       "44875                          NaN                           NaN   \n",
       "10118                          NaN                           NaN   \n",
       "43731                          NaN                           NaN   \n",
       "\n",
       "       TotalProtocolAmount_min_l12m  ProtocolAmount_avg_l12m  \\\n",
       "17020                           NaN                      NaN   \n",
       "37465                           NaN                      NaN   \n",
       "44875                           NaN                      NaN   \n",
       "10118                           NaN                      NaN   \n",
       "43731                           NaN                      NaN   \n",
       "\n",
       "       CanceledProtocolAmount_avg_l12m  LiveProtocolCount_max_l12m  \\\n",
       "17020                              NaN                         NaN   \n",
       "37465                              NaN                         NaN   \n",
       "44875                              NaN                         NaN   \n",
       "10118                              NaN                         NaN   \n",
       "43731                              NaN                         NaN   \n",
       "\n",
       "       ActivatedProtocolAmount_max_l12m  LiveProtocolAmount_max_l12m  \n",
       "17020                               NaN                          NaN  \n",
       "37465                               NaN                          NaN  \n",
       "44875                               NaN                          NaN  \n",
       "10118                               NaN                          NaN  \n",
       "43731                               NaN                          NaN  \n",
       "\n",
       "[5 rows x 121 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>Unique Number</th>\n",
       "      <th>Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>index</td>\n",
       "      <td>35629</td>\n",
       "      <td>[17020, 37465, 44875, 10118, 43731, 1049, 1627...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>contactcode</td>\n",
       "      <td>31279</td>\n",
       "      <td>[14583529, 11398561, 64813000, 2204640, 373253...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BusinessUnitCode</td>\n",
       "      <td>5</td>\n",
       "      <td>[22, 21, 20, 12, 29]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AssetManagementCode</td>\n",
       "      <td>64</td>\n",
       "      <td>[23, 5580, 5643, 718, 5688, 5776, 4899, 2035, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ReachStatusCode</td>\n",
       "      <td>5</td>\n",
       "      <td>[3.0, 2.0, 1.0, 0.0, 4.0, nan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>ProtocolAmount_avg_l12m</td>\n",
       "      <td>899</td>\n",
       "      <td>[nan, 20000.0, 40000.0, 25000.0, 7500.0, 6499....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>CanceledProtocolAmount_avg_l12m</td>\n",
       "      <td>554</td>\n",
       "      <td>[nan, 20000.0, 7500.0, 750.0, 500.07, 2471.64,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>LiveProtocolCount_max_l12m</td>\n",
       "      <td>8</td>\n",
       "      <td>[nan, 0.0, 4.0, 2.0, 1.0, 3.0, 5.0, 6.0, 9.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>ActivatedProtocolAmount_max_l12m</td>\n",
       "      <td>72</td>\n",
       "      <td>[nan, 26475.24, 11510.09, 18051.07, 2274.36, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>LiveProtocolAmount_max_l12m</td>\n",
       "      <td>200</td>\n",
       "      <td>[nan, 6499.98, 25642.0, 4999.0, 11499.26, 3500...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>121 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Features  Unique Number  \\\n",
       "0                               index          35629   \n",
       "1                         contactcode          31279   \n",
       "2                    BusinessUnitCode              5   \n",
       "3                 AssetManagementCode             64   \n",
       "4                     ReachStatusCode              5   \n",
       "..                                ...            ...   \n",
       "116           ProtocolAmount_avg_l12m            899   \n",
       "117   CanceledProtocolAmount_avg_l12m            554   \n",
       "118        LiveProtocolCount_max_l12m              8   \n",
       "119  ActivatedProtocolAmount_max_l12m             72   \n",
       "120       LiveProtocolAmount_max_l12m            200   \n",
       "\n",
       "                                                Values  \n",
       "0    [17020, 37465, 44875, 10118, 43731, 1049, 1627...  \n",
       "1    [14583529, 11398561, 64813000, 2204640, 373253...  \n",
       "2                                 [22, 21, 20, 12, 29]  \n",
       "3    [23, 5580, 5643, 718, 5688, 5776, 4899, 2035, ...  \n",
       "4                       [3.0, 2.0, 1.0, 0.0, 4.0, nan]  \n",
       "..                                                 ...  \n",
       "116  [nan, 20000.0, 40000.0, 25000.0, 7500.0, 6499....  \n",
       "117  [nan, 20000.0, 7500.0, 750.0, 500.07, 2471.64,...  \n",
       "118      [nan, 0.0, 4.0, 2.0, 1.0, 3.0, 5.0, 6.0, 9.0]  \n",
       "119  [nan, 26475.24, 11510.09, 18051.07, 2274.36, 2...  \n",
       "120  [nan, 6499.98, 25642.0, 4999.0, 11499.26, 3500...  \n",
       "\n",
       "[121 rows x 3 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#değişkenlerin içeriklerini gösterir:\n",
    "attFeatures2 = []\n",
    "for i in train_x.columns:\n",
    "    attFeatures2.append([i, train_x[i].nunique(), train_x[i].drop_duplicates().values])\n",
    "pd.DataFrame(attFeatures2, columns = ['Features', 'Unique Number', 'Values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(attFeatures2).to_excel('Ticari_train_col_Details_'+str(datetime.today().strftime(\"%Y_%m_%d\"))+'.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# aşağısı silinecek:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alttaki iki kolon için refdateten fark hesaplanacak\n",
    "\n",
    "\n",
    "#import datetime\n",
    "#date1=data['RefDate'][0]\n",
    "#date2=str(data['min_acqdate_ever'][0])\n",
    "#mdate1 = datetime.datetime.strptime(date1, \"%Y-%m-%d\").date()\n",
    "#rdate1 = datetime.datetime.strptime(date2, \"%Y-%m-%d\").date()\n",
    "#delta =  (mdate1 - rdate1).days\n",
    "#print (delta)\n",
    "#train_x['min_acqdate_ever']\n",
    "#train_x['min_acqdate_open']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elenecek değişkenleri listeler\n",
    "elimination_list = dc+ddc+elc\n",
    "elimination_list=pd.DataFrame(elimination_list)\n",
    "elimination_list['elimination'] = ''\n",
    "elimination_list['elimination'][:(len(dc)-1)]='Constant'\n",
    "elimination_list['elimination'][len(dc):(len(dc)+len(ddc)-1)]='Duplicate'\n",
    "elimination_list['elimination'][(len(dc)+len(ddc)):]='Correlation'\n",
    "\n",
    "#elenen değişkenler raporlanabilir\n",
    "elimination_list.to_excel('Ticari_elimination_list_'+str(datetime.today().strftime(\"%Y_%m_%d\"))+'.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elenmesi istenmeyen ama elenen değişkenler mauel olarak eklenir:\n",
    "\n",
    "#train_x = pd.merge(train_x_new,train_x[['APP_ID',\n",
    "#'RT_KKB_CC_O_TOT_LIM',\n",
    "#'RT_KKB_CC_MAX_O_C_LIM_L1Y',\n",
    "#'RT_CUST_TENURE',\n",
    "#'RT_CRE_TENURE',\n",
    "#'RT_EDUCATION',\n",
    "#'RT_MMZC_TOT_CASH_LIM_LM',\n",
    "#'RT_KKB_OD_O_MAX_LIM',\n",
    "#'RT_KKB_O_TOT_CC_AMT',\n",
    "#'VAR21',\n",
    "#'RT_KKB_CC_2ND_MAX_O_C_LIM_L1Y',\n",
    "#'RT_KKB_CC_O_TOT_RISK',\n",
    "#'RT_KKB_INST_O_TOT_GPL_AMT',\n",
    "#'RT_KKB_OD_O_TOT_RISK',\n",
    "#'RT_KKB_SCORE',\n",
    "#'VAR20',\n",
    "#'VAR22',\n",
    "#'VAR23',\n",
    "#'VAR31',\n",
    "#'VAR68',\n",
    "#'VAR95'                                        \n",
    "#]],on='APP_ID', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#id kolonunu silme:\n",
    "#train_x = train_x.drop(columns=['APP_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_x['IsSalaryDeduction']=np.where(train_x['IsSalaryDeduction']=='True',1,np.where(train_x['IsSalaryDeduction']=='False',0,-999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_x['IsTCCitizen']=np.where(train_x['IsTCCitizen']=='True',1,np.where(train_x['IsTCCitizen']=='False',0,-999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_x['IsSalaryDeduction']=np.where(test_x['IsSalaryDeduction']=='True',1,np.where(test_x['IsSalaryDeduction']=='False',0,-999))\n",
    "#oot_x['IsSalaryDeduction']=np.where(oot_x['IsSalaryDeduction']=='True',1,np.where(oot_x['IsSalaryDeduction']=='False',0,-999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_x['IsTCCitizen']=np.where(test_x['IsTCCitizen']=='True',1,np.where(test_x['IsTCCitizen']=='False',0,-999))\n",
    "#oot_x['IsTCCitizen']=np.where(oot_x['IsTCCitizen']=='True',1,np.where(oot_x['IsTCCitizen']=='False',0,-999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_x = train_x.drop(columns=['CityName','Region','BirthPlace'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_x = test_x.drop(columns=['CityName','Region','BirthPlace'])\n",
    "#oot_x = oot_x.drop(columns=['CityName','Region','BirthPlace'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_x['CityCode'] = train_x['CityCode'].astype(int)\n",
    "#test_x['CityCode'] = test_x['CityCode'].astype(int)\n",
    "#oot_x['CityCode'] = oot_x['CityCode'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eklenen kolonlar olduğu için yeni obje gelenleri listeler:\n",
    "k=[]\n",
    "for i in train_x.columns: \n",
    "    if(train_x[i].dtype == object):\n",
    "        k.append(i)\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#null replacement:\n",
    "\n",
    "for i in range(len(k)):\n",
    "    train_x[k[i]].fillna('-999',inplace=True)\n",
    "    train_x[k[i]]=train_x[k[i]].replace([np.nan],'-999')\n",
    "    train_x[k[i]]=train_x[k[i]].replace([np.inf,-np.inf],'-999')\n",
    "    train_x[k[i]]=train_x[k[i]].replace([np.inf,-np.inf],'-999')\n",
    "    train_x[k[i]]=train_x[k[i]].astype(str)\n",
    "\n",
    "train_x.fillna(-999,inplace=True)\n",
    "train_x=train_x.replace([np.nan],-999)\n",
    "train_x=train_x.replace([np.inf,-np.inf],-999)\n",
    "train_x=train_x.replace([np.inf,-np.inf],-999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding:\n",
    "labelEncoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obje değişkenler için _L suffix ekler ve label encoding yapar:\n",
    "for i in range(len(k)):\n",
    "    train_x[k[i]+'_L'] = labelEncoder.fit_transform(train_x[k[i]])\n",
    "    \n",
    "for i in range(len(k)):\n",
    "    test_x[k[i]+'_L'] = labelEncoder.fit_transform(test_x[k[i]])\n",
    "    \n",
    "for i in range(len(k)):\n",
    "    oot_x[k[i]+'_L'] = labelEncoder.fit_transform(oot_x[k[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obje kolonları siler:\n",
    "train_x.drop(k, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot encoding: \n",
    "#train_x = pd.get_dummies(train_x, columns=['RT_OCCUPATION','RT_SECTOR_TYPE_DESC','RT_WORKING_TYPE_DESC','RT_EMPLOYMENT_TYPE','RT_CUSTOMER_TYPE' ])    \n",
    "#test_x = pd.get_dummies(test_x, columns=['RT_OCCUPATION','RT_SECTOR_TYPE_DESC','RT_WORKING_TYPE_DESC','RT_EMPLOYMENT_TYPE','RT_CUSTOMER_TYPE' ]) \n",
    "#oot_x = pd.get_dummies(oot_x, columns=['RT_OCCUPATION','RT_SECTOR_TYPE_DESC','RT_WORKING_TYPE_DESC','RT_EMPLOYMENT_TYPE','RT_CUSTOMER_TYPE' ]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kolon isimlerini değiştirme:\n",
    "#train_x.rename(columns={'RT_SECTOR_TYPE_DESC_-9999.0':'RT_SECTOR_TYPE_DESC_9999',\n",
    "#                        'RT_WORKING_TYPE_DESC_-9999.0':'RT_WORKING_TYPE_DESC_9999'}, inplace = True) \n",
    "#test_x.rename(columns={'RT_SECTOR_TYPE_DESC_-9999.0':'RT_SECTOR_TYPE_DESC_9999.0',\n",
    "#                        'RT_WORKING_TYPE_DESC_-9999.0':'RT_WORKING_TYPE_DESC_9999'}, inplace = True) \n",
    "#oot_x.rename(columns={'RT_SECTOR_TYPE_DESC_-9999.0':'RT_SECTOR_TYPE_DESC_9999.0',\n",
    "#                        'RT_WORKING_TYPE_DESC_-9999.0':'RT_WORKING_TYPE_DESC_9999'}, inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kolon isimlerini listeler:\n",
    "pd.DataFrame(train_x.columns).to_excel('ticari_final_shortlist'+str(datetime.today().strftime(\"%Y_%m_%d\"))+'.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oot_x_new = oot_x[train_x.columns] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_x[['RT_EMPLOYMENT_TYPE_10.0', 'RT_EMPLOYMENT_TYPE_13.0', \n",
    "#        'RT_EMPLOYMENT_TYPE_31.0', 'RT_EMPLOYMENT_TYPE_33.0', \n",
    "#        'RT_EMPLOYMENT_TYPE_34.0', 'RT_EMPLOYMENT_TYPE_35.0', \n",
    "#        'RT_EMPLOYMENT_TYPE_36.0', 'RT_EMPLOYMENT_TYPE_48.0', \n",
    "#        'RT_EMPLOYMENT_TYPE_52.0', 'RT_EMPLOYMENT_TYPE_55.0']] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_new = test_x[train_x.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgs = [sk.ensemble.RandomForestRegressor(n_jobs=-1), sk.ensemble.GradientBoostingRegressor(),\n",
    "      XGBRegressor()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=train_x.drop(columns=['min_acqdate_open','min_acqdate_ever'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hangi model daha iyi sonuç verir: (metrik:MAE) (cross_val_score'un scoring değiştirilecek)\n",
    "nfolds = 2\n",
    "kf = KFold(n_splits=nfolds,random_state=0,shuffle=True)\n",
    "bestmae = 9999999999\n",
    "bestrg = \"\"\n",
    "for rg in rgs:\n",
    "    mae = (sk.model_selection.cross_val_score(rg,train_x,train_y,cv=kf,n_jobs=-1,scoring='neg_mean_absolute_error').mean())\n",
    "    print (str(rg) + ' ' + str(mae))\n",
    "    if  -mae < bestmae:\n",
    "        bestrg = rg\n",
    "        bestmae = -mae\n",
    "        \n",
    "\n",
    "print('***********************************************')\n",
    "print ('Best is... ' + str(bestrg) + ' ' + str(-bestmae) + str(pprint(bestrg.get_params())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rkareye göre : hangi model daha iyi sonuç verir: (metrik:MAE) (cross_val_score'un scoring değiştirilecek)\n",
    "nfolds = 2\n",
    "kf = KFold(n_splits=nfolds,random_state=0,shuffle=True)\n",
    "worst_r = -9999999999\n",
    "bestrg = \"\"\n",
    "for rg in rgs:\n",
    "    r_square = (sk.model_selection.cross_val_score(rg,train_x,train_y,cv=kf,n_jobs=-1,scoring='r2').mean())\n",
    "    print (str(rg) + ' ' + str(mae))\n",
    "    if  r_square > worst_r:\n",
    "        bestrg = rg\n",
    "        bestmae = r_square\n",
    "        \n",
    "\n",
    "print('***********************************************')\n",
    "print ('Best is... ' + str(bestrg) + ' ' + str(-bestmae) + str(pprint(bestrg.get_params())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#en iyi modeli fit etme\n",
    "model=bestrg.fit(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importanceları çıkartır:\n",
    "sorted(zip((model.feature_importances_)*100,train_x.columns),reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final modeli fit etme:\n",
    "#rg = XGBRegressor()\n",
    "#nfolds = 10\n",
    "#kf = KFold(n_splits=nfolds,random_state=0,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_xgb = rg.fit(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = (sk.model_selection.cross_val_score(rg,train_x,train_y,cv=kf,n_jobs=-1,scoring='neg_mean_absolute_error').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [5, 10, 30],\n",
    "    'max_features': ['auto'],\n",
    "    'min_samples_leaf': [100,200,300,500],\n",
    "    'min_samples_split': [2 ,4, 8],\n",
    "    'n_estimators': [100, 200, 300]\n",
    "}\n",
    "# Create a based model\n",
    "rf = RandomForestRegressor()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(train_x, train_y_new)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#değişecek gini:\n",
    "def evaluate(model, test_features, test_labels):\n",
    "    predictions = model.predict(test_features)\n",
    "    errors = abs(predictions - test_labels)\n",
    "    mape = 100 * np.mean(errors / test_labels)\n",
    "    accuracy = 100 - mape\n",
    "    print('Model Performance')\n",
    "    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "    print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "    \n",
    "    return accuracy\n",
    "base_model = RandomForestRegressor(n_estimators = 10, random_state = 42)\n",
    "base_model.fit(train_x, train_y_new)\n",
    "base_accuracy = evaluate(base_model, test_x[train_x.columns], test_y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "best_grid = grid_search.best_estimator_\n",
    "grid_accuracy = evaluate(best_grid, test_x[train_x.columns], test_y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#oot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_accuracy = evaluate(best_grid, oot_x_new, oot_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_accuracy = evaluate(best_grid, train_x, train_y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bundan sonrası yok:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_new = test_y['target_w_inf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfolds = 10\n",
    "kf = KFold(n_splits=nfolds,random_state=0,shuffle=True)\n",
    "bestmae = 9999999999\n",
    "bestrg = \"\"\n",
    "for rg in rgs:\n",
    "    mae = (sk.model_selection.cross_val_score(rg,test_x,test_y_new,cv=kf,n_jobs=-1,scoring='neg_mean_absolute_error').mean())\n",
    "    print (str(rg) + ' ' + str(mae))\n",
    "    if  -mae < bestmae:\n",
    "        bestrg = rg\n",
    "        bestmae = -mae\n",
    "        \n",
    "\n",
    "print('***********************************************')\n",
    "print ('Best is... ' + str(bestrg) + ' ' + str(-bestmae) + 'Parameters currently in use:\\n' + str(pprint(bestrg.get_params())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##OOT\n",
    "nfolds = 10\n",
    "kf = KFold(n_splits=nfolds,random_state=0,shuffle=True)\n",
    "bestmae = 9999999999\n",
    "bestrg = \"\"\n",
    "for rg in rgs:\n",
    "    mae = (sk.model_selection.cross_val_score(rg,oot_x_new,oot_y,cv=kf,n_jobs=-1,scoring='neg_mean_absolute_error').mean())\n",
    "    print (str(rg) + ' ' + str(mae))\n",
    "    if  -mae < bestmae:\n",
    "        bestrg = rg\n",
    "        bestmae = -mae\n",
    "        \n",
    "\n",
    "print('***********************************************')\n",
    "print ('Best is... ' + str(bestrg) + ' ' + str(-bestmae) + 'Parameters currently in use:\\n' + str(pprint(bestrg.get_params())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset\n",
    "\n",
    "# create pipeline\n",
    "rfe = RFE(estimator=RandomForestRegressor(n_jobs=-1), n_features_to_select=100, step=1) ##seçilecek feature adetini revize et(100,75,50)\n",
    "model = RandomForestRegressor()\n",
    "pipeline = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "# evaluate model\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(pipeline, train_x, train_y_new, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n",
    "# report performance\n",
    "print('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
    "\n",
    "rfe.support_\n",
    "# summarize all features\n",
    "#for i in range(train_x.shape[1]):\n",
    "#print('Column: %d, Selected %s, Rank: %.3f' % (i, rfe.support_[i], rfe.ranking_[i])) ##bunu kontrol et\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline\n",
    "rfe = RFE(estimator=RandomForestRegressor(), n_features_to_select=5)\n",
    "model = RandomForestRegressor()\n",
    "pipeline = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "# evaluate model\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(pipeline, train_x, train_y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n",
    "# report performance\n",
    "print('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
***
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc #garbage collector\n",
    "import sys\n",
    "import os\n",
    "from pandas.api.types import CategoricalDtype\n",
    "#from xgboost import XGBRegressor as xgb\n",
    "import xgboost as xgb\n",
    "from itertools import product\n",
    "from statistics import stdev \n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.backends.backend_pdf\n",
    "from datetime import date\n",
    "import pickle\n",
    "import pyodbc\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from datetime import date\n",
    "from xgboost import XGBRegressor\n",
    "from dateutil.relativedelta import *\n",
    "from pandas import ExcelWriter\n",
    "from datetime import datetime\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn2pmml import sklearn2pmml\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn2pmml import PMMLPipeline\n",
    "from sklearn2pmml import Pipeline\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn2pmml.decoration import CategoricalDomain, ContinuousDomain\n",
    "from sklearn.preprocessing import OneHotEncoder, PolynomialFeatures, StandardScaler\n",
    "from sklearn2pmml.preprocessing.xgboost import make_xgboost_column_transformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn2pmml.preprocessing import PMMLLabelEncoder\n",
    "from sklearn2pmml import make_pmml_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "sns.set()\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############################\n",
    "##### Helper Functions #####\n",
    "############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_na_report(df):\n",
    "    \"\"\"\n",
    "    create a report of variables whose values are NA across all table, and eliminate them\n",
    "    \"\"\"\n",
    "    print(\"\\nDataset has {} entries and {} features\".format(*df.shape))\n",
    "    na_report = df.isna().sum()                          #create a list of columns and sum of null values\n",
    "    na_report = pd.DataFrame(na_report, index=None)     #convert list to a dataframe\n",
    "    na_report.reset_index(inplace=True)                  #reset index to range(0 : len(df)-1)\n",
    "    na_report.columns = [\"variable\", \"na_count\"]        #set column names\n",
    "    na_report[\"perc\"] = na_report[\"na_count\"]/df.shape[0]           #add a new column which is percentage of null vales by column name\n",
    "    gc.collect()                                                #garbage collecter for memory efficiency\n",
    "    return na_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dtype_report(df):\n",
    "    \"\"\"\n",
    "    define the following reports:\n",
    "    num_cols: indicating which variables are numeric\n",
    "    char_cols: indicating which variables are of type character\n",
    "    \"\"\"\n",
    "    df.replace({True: 1, False: 0}, inplace=True)\n",
    "    df_cols = df.columns\n",
    "    num_cols = list(df._get_numeric_data().columns)\n",
    "    char_cols = list(set(df_cols) - set(num_cols))\n",
    "    gc.collect()\n",
    "    return num_cols, char_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminate_missing(df, na_report, max_threshold):\n",
    "    ''' \n",
    "    # define maximum threshold of missing percentage\n",
    "    # eliminate columns that do not fit the threshold\n",
    "    '''\n",
    "    cols_before = df.columns.tolist()\n",
    "    cols_ = df.columns[df.columns.isin(list(na_report[na_report['perc'] < max_threshold]['variable']) or df.columns.isin(list(key_var)))]\n",
    "    df = df[cols_]\n",
    "    print(len(cols_before)-len(cols_),\"columns eliminated due to missing values\")\n",
    "    del cols_, cols_before\n",
    "    gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminate_single_unique(df):\n",
    "    ''' \n",
    "    # eliminate columns having a single unique value \n",
    "    '''\n",
    "    cols_before = df.columns.tolist()\n",
    "    unique_report = df.nunique()                          \n",
    "    unique_report = pd.DataFrame(unique_report, index=None)    \n",
    "    unique_report.reset_index(inplace=True)\n",
    "    unique_report.columns = [\"variable\", \"number_unique\"]        #set column names\n",
    "    unique_report = unique_report[~unique_report['variable'].isin(key_var)]  #discard key variables  \n",
    "    u_cols = df.columns[~df.columns.isin(list(unique_report[unique_report['number_unique'] == 1]['variable']))]\n",
    "    df = df[u_cols]\n",
    "    print(len(cols_before)-len(u_cols),\"columns eliminated due to single unique values, namely:\",list(set(cols_before) - set(u_cols)))\n",
    "    del unique_report, u_cols, cols_before\n",
    "    gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoding_matrix(df):\n",
    "    \"\"\"\n",
    "    create a matrix (or dataframe) with 2 columns:\n",
    "    variable: categorical columns of dataframe\n",
    "    levels: levels of unique values in categorical columns (including missing values)\n",
    "    \"\"\"\n",
    "    missing_string = \"MISSING\"\n",
    "    df_temp = df.copy()\n",
    "    cols = list(set(list(df_temp.select_dtypes(include=['category', 'object']).columns)) - set(key_vars))\n",
    "    assert len(cols) == len(set(cols)) , \"please ensure that you are using unique column names.\"\n",
    "    enc_df = pd.DataFrame(columns = [\"variable\", \"levels\"])\n",
    "    for i in cols:\n",
    "        if (pd.api.types.is_categorical_dtype(df_temp[i])):\n",
    "            df_temp[i] = df_temp[i].cat.add_categories([missing_string])       \n",
    "        df_temp[i] = df_temp[i].fillna(value=missing_string)\n",
    "        colLevels = list(df_temp[i].unique())\n",
    "        if missing_string not in colLevels:\n",
    "           colLevels.append(missing_string)\n",
    "        enc_df = enc_df.append({'variable' : i , 'levels' : colLevels} , ignore_index=True)\n",
    "    del df_temp, cols\n",
    "    gc.collect()\n",
    "    return enc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_from_matrix(df, enc_df):\n",
    "    \"\"\"\n",
    "    transforms categorical columns into numeric values based on encoded matrix\n",
    "    enc_df: result of create_encoding_matrix() function composed of 2 columns:\n",
    "        variable: categorical columns of dataframe\n",
    "        levels: levels of unique values in categorical columns (including missing values)\n",
    "    \"\"\"\n",
    "    missing_string = \"MISSING\"\n",
    "    df_temp = df.copy()\n",
    "    matrix_cols = list(enc_df['variable'].values)\n",
    "    for i in matrix_cols:\n",
    "        if (pd.api.types.is_categorical_dtype(df_temp[i])):\n",
    "            df_temp[i] = df_temp[i].cat.add_categories([missing_string])\n",
    "        df_temp[i] = df_temp[i].fillna(value=missing_string)\n",
    "        df_temp[i] = df_temp[i].astype('category')\n",
    "        temp_levels = list(enc_df[enc_df['variable']==i]['levels'])[0]\n",
    "        df_temp[i] = pd.Categorical(df_temp[i], categories=temp_levels).codes +1\n",
    "        del temp_levels\n",
    "    del matrix_cols\n",
    "    gc.collect()\n",
    "    return df_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelleme asamalari için gerekli fonksiyonlar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_mae_and_mape(df, sample='train', pred_name='predicted'):\n",
    "    temp = df.copy()\n",
    "    mae_ = mean_absolute_error(temp.loc[temp['sample'] == sample]['actual'], temp.loc[temp['sample'] == sample][pred_name])   \n",
    "    mape_ = np.mean(np.abs((temp.loc[temp['sample'] == sample]['actual']-temp.loc[temp['sample'] == sample][pred_name])/temp.loc[temp['sample'] == sample]['actual'])) \n",
    "    del temp\n",
    "    gc.collect()\n",
    "    return mae_,mape_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_mae_and_rmse(df, sample='train', pred_name='predicted'):\n",
    "    temp = df.copy()\n",
    "    mae_ = mean_absolute_error(temp.loc[temp['sample'] == sample]['actual'], temp.loc[temp['sample'] == sample][pred_name])   \n",
    "    rmse_ = sqrt(mean_squared_error(temp.loc[temp['sample'] == sample]['actual'], temp.loc[temp['sample'] == sample][pred_name]))\n",
    "    del temp\n",
    "    gc.collect()\n",
    "    return mae_,rmse_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_grid(dictionary):\n",
    "   \"\"\"Create a dataframe from every combination of given values.\"\"\"\n",
    "   return pd.DataFrame([row for row in product(*dictionary.values())],\n",
    "                       columns=dictionary.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_split_index(df, splitting_perc, seed, min_date):\n",
    "    # allocate Development, and OOT validation data\n",
    "    oot_index = df.loc[df['refdate'] >= min_date].index.values\n",
    "    dev_index = df.loc[df['refdate'] < min_date].index.values  \n",
    "    \n",
    "    y_all = df['target'].iloc[dev_index]\n",
    "    \n",
    "    train_index, test_index, y_train, y_test = train_test_split(dev_index, y_all, test_size=splitting_perc, random_state=seed)   #, stratify=y_all çikarildi\n",
    "    print(\"\\nShape of dataframes; train: {}, test: {}, oot: {}\".format(train_index.shape,test_index.shape,oot_index.shape))\n",
    "    print(\"\\nPercentage of dataframes; train: {:.2%}, test: {:.2%}, oot: {:.2%}\".format(len(train_index)/len(df),len(test_index)/len(df),len(oot_index)/len(df)))\n",
    "    return train_index, test_index, oot_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df_by_index(df, key_var, train_index, test_index, oot_index):\n",
    "    # first get TARGET arrays\n",
    "    y_train, y_test, y_oot = df['target'].iloc[train_index], df['target'].iloc[test_index], df['target'].iloc[oot_index]\n",
    "    \n",
    "    # get key values apart including TARGET\n",
    "    cols_ = df.columns[df.columns.isin(list(key_var))] #including TARGET\n",
    "    df_key_var = df[cols_]  #save key values\n",
    "    df_n = df.drop(cols_, axis=1)\n",
    "    \n",
    "    # set dataframes\n",
    "    X_train, X_test, X_oot = df_n.iloc[train_index], df_n.iloc[test_index], df_n.iloc[oot_index]\n",
    "    train_keys, test_keys, oot_keys = df_key_var.iloc[train_index], df_key_var.iloc[test_index], df_key_var.iloc[oot_index]\n",
    "    \n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "    doot = xgb.DMatrix(X_oot, label=y_oot)\n",
    "    del df_n\n",
    "    gc.collect()\n",
    "    return dtrain, dtest, doot, y_train, y_test, y_oot, train_keys, test_keys, oot_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_performance_measurement_results(df, title):\n",
    "    df_sum = df.copy()\n",
    "    \n",
    "    divide_by_score = np.arange(0, 100001, 1000)\n",
    "    divide_by_cnt = 10\n",
    "   \n",
    "    df_sum['score_grp'] = pd.cut(df_sum['predicted'], divide_by_score).astype('category')\n",
    "    temp_score = df_sum.groupby(\"score_grp\")[\"actual\", \"predicted\", \"count\",\"score_grp\"].agg(\"sum\").reset_index() #, observed=True\n",
    "    temp_score=temp_score[temp_score['count']!=0]\n",
    "    temp_score.index = np.arange(1, len(temp_score) + 1)\n",
    "    \n",
    "    df_sum['row'] = np.arange(1, df_sum.shape[0]+1)\n",
    "    df_sum['row_grp'] = pd.cut(df_sum['row'], divide_by_cnt).astype('category')\n",
    "    temp_row = df_sum.groupby(\"row_grp\")[\"actual\", \"predicted\", \"count\",\"score_grp\"].agg(\"sum\").reset_index() #, observed=True \n",
    "    temp_row=temp_row[temp_row['count']!=0]\n",
    "    temp_row.index = np.arange(1, len(temp_row) + 1)\n",
    "    \n",
    "    temp_score['act_avg_pd'] = temp_score[\"actual\"]/temp_score[\"count\"]\n",
    "    temp_score['pred_avg_pd'] = temp_score[\"predicted\"]/temp_score[\"count\"]\n",
    "    temp_score['dist'] = temp_score[\"count\"]/df_sum.shape[0]\n",
    "    \n",
    "    temp_row['act_avg_pd'] = temp_row[\"actual\"]/temp_row[\"count\"]\n",
    "    temp_row['pred_avg_pd'] = temp_row[\"predicted\"]/temp_row[\"count\"]\n",
    "    temp_row['dist'] = temp_row[\"count\"]/df_sum.shape[0]\n",
    "    \n",
    "    fig_score = plt.figure()\n",
    "    plt.plot(temp_score[\"act_avg_pd\"], '-r')  # red\n",
    "    plt.plot(temp_score[\"pred_avg_pd\"], '--b')  # blue\n",
    "    #plt.plot(temp_row[\"count\"],marker=\".\",linestyle=\"\",color=\"tan\")\n",
    "    plt.legend(['Actual', 'Predicted'], loc='upper right', shadow=True)\n",
    "    plt.title(title+\" - Average PD by Score\")\n",
    "    plt.xlabel(\"Score bins\")\n",
    "    plt.ylabel(\"pd\")\n",
    "    plt.close()\n",
    "    \n",
    "    fig_row = plt.figure()\n",
    "    plt.plot(temp_row[\"act_avg_pd\"], '-r')  # red\n",
    "    plt.plot(temp_row[\"pred_avg_pd\"], '--b')  # blue\n",
    "    plt.legend(['Actual', 'Predicted'], loc='upper right', shadow=True)\n",
    "    plt.title(title+\" - Average PD by Population Bins\")\n",
    "    plt.xlabel(\"Population bins\")\n",
    "    plt.ylabel(\"pd\")\n",
    "    plt.close()\n",
    "    del df_sum\n",
    "    gc.collect()\n",
    "    return temp_score, temp_row, fig_score, fig_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gridsearch kapalıysa buradaki parametrelerle aşağıdaki akış çalışır:\n",
    "def set_params(seed): \n",
    "    params = {\n",
    "    # Parameters that we are going to tune.\n",
    "    'eta' : 0.01,\n",
    "    'max_depth': 15, #15\n",
    "    'min_child_weight': 250, #75\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 1,\n",
    "    'gamma' : 40,\n",
    "    'reg_alpha' :0.1,\n",
    "    #'scale_pos_weight':  5,\n",
    "    # Other parameters\n",
    "    'objective': \"reg:squarederror\",\n",
    "    'eval_metric' : \"mae\",\n",
    "    'booster' : \"gbtree\",\n",
    "    'seed' : seed\n",
    "    }\n",
    "    return params \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "################################\n",
    "### Part 6 - Run GRID search ###\n",
    "################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_grid_search(df, key_var, grid_params, splitting_perc, min_date, seed, random_search, diff_threshold, grid_size):\n",
    "        \n",
    "    train_index, test_index, oot_index = set_split_index(df, splitting_perc, seed, min_date)\n",
    "    dtrain, dtest, doot, y_train, y_test, y_oot, train_keys, test_keys, oot_keys = split_df_by_index(df, key_var, train_index, test_index, oot_index)\n",
    "    \n",
    "    evallist = [(dtrain, 'train'),(dtest, 'eval')]\n",
    "    \n",
    "    grid_params = expand_grid(grid_params)\n",
    "    \n",
    "    if random_search:\n",
    "        grid_params = grid_params.sample(n=grid_size, replace = False, random_state=seed)\n",
    "        \n",
    "    xgb_gs = pd.DataFrame() #create xgboost grid search dataframe\n",
    "    \n",
    "    iter_count = 0\n",
    "    for i in range(grid_params.shape[0]):\n",
    "        params = {\n",
    "            'eta' : grid_params['eta'].iloc[i],\n",
    "            'max_depth': grid_params['max_depth'].iloc[i],\n",
    "            'min_child_weight': grid_params['min_child_weight'].iloc[i],\n",
    "            'subsample': grid_params['subsample'].iloc[i],\n",
    "            'colsample_bytree': grid_params['colsample_bytree'].iloc[i],\n",
    "            'gamma' : grid_params['gamma'].iloc[i],  # [10, 20, 30, 40]\n",
    "            'reg_alpha' : grid_params['reg_alpha'].iloc[i],\n",
    "            # Other parameters\n",
    "            'objective': grid_params['objective'].iloc[i],\n",
    "            'eval_metric' : grid_params['eval_metric'].iloc[i],\n",
    "            'booster' : grid_params['booster'].iloc[i],\n",
    "            'seed' : seed           \n",
    "            }\n",
    "        iter_count = iter_count+1\n",
    "        print('\\niteration:', iter_count)\n",
    "        bst = xgb.train(params=params, dtrain=dtrain, num_boost_round=500, evals=evallist, early_stopping_rounds=20, verbose_eval=50 )\n",
    "        \n",
    "        pred_train = bst.predict(dtrain, ntree_limit=bst.best_ntree_limit)\n",
    "        pred_test = bst.predict(dtest, ntree_limit=bst.best_ntree_limit) \n",
    "        pred_oot = bst.predict(doot, ntree_limit=bst.best_ntree_limit)\n",
    "        \n",
    "        train_df = pd.DataFrame({'sample': 'train', \n",
    "                                  'actual': y_train, \n",
    "                                  'predicted' : pred_train\n",
    "                                #  'FM_CUSTOMER_ID':, \n",
    "                                #  'REPORT_DATE': , \n",
    "                                  })\n",
    "        test_df = pd.DataFrame({'sample': 'test', \n",
    "                                  'actual': y_test, \n",
    "                                  'predicted' : pred_test\n",
    "                                #  'FM_CUSTOMER_ID':, \n",
    "                                #  'REPORT_DATE': , \n",
    "                                  })\n",
    "        oot_df = pd.DataFrame({'sample': 'oot', \n",
    "                                  'actual': y_oot, \n",
    "                                  'predicted' : pred_oot\n",
    "                                #  'FM_CUSTOMER_ID':, \n",
    "                                #  'REPORT_DATE': , \n",
    "                                  })\n",
    "        predictions = train_df.append([test_df, oot_df])\n",
    "        ##predictions[['actual', 'predicted']] = predictions[['actual', 'predicted']].apply(lambda x: round(100*x, 2))\n",
    "\n",
    "        mae_train, rmse_train = cal_mae_and_rmse(predictions, sample='train',pred_name='predicted')\n",
    "        mae_test, rmse_test = cal_mae_and_rmse(predictions, sample='test',pred_name='predicted')\n",
    "        mae_oot, rmse_oot = cal_mae_and_rmse(predictions, sample='oot', pred_name='predicted')\n",
    "        \n",
    "        perf_sum = predictions.groupby(\"sample\").agg(\"mean\").reset_index()\n",
    "        #perf_sum[['actual', 'predicted']] = perf_sum[['actual', 'predicted']].apply(lambda x: round(100*x, 2))\n",
    "        \n",
    "        perf_sum['mae'] = 0\n",
    "        perf_sum.loc[perf_sum['sample'] == 'train', 'mae'] = mae_train\n",
    "        perf_sum.loc[perf_sum['sample'] == 'test', 'mae'] = mae_test\n",
    "        perf_sum.loc[perf_sum['sample'] == 'oot', 'mae'] = mae_oot\n",
    "        \n",
    "        perf_sum['rmse'] = 0\n",
    "        perf_sum.loc[perf_sum['sample'] == 'train', 'rmse'] = rmse_train\n",
    "        perf_sum.loc[perf_sum['sample'] == 'test', 'rmse'] = rmse_test\n",
    "        perf_sum.loc[perf_sum['sample'] == 'oot', 'rmse'] = rmse_oot\n",
    "\n",
    "        \n",
    "        perf_sum['eta'] = grid_params['eta'].iloc[i]\n",
    "        perf_sum['max_depth'] = grid_params['max_depth'].iloc[i]\n",
    "        perf_sum['min_child_weight'] = grid_params['min_child_weight'].iloc[i]\n",
    "        perf_sum['subsample'] = grid_params['subsample'].iloc[i]\n",
    "        perf_sum['colsample_bytree'] = grid_params['colsample_bytree'].iloc[i]\n",
    "        perf_sum['gamma'] = grid_params['gamma'].iloc[i]\n",
    "        perf_sum['reg_alpha'] = grid_params['reg_alpha'].iloc[i]\n",
    "        perf_sum['objective'] = grid_params['objective'].iloc[i]\n",
    "        perf_sum['eval_metric'] = grid_params['eval_metric'].iloc[i]\n",
    "        perf_sum['booster'] = grid_params['booster'].iloc[i]\n",
    "        perf_sum['grid_iter'] = iter_count\n",
    "        \n",
    "        xgb_gs = xgb_gs.append(perf_sum, ignore_index=True)\n",
    "        \n",
    "        del rmse_train, rmse_test, rmse_oot, mae_train, mae_test, mae_oot, bst, predictions, train_df, test_df, oot_df, pred_train, pred_test, pred_oot\n",
    "        gc.collect()\n",
    "        \n",
    "    print(\"\\n----selecting best grid iteration----\")\n",
    "    #select best grid results\n",
    "    xgb_gs_train = xgb_gs.loc[xgb_gs['sample'] == \"train\"]\n",
    "    xgb_gs_train = xgb_gs_train.sort_values('mae', ascending=False)\n",
    "    \n",
    "    best_gs_iter = np.nan\n",
    "    xgb_gs_train['trainMae'] = np.nan\n",
    "    xgb_gs_train['testMae'] = np.nan\n",
    "    xgb_gs_train['ootMae'] = np.nan\n",
    "    xgb_gs_train['testDiff'] = np.nan\n",
    "    xgb_gs_train['ootDiff'] = np.nan\n",
    "    xgb_gs_train['avgDiff'] = np.nan\n",
    "    \n",
    "    for i in xgb_gs_train.index:\n",
    "        iter_ = xgb_gs_train.loc[i, 'grid_iter']\n",
    "            \n",
    "        train_mae = xgb_gs.loc[(xgb_gs['grid_iter'] == iter_) & (xgb_gs['sample']=='train'), 'mae'].iloc[0]\n",
    "        test_mae = xgb_gs.loc[(xgb_gs['grid_iter'] == iter_) & (xgb_gs['sample']=='test'), 'mae'].iloc[0]\n",
    "        oot_mae = xgb_gs.loc[(xgb_gs['grid_iter'] == iter_) & (xgb_gs['sample']=='oot'), 'mae'].iloc[0]    \n",
    "        \n",
    "        xgb_gs_train.loc[i, 'trainMae'] = train_mae\n",
    "        xgb_gs_train.loc[i, 'testMae'] = test_mae\n",
    "        xgb_gs_train.loc[i, 'ootMae'] = oot_mae\n",
    "        xgb_gs_train.loc[i, 'testDiff'] = abs(train_mae - test_mae)\n",
    "        xgb_gs_train.loc[i, 'ootDiff'] = abs(train_mae - oot_mae)\n",
    "        xgb_gs_train.loc[i, 'avgDiff'] = (abs(train_mae - test_mae) + abs(train_mae - oot_mae))/2\n",
    "\n",
    "    del i, iter_, train_mae, test_mae, oot_mae\n",
    "    gc.collect()\n",
    "\n",
    "    xgb_gs_train_base = xgb_gs_train.copy()\n",
    "    xgb_gs_train = xgb_gs_train.loc[xgb_gs_train['mae'] > 0].copy()\n",
    "    minAvgDiff = xgb_gs_train['avgDiff'].min()\n",
    "\n",
    "    if minAvgDiff <= diff_threshold:\n",
    "        xgb_gs_train_f = xgb_gs_train.loc[xgb_gs_train['avgDiff'] <= diff_threshold].copy()\n",
    "        min_train_mae = xgb_gs_train_f['trainMae'].min()\n",
    "        \n",
    "        xgb_gs_train_f['StdDev'] = np.nan\n",
    "        xgb_gs_train_f['trainMaePrevious'] = min_train_mae\n",
    "        xgb_gs_train_f['trainMaePreviousDiff'] = np.nan\n",
    "        xgb_gs_train_f['finalEvaluationScore'] = np.nan\n",
    "        \n",
    "        for i in xgb_gs_train_f.index: \n",
    "            train_mae = xgb_gs_train_f.loc[i, 'trainMae']\n",
    "            test_mae = xgb_gs_train_f.loc[i, 'testMae']\n",
    "            oot_mae = xgb_gs_train_f.loc[i, 'ootMae']\n",
    "            xgb_gs_train_f.loc[i, 'StdDev'] = stdev([train_mae,test_mae,oot_mae])\n",
    "            xgb_gs_train_f.loc[i, 'trainMaePreviousDiff'] = train_mae - min_train_mae\n",
    "            xgb_gs_train_f.loc[i, 'finalEvaluationScore'] = ((0.50 * xgb_gs_train_f.loc[i, 'trainMaePreviousDiff']) + (0.20 * xgb_gs_train_f.loc[i, 'avgDiff']) + (0.3 * xgb_gs_train_f.loc[i, 'StdDev']))\n",
    "            \n",
    "            \n",
    "        xgb_gs_train = pd.merge(xgb_gs_train,xgb_gs_train_f[['grid_iter','StdDev', 'trainMaePrevious', 'trainMaePreviousDiff','finalEvaluationScore']],on='grid_iter', how='left')\n",
    "        min_final_eva_score = copy.copy(xgb_gs_train_f['finalEvaluationScore'].min())\n",
    "        best_gs_iter = xgb_gs_train_f.loc[xgb_gs_train_f['finalEvaluationScore'] <= min_final_eva_score, 'grid_iter'].iloc[0]\n",
    "        xgb_gs_train = pd.merge(xgb_gs_train_base,xgb_gs_train[['grid_iter','StdDev', 'trainMaePrevious', 'trainMaePreviousDiff','finalEvaluationScore']],on='grid_iter', how='left')\n",
    "        del i, train_mae, test_mae, oot_mae\n",
    "        gc.collect()\n",
    "    else:\n",
    "        min_diff = xgb_gs_train['avgDiff'].min()\n",
    "        min_diff_mae = xgb_gs_train.loc[xgb_gs_train['avgDiff'] <= minAvgDiff, 'trainMae'].iloc[0]\n",
    "        for i in xgb_gs_train.index:\n",
    "            #iter_ = j['grid_iter']\n",
    "            diff_increase = xgb_gs_train.loc[i, 'avgDiff'] - min_diff\n",
    "            mae_decrease = xgb_gs_train.loc[i, 'trainMae'] - min_diff_mae\n",
    "            xgb_gs_train.loc[i, 'finalEvaluationScore'] = 5*diff_increase - mae_decrease\n",
    "        \n",
    "        min_final_eva_score = xgb_gs_train['finalEvaluationScore'].min()\n",
    "        best_gs_iter = xgb_gs_train.loc[xgb_gs_train['finalEvaluationScore'] <= min_final_eva_score, 'grid_iter'].iloc[0]\n",
    "        xgb_gs_train = xgb_gs_train.reset_index(drop=True)\n",
    "        xgb_gs_train = pd.merge(xgb_gs_train_base,xgb_gs_train[['grid_iter','finalEvaluationScore']],on='grid_iter', how='left')\n",
    "        del i, diff_increase, mae_decrease, min_diff, min_diff_mae\n",
    "        gc.collect()\n",
    "    print(\"best grid iteration:\", best_gs_iter)\n",
    "    \n",
    "    params = {\n",
    "    # Parameters that we are going to tune.\n",
    "    'eta' : xgb_gs_train.loc[xgb_gs_train['grid_iter'] == best_gs_iter,'eta'].iloc[0],\n",
    "    'max_depth': xgb_gs_train.loc[xgb_gs_train['grid_iter'] == best_gs_iter,'max_depth'].iloc[0],\n",
    "    'min_child_weight': xgb_gs_train.loc[xgb_gs_train['grid_iter'] == best_gs_iter,'min_child_weight'].iloc[0],\n",
    "    'subsample': xgb_gs_train.loc[xgb_gs_train['grid_iter'] == best_gs_iter,'subsample'].iloc[0],\n",
    "    'colsample_bytree': xgb_gs_train.loc[xgb_gs_train['grid_iter'] == best_gs_iter,'colsample_bytree'].iloc[0],\n",
    "    'gamma' : xgb_gs_train.loc[xgb_gs_train['grid_iter'] == best_gs_iter,'gamma'].iloc[0],\n",
    "    'reg_alpha' : xgb_gs_train.loc[xgb_gs_train['grid_iter'] == best_gs_iter,'reg_alpha'].iloc[0],\n",
    "    # Other parameters\n",
    "    'objective': xgb_gs_train.loc[xgb_gs_train['grid_iter'] == best_gs_iter,'objective'].iloc[0],\n",
    "    'eval_metric' : xgb_gs_train.loc[xgb_gs_train['grid_iter'] == best_gs_iter,'eval_metric'].iloc[0],\n",
    "    'booster' : xgb_gs_train.loc[xgb_gs_train['grid_iter'] == best_gs_iter,'booster'].iloc[0],\n",
    "    'seed' : seed\n",
    "    }\n",
    "    xgb_gs_train[['actual', 'predicted']] = xgb_gs_train[['actual', 'predicted']].apply(lambda x: round(x, 2))\n",
    "\n",
    "    return xgb_gs_train, best_gs_iter, params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################\n",
    "### Part 7 - Run final model with the best grid parameters ###\n",
    "###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_final_model(df, key_var, params, splitting_perc, seed, num_boost_round, early_stopping_rounds, verbose_eval):\n",
    "    print(\"\\n----final model----\")\n",
    "    train_index, test_index, oot_index = set_split_index(df, splitting_perc, seed, min_date)\n",
    "    dtrain, dtest, doot, y_train, y_test, y_oot, train_keys, test_keys, oot_keys = split_df_by_index(df, key_var, train_index, test_index, oot_index)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    evallist = [(dtrain, 'train'),(dtest, 'eval')]\n",
    "    \n",
    "    bst = xgb.train(params=params, dtrain=dtrain, num_boost_round=num_boost_round, evals=evallist, early_stopping_rounds=early_stopping_rounds, verbose_eval=verbose_eval )\n",
    "    \n",
    "    pred_train = bst.predict(dtrain, ntree_limit=bst.best_ntree_limit)\n",
    "    pred_test = bst.predict(dtest, ntree_limit=bst.best_ntree_limit) \n",
    "    pred_oot = bst.predict(doot, ntree_limit=bst.best_ntree_limit)\n",
    "    \n",
    "    train_df = pd.DataFrame({'sample': 'train', \n",
    "                              'actual': y_train, \n",
    "                              'predicted' : pred_train,\n",
    "                              'contactcode': train_keys[\"contactcode\"], \n",
    "                              'refdate': train_keys[\"refdate\"]\n",
    "                              #'APP_ID': train_keys[\"APP_ID\"]\n",
    "                              })\n",
    "    \n",
    "    test_df = pd.DataFrame({'sample': 'test', \n",
    "                             'actual': y_test, \n",
    "                             'predicted' : pred_test,\n",
    "                              'contactcode': test_keys[\"contactcode\"], \n",
    "                              'refdate': test_keys[\"refdate\"]\n",
    "                             #'APP_ID': test_keys[\"APP_ID\"]\n",
    "                              })\n",
    "    oot_df = pd.DataFrame({'sample': 'oot', \n",
    "                            'actual': y_oot, \n",
    "                            'predicted' : pred_oot,\n",
    "                              'contactcode': oot_keys[\"contactcode\"], \n",
    "                              'refdate': oot_keys[\"refdate\"]\n",
    "                             #'APP_ID': oot_keys[\"APP_ID\"]\n",
    "                              })\n",
    "    predictions = train_df.append([test_df, oot_df])\n",
    "    del train_df, test_df, oot_df, pred_train, pred_test, pred_oot\n",
    "    gc.collect()\n",
    "    perf_sum = predictions.groupby(\"sample\").agg(\"mean\").reset_index()#.drop(columns='contactcode')\n",
    "    #perf_sum[['actual', 'predicted']] = perf_sum[['actual', 'predicted']].apply(lambda x: round(100*x, 2))\n",
    "    predictions.loc[:,'count'] = 1\n",
    "    \n",
    "    mae_train,rmse_train = cal_mae_and_rmse(predictions, sample='train', pred_name='predicted')\n",
    "    mae_test, rmse_test = cal_mae_and_rmse(predictions, sample='test', pred_name='predicted')\n",
    "    mae_oot, rmse_oot = cal_mae_and_rmse(predictions, sample='oot', pred_name='predicted')\n",
    "   \n",
    "    perf_sum['mae'] = 0\n",
    "    perf_sum.loc[perf_sum['sample'] == 'train', 'mae'] = mae_train\n",
    "    perf_sum.loc[perf_sum['sample'] == 'test', 'mae'] = mae_test\n",
    "    perf_sum.loc[perf_sum['sample'] == 'oot', 'mae'] = mae_oot\n",
    "    \n",
    "    perf_sum['rmse'] = 0\n",
    "    perf_sum.loc[perf_sum['sample'] == 'train', 'rmse'] = rmse_train\n",
    "    perf_sum.loc[perf_sum['sample'] == 'test', 'rmse'] = rmse_test\n",
    "    perf_sum.loc[perf_sum['sample'] == 'oot', 'rmse'] = rmse_oot\n",
    "    \n",
    "  \n",
    "    save_params = params.copy()\n",
    "    save_params[\"num_boost_round\"] = num_boost_round\n",
    "    save_params[\"early_stopping_rounds\"] = early_stopping_rounds\n",
    "    save_params[\"verbose_eval\"] = verbose_eval\n",
    "    perf_sum[\"parameters\"] = str(save_params)\n",
    "    \n",
    "    ## feature importance \n",
    "    \n",
    "    # ‘weight’: the number of times a feature is used to split the data across all trees.\n",
    "    # ‘gain’: the average gain across all splits the feature is used in.\n",
    "    # ‘cover’: the average coverage across all splits the feature is used in.\n",
    "    # ‘total_gain’: the total gain across all splits the feature is used in.\n",
    "    # ‘total_cover’: the total coverage across all splits the feature is used in.\n",
    "    \n",
    "    importance_xgb = pd.DataFrame.from_dict(data=bst.get_score(importance_type='weight'), orient='index')\n",
    "    importance_xgb = importance_xgb.reset_index()\n",
    "    importance_xgb.columns = [\"features\",\"weight\"]\n",
    "    \n",
    "    for i in ['gain', 'cover', 'total_gain', 'total_cover']:\n",
    "        temp_imp = pd.DataFrame.from_dict(data=bst.get_score(importance_type=i), orient='index')\n",
    "        temp_imp.columns = [i]\n",
    "        importance_xgb = importance_xgb.merge(temp_imp, left_on=\"features\", right_index=True)\n",
    "        del temp_imp\n",
    "        gc.collect()\n",
    "        \n",
    "    importance_xgb[\"total_gain_score\"] =  importance_xgb[\"total_gain\"] / importance_xgb[\"total_gain\"].sum()\n",
    "    importance_xgb = importance_xgb.sort_values('total_gain_score', ascending=False)\n",
    "    del rmse_train, rmse_test, rmse_oot, mae_train, mae_test, mae_oot, save_params\n",
    "    gc.collect()\n",
    "    return bst, predictions, perf_sum, importance_xgb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## performance measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_measurement():\n",
    "    # performance measurement\n",
    "    train_sum_by_pred_score, train_sum_by_pred_row, fig_train_score, fig_train_row = create_performance_measurement_results(predictions[predictions['sample'] == 'train'], title=\"Train\")\n",
    "    test_sum_by_pred_score, test_sum_by_pred_row, fig_test_score, fig_test_row = create_performance_measurement_results(predictions[predictions['sample'] == 'test'], title=\"Test\")\n",
    "    oot_sum_by_pred_score, oot_sum_by_pred_row, fig_oot_score, fig_oot_row = create_performance_measurement_results(predictions[predictions['sample'] == 'oot'], title=\"Oot\")\n",
    "    return train_sum_by_pred_score, train_sum_by_pred_row, fig_train_score, fig_train_row, test_sum_by_pred_score, test_sum_by_pred_row, fig_test_score, fig_test_row, oot_sum_by_pred_score, oot_sum_by_pred_row, fig_oot_score, fig_oot_row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##performance for segments##\n",
    "############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mae_mape_for_segment(dfPredictions, segmentType, segmentTypeGroup, pred_name):\n",
    "  \n",
    "    try:\n",
    "        temp = dfPredictions.copy()\n",
    "        \n",
    "        if segmentType == \"all\":\n",
    "           mae_ = mean_absolute_error(temp['actual'], temp[pred_name])\n",
    "           mape_ = np.mean(np.abs((temp['actual']-temp[pred_name])/temp['actual']))\n",
    "        else:\n",
    "           mae_ = mean_absolute_error(temp.loc[temp[segmentType] == segmentTypeGroup]['actual'], temp.loc[temp[segmentType] == segmentTypeGroup][pred_name])\n",
    "           mape_ = np.mean(np.abs((temp.loc[temp[segmentType] == segmentTypeGroup]['actual']-temp.loc[temp[segmentType] == segmentTypeGroup][pred_name])/temp.loc[temp[segmentType] == segmentTypeGroup]['actual'])) \n",
    "        \n",
    "    except:    \n",
    "        print('Mae is not calculated for:' , segmentType, segmentTypeGroup)\n",
    "        mae_ = \"not calculated\"\n",
    "        mape_ = \"not calculated\"\n",
    "\n",
    "    del temp\n",
    "    gc.collect()\n",
    "    \n",
    "    return mae_,mape_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_performance_by_segment(df):\n",
    "    # performance measurement by different segmentation scenarios    \n",
    "    temp=df.copy()\n",
    "    #validation for PERIOD\n",
    "    temp['date'] = pd.to_datetime(temp['refdate'])\n",
    "    temp['year'] = temp['date'].dt.year\n",
    "    temp['month'] = temp['date'].dt.month\n",
    "    temp['PERIOD'] = temp['year'].astype(str) + temp['month'].astype(str)\n",
    "    \n",
    "    temp=temp.drop(['date','year','month'],axis=1)\n",
    "    #validation for ACTUALINCOMEBUCKET\n",
    "    divide_by_score=np.arange(0, 100001, 1000)\n",
    "    temp['PREDINCOMEBUCKET'] = pd.cut(temp['predicted'], divide_by_score).astype('category')\n",
    "   \n",
    "    temp['ACTINCOMEBUCKET'] = pd.cut(temp['actual'], divide_by_score).astype('category')\n",
    "\n",
    "    \n",
    "    segment_summary_all_sample = temp.groupby(\"count\")[\"actual\", \"predicted\", \"count\"].agg(\"sum\")\n",
    "    segment_summary_all_sample.insert(loc = 0, column = 'segmentType', value = \"all\")\n",
    "    segment_summary_all_sample.insert(loc = 0, column = 'segmentTypeGroup', value = \"all\")\n",
    "    \n",
    "    segment_summary_period = temp.groupby(\"PERIOD\")[\"actual\", \"predicted\", \"count\"].agg(\"sum\").reset_index() \n",
    "    segment_summary_period=segment_summary_period[segment_summary_period['count']!=0]\n",
    "    segment_summary_period.insert(loc = 0, column = 'segmentType', value = \"PERIOD\")\n",
    "    segment_summary_period.rename(columns={'PERIOD':'segmentTypeGroup'}, inplace = True) \n",
    "\n",
    "    segment_summary_income_bucket = temp.groupby(\"PREDINCOMEBUCKET\")[\"actual\", \"predicted\", \"count\"].agg(\"sum\").reset_index() \n",
    "    segment_summary_income_bucket=segment_summary_income_bucket[segment_summary_income_bucket['count']!=0]\n",
    "    segment_summary_income_bucket.insert(loc = 0, column = 'segmentType', value = \"PREDINCOMEBUCKET\")\n",
    "    segment_summary_income_bucket.rename(columns={'PREDINCOMEBUCKET':'segmentTypeGroup'}, inplace = True)\n",
    "\n",
    "    segment_summary_decincome_bucket = temp.groupby(\"ACTINCOMEBUCKET\")[\"actual\", \"predicted\", \"count\"].agg(\"sum\").reset_index() \n",
    "    segment_summary_decincome_bucket=segment_summary_decincome_bucket[segment_summary_decincome_bucket['count']!=0]\n",
    "    segment_summary_decincome_bucket.insert(loc = 0, column = 'segmentType', value = \"ACTINCOMEBUCKET\")\n",
    "    segment_summary_decincome_bucket.rename(columns={'ACTINCOMEBUCKET':'segmentTypeGroup'}, inplace = True)\n",
    "\n",
    "\n",
    "    segment_summary_all = segment_summary_all_sample.append([segment_summary_period, segment_summary_income_bucket, segment_summary_decincome_bucket])\n",
    "    del segment_summary_all_sample, segment_summary_period, segment_summary_income_bucket, segment_summary_decincome_bucket\n",
    "    \n",
    "    segment_summary_all[\"avg_pd\"] = segment_summary_all[\"predicted\"]/segment_summary_all[\"count\"]\n",
    "   \n",
    "    segment_summary_all[\"mae\"] = np.nan\n",
    "    segment_summary_all[\"mape\"] = np.nan\n",
    "\n",
    "    \n",
    "    for segmentType in pd.unique(segment_summary_all['segmentType']):\n",
    "        for segmentTypeGroup in pd.unique(segment_summary_all.loc[segment_summary_all['segmentType']== segmentType]['segmentTypeGroup']):\n",
    "            maeSegment,mapeSegment = calculate_mae_mape_for_segment(temp, segmentType, segmentTypeGroup, \"predicted\")\n",
    "            segment_summary_all.loc[(segment_summary_all['segmentType'] == segmentType) & (segment_summary_all['segmentTypeGroup'] == segmentTypeGroup), 'mae'] = maeSegment\n",
    "            segment_summary_all.loc[(segment_summary_all['segmentType'] == segmentType) & (segment_summary_all['segmentTypeGroup'] == segmentTypeGroup), 'mape'] = mapeSegment\n",
    "\n",
    "    return segment_summary_all      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_measurement_for_segment():\n",
    "    # performance measurement\n",
    "    segment_summary_all_train = evaluate_performance_by_segment(predictions[predictions['sample'] == 'train'])\n",
    "    segment_summary_all_test = evaluate_performance_by_segment(predictions[predictions['sample'] == 'test'])\n",
    "    segment_summary_all_oot = evaluate_performance_by_segment(predictions[predictions['sample'] == 'oot'])\n",
    "    segment_summary_all = evaluate_performance_by_segment(predictions)\n",
    "\n",
    "    return segment_summary_all_train, segment_summary_all_test,segment_summary_all_oot, segment_summary_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####################\n",
    "### Save Results ###\n",
    "####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(path):\n",
    "    print(\"\\nSaving Results...\")\n",
    "    # create folder\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    \n",
    "    # save grid search results\n",
    "    if grid_search_:\n",
    "        xgb_gs_train.to_excel(path+\"fullgrid.xlsx\" )\n",
    "    \n",
    "    # save predictions\n",
    "#    predictions_calb.to_csv(path+\"predictions_calb.csv\" )\n",
    "    \n",
    "    # save performance results\n",
    "    perf_sum.to_excel(path+\"performance_summary.xlsx\" )\n",
    "    \n",
    "    # save comparison plot - actual vs predicted\n",
    "    with matplotlib.backends.backend_pdf.PdfPages(path+'Performance_Details.pdf') as pdf:\n",
    "        for i in [fig_train_score, fig_train_row, fig_test_score, fig_test_row, fig_oot_score, fig_oot_row]:\n",
    "            pdf.savefig(i)\n",
    "        \n",
    "    # save importance features' information\n",
    "    importance_xgb.to_excel(path+\"importance_xgb.xlsx\")   \n",
    "    \n",
    "    \n",
    "    # save model performance summary data\n",
    "    perf_dict = {'train_score': train_sum_by_pred_score, 'train_count': train_sum_by_pred_row,\n",
    "                 'test_score': test_sum_by_pred_score, 'test_count': test_sum_by_pred_row,\n",
    "                 'oot_score': oot_sum_by_pred_score, 'oot_count': oot_sum_by_pred_row}\n",
    "    \n",
    "    with pd.ExcelWriter(path+'Performance_Details_data.xlsx') as writer:\n",
    "        for name_ in perf_dict:\n",
    "            perf_dict[name_].to_excel(writer, sheet_name=name_)\n",
    "    \n",
    "    del perf_dict\n",
    "    gc.collect()\n",
    "    \n",
    "    # save gini performance by various segment breakdown\n",
    "    #segment_summary_all.to_excel(path+\"segment_breakdown.xlsx\")\n",
    "    \n",
    "    # save model performance summary data\n",
    "    #perf_dict = {'summary_all_train': segment_summary_all_train,\n",
    "    #             'summary_all_test': segment_summary_all_test,\n",
    "    #             'summary_all_oot': segment_summary_all_oot,\n",
    "    #             'summary_all': segment_summary_all}\n",
    "    \n",
    "    #with pd.ExcelWriter(path+'Performance_Details_for_segment_data.xlsx') as writer:\n",
    "    #    for name_ in perf_dict:\n",
    "    #        perf_dict[name_].to_excel(writer, sheet_name=name_)\n",
    "    \n",
    "    #del perf_dict\n",
    "    #gc.collect()  \n",
    "    \n",
    "    \n",
    "    # save xgb model object\n",
    "    pickle.dump(bst, open(path+\"model_xgb.pkl\", 'wb'))\n",
    "    # load the xgb model from disk\n",
    "    #loaded_model = pickle.load(open(path+filename, 'rb'))\n",
    "    \n",
    "    # save pmml object\n",
    "    #sklearn2pmml(pipeline, path + str(\"xgboost_model.pmml\"), with_repr=True)\n",
    "    \n",
    "    #save pmml predictions\n",
    "    #pred_all.to_excel(path + str('predictions_with_pmml.xlsx'))\n",
    "    \n",
    "    # save pmml object\n",
    "    #sklearn2pmml(pipeline_ren, path + str(\"xgboost_model_renamed.pmml\"), with_repr=True)\n",
    "    \n",
    "    #save pmml predictions\n",
    "    #pred_all_ren.to_excel(path + str('predictions_with_pmml_renamed.xlsx'))\n",
    "    \n",
    "    # save character encoding matrix\n",
    "    #encoding_matrix.to_excel(path+\"encoding_matrix.xlsx\")\n",
    "    #encoding_matrix.to_pickle(path+\"encoding_matrix.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_preprocess(df, max_threshold):\n",
    "    '''\n",
    "    execute all functions defined for preprocessing\n",
    "    '''\n",
    "    na_report = create_na_report(df)\n",
    "    num_cols, char_cols = create_dtype_report(df)\n",
    "    new_df = eliminate_missing(df, na_report, max_threshold)\n",
    "    new_df = eliminate_single_unique(new_df)\n",
    "    #enc_df = create_encoding_matrix(new_df)\n",
    "    #new_df = encode_from_matrix(new_df, enc_df)\n",
    "    \n",
    "    #save enc_df for later use in prediction code\n",
    "    gc.collect()\n",
    "    print(\"\\nPreprocess is done...\")\n",
    "    return new_df#, enc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df_by_index_pmml(df, key_var, train_index, test_index, oot_index):\n",
    "    # first get TARGET arrays\n",
    "    y_train, y_test, y_oot = df['target'].iloc[train_index], df['target'].iloc[test_index], df['target'].iloc[oot_index]\n",
    "    \n",
    "    # get key values apart including TARGET\n",
    "    cols_ = df.columns[df.columns.isin(list(key_var))] #including TARGET\n",
    "    df_key_var = df[cols_]  #save key values\n",
    "    df_n = df.drop(cols_, axis=1)\n",
    "    \n",
    "    # set dataframes\n",
    "    X_train, X_test, X_oot = df_n.iloc[train_index], df_n.iloc[test_index], df_n.iloc[oot_index]\n",
    "    train_keys, test_keys, oot_keys = df_key_var.iloc[train_index], df_key_var.iloc[test_index], df_key_var.iloc[oot_index]\n",
    "\n",
    "    del df_n\n",
    "    gc.collect()\n",
    "    return X_train, X_test, X_oot, y_train, y_test, y_oot, train_keys, test_keys, oot_keys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = pyodbc.connect('Driver={SQL Server};' \n",
    "                      'Server=DEVELOPMENT-01;'\n",
    "                      'Database=Erc_Project;'\n",
    "                      'Trusted_Connection=yes;')\n",
    "cursor = conn.cursor()\n",
    "data = pd.read_sql_query((\"\"\" SELECT a.contactcode,a.refdate,a.target\n",
    "\n",
    ",BusinessUnitCode\n",
    ",AssetManagementCode\n",
    ",ReachStatusCode\n",
    ",FinancialStatusCode\n",
    ",FirstReachDate_monthdiff\n",
    ",FollowingStartDate_monthdiff\n",
    ",Sensibility\n",
    ",IsTCCitizen\n",
    ",ContactTypeCode\n",
    ",Age\n",
    ",GenderCode\n",
    ",MaritalStatusCode\n",
    ",CityCode\n",
    ",ExistingFlag\n",
    ",kefil_cnt\n",
    ",min_acqdate_open_diff\n",
    ",gsmsms_cnt\n",
    ",CustomerDifficulty\n",
    ",restate_haciz_cnt\n",
    ",restate_aktif_TenderPrice\n",
    ",restate_haciz_TenderPrice\n",
    ",restate_ipotek_TenderPrice\n",
    ",restate_pasif_TenderPrice\n",
    ",restate_aktif_MarketValue\n",
    ",restate_haciz_MarketValue\n",
    ",restate_pasif_MarketValue\n",
    ",ticari_arac_anlasma_cnt\n",
    ",ticari_arac_icra_tenderprice\n",
    ",otomobil_icra_tenderprice\n",
    ",ticari_arac_diger_tenderprice\n",
    ",otomobil_diger_tenderprice\n",
    ",ticari_arac_icra_NetPaymentAmount\n",
    ",ticari_arac_anlasma_MarketingValue\n",
    ",otomobil_anlasma_MarketingValue\n",
    ",ticari_arac_icra_MarketingValue\n",
    ",ticari_arac_deger_yok_MarketingValue\n",
    ",otomobil_deger_yok_MarketingValue\n",
    ",motor_deger_yok_MarketingValue\n",
    ",diger_deger_yok_MarketingValue\n",
    ",ticari_arac_yakalama_MarketingValue\n",
    ",otomobil_yakalama_MarketingValue\n",
    ",diger_yakalama_MarketingValue\n",
    ",ticari_arac_diger_MarketingValue\n",
    ",otomobil_diger_MarketingValue\n",
    ",diger_MarketingValue\n",
    ",tah_icra_odemesi\n",
    ",tah_icra_odemesi_avg_l3m\n",
    ",tah_icra_satis_avg_l3m\n",
    ",tah_diger_avg_l3m\n",
    ",outbound_call_cnt_max_l3m\n",
    ",tah_odeme_teklifi_min_l3m\n",
    ",tah_total_min_l3m\n",
    ",outbound_call_cnt_max_l6m\n",
    ",gsmsms_cnt_max_l6m\n",
    ",tah_icra_odemesi_max_l6m\n",
    ",tcknsms_cnt_min_l6m\n",
    ",IntAmount_UPB_rat_avg_l12m\n",
    ",sum_ExpenseAmount_avg_l12m\n",
    ",tah_total_avg_l12m\n",
    ",Current_UPB_Debt_rat_min_l12m\n",
    ",tah_odeme_teklifi_min_l12m\n",
    ",Current_UPB_Debt_rat_max_l12m\n",
    ",outbound_call_cnt_max_l12m\n",
    ",gsmsms_cnt_max_l12m\n",
    ",tcknsms_cnt_max_l12m\n",
    ",tah_icra_odemesi_max_l12m\n",
    ",tah_diger_max_l12m\n",
    ",min_cutoffdate_diff\n",
    ",acq_tot_debt\n",
    ",min_noticedate_diff\n",
    ",bankcode_cnt\n",
    ",AcqPaymentTRY\n",
    ",loantype1_cnt\n",
    ",loantype2_cnt\n",
    ",loantype3_cnt\n",
    ",loantype4_cnt\n",
    ",loantype5_cnt\n",
    ",loantype6_cnt\n",
    ",loantype7_cnt\n",
    ",loantype8_cnt\n",
    ",mirasci_flg\n",
    ",call_duration\n",
    ",first_call_diff\n",
    ",KKBbankaTSmonth\n",
    ",MaxÜrünKapatmaTarihi\n",
    ",xDevirBakiye\n",
    ",MaxVYSKapatmaTarihi_diff\n",
    ",ToplamMemzuc\n",
    ",ProtocolAmount\n",
    ",ActivatedProtocolAmount\n",
    ",BrokeProtocolAmount\n",
    ",CanceledProtocolAmount\n",
    ",LiveProtocolAmount\n",
    ",TotalProtocolAmount\n",
    ",masraf_tutari\n",
    ",meslek_L\n",
    ",tenure_open\n",
    ",BrokeProtocolAmount_min_l3m\n",
    ",ProtocolAmount_min_l6m\n",
    ",ActivatedProtocolAmount_min_l6m\n",
    ",BrokeProtocolAmount_min_l6m\n",
    ",CanceledProtocolAmount_min_l6m\n",
    ",LiveProtocolAmount_avg_l6m\n",
    ",LiveProtocolCount_max_l6m\n",
    ",FulledTotalProtocolAmount_max_l6m\n",
    ",TotalProtocolAmount_max_l6m\n",
    ",BrokeProtocolAmount_min_l12m\n",
    ",TotalProtocolAmount_min_l12m\n",
    ",ProtocolAmount_avg_l12m\n",
    ",CanceledProtocolAmount_avg_l12m\n",
    ",LiveProtocolCount_max_l12m\n",
    ",ActivatedProtocolAmount_max_l12m\n",
    ",LiveProtocolAmount_max_l12m\n",
    ",Sensibility_avg_rat_l1m_l3m\n",
    ",Current_UPB_Debt_rat_avg_rat_l1m_l3m\n",
    ",ExpenseAmount_Debt_rat_avg_rat_l1m_l3m\n",
    ",Inbound_call_20_cnt_avg_rat_l1m_l3m\n",
    ",Inbound_call_cnt_avg_rat_l1m_l3m\n",
    ",Manuel_call_20_cnt_avg_rat_l1m_l3m\n",
    ",Manuel_call_cnt_avg_rat_l1m_l3m\n",
    ",SalaryConfiscationFlag_avg_rat_l1m_l3m\n",
    ",tcknsms_cnt_avg_rat_l1m_l6m\n",
    ",ActivatedProtocolCount_avg_rat_l1m_l6m\n",
    ",BrokeProtocolCount_avg_rat_l1m_l6m\n",
    ",outbound_call_20_cnt_avg_rat_l3m_l6m\n",
    ",Inbound_call_20_cnt_avg_rat_l3m_l6m\n",
    ",Inbound_call_cnt_avg_rat_l3m_l6m\n",
    ",Manuel_call_20_cnt_avg_rat_l3m_l6m\n",
    ",Manuel_call_cnt_avg_rat_l3m_l6m\n",
    ",tah_icra_satis_avg_rat_l3m_l6m\n",
    ",ActivatedProtocolCount_avg_rat_l3m_l6m\n",
    ",BrokeProtocolCount_avg_rat_l3m_l6m\n",
    ",LiveProtocolAmount_avg_rat_l3m_l6m\n",
    ",ActivatedProtocolCount_avg_rat_l1m_l12m\n",
    ",FullyPaidProtocolCount_avg_rat_l1m_l12m\n",
    ",LiveProtocolCount_avg_rat_l1m_l12m\n",
    ",ProtocolAmount_avg_rat_l1m_l12m\n",
    ",BrokeProtocolAmount_avg_rat_l1m_l12m\n",
    ",outbound_call_20_cnt_avg_rat_l3m_l12m\n",
    ",ActivatedProtocolCount_avg_rat_l3m_l12m\n",
    ",CanceledProtocolCount_avg_rat_l3m_l12m\n",
    ",FullyPaidProtocolCount_avg_rat_l3m_l12m\n",
    ",ProtocolAmount_avg_rat_l3m_l12m\n",
    ",CanceledProtocolAmount_avg_rat_l3m_l12m\n",
    ",TotalProtocolAmount_avg_rat_l3m_l12m\n",
    ",Inbound_call_20_cnt_avg_rat_l6m_l12m\n",
    ",Inbound_call_cnt_avg_rat_l6m_l12m\n",
    ",Manuel_call_20_cnt_avg_rat_l6m_l12m\n",
    ",Manuel_call_cnt_avg_rat_l6m_l12m\n",
    ",tah_icra_odemesi_Debt_rat_avg_rat_l6m_l12m\n",
    ",tah_icra_satis_avg_rat_l6m_l12m\n",
    ",tah_diger_Debt_rat_avg_rat_l6m_l12m\n",
    ",BrokeProtocolCount_avg_rat_l6m_l12m\n",
    ",CanceledProtocolCount_avg_rat_l6m_l12m\n",
    ",FullyPaidProtocolCount_avg_rat_l6m_l12m\n",
    ",LiveProtocolCount_avg_rat_l6m_l12m\n",
    ",NewProtocolCount_avg_rat_l6m_l12m\n",
    ",outbound_call_20_cnt_min_rat_l1m_l3m\n",
    ",Inbound_call_20_cnt_min_rat_l1m_l3m\n",
    ",Inbound_call_cnt_min_rat_l1m_l3m\n",
    ",Manuel_call_20_cnt_min_rat_l1m_l3m\n",
    ",Manuel_call_cnt_min_rat_l1m_l3m\n",
    ",tah_diger_min_rat_l1m_l3m\n",
    ",ActivatedProtocolCount_min_rat_l1m_l3m\n",
    ",BrokeProtocolCount_min_rat_l1m_l3m\n",
    ",Manuel_call_20_cnt_min_rat_l1m_l6m\n",
    ",Manuel_call_cnt_min_rat_l1m_l6m\n",
    ",ActivatedProtocolCount_min_rat_l1m_l6m\n",
    ",BrokeProtocolCount_min_rat_l1m_l6m\n",
    ",FullyPaidProtocolCount_min_rat_l1m_l6m\n",
    ",outbound_call_20_cnt_min_rat_l3m_l6m\n",
    ",Inbound_call_20_cnt_min_rat_l3m_l6m\n",
    ",Inbound_call_cnt_min_rat_l3m_l6m\n",
    ",Manuel_call_20_cnt_min_rat_l3m_l6m\n",
    ",Manuel_call_cnt_min_rat_l3m_l6m\n",
    ",ActivatedProtocolCount_min_rat_l3m_l6m\n",
    ",BrokeProtocolCount_min_rat_l3m_l6m\n",
    ",outbound_call_20_cnt_min_rat_l1m_l12m\n",
    ",Manuel_call_20_cnt_min_rat_l1m_l12m\n",
    ",Manuel_call_cnt_min_rat_l1m_l12m\n",
    ",tah_icra_odemesi_Debt_rat_min_rat_l1m_l12m\n",
    ",CanceledProtocolCount_min_rat_l1m_l12m\n",
    ",FullyPaidProtocolCount_min_rat_l1m_l12m\n",
    ",LiveProtocolCount_min_rat_l1m_l12m\n",
    ",TotalProtocolAmount_min_rat_l1m_l12m\n",
    ",Inbound_call_20_cnt_min_rat_l3m_l12m\n",
    ",Inbound_call_cnt_min_rat_l3m_l12m\n",
    ",Manuel_call_20_cnt_min_rat_l3m_l12m\n",
    ",Manuel_call_cnt_min_rat_l3m_l12m\n",
    ",tah_icra_satis_min_rat_l3m_l12m\n",
    ",tah_odeme_teklifi_min_rat_l3m_l12m\n",
    ",CanceledProtocolCount_min_rat_l3m_l12m\n",
    ",sum_IntAmount_min_rat_l6m_l12m\n",
    ",IntAmount_UPB_rat_min_rat_l6m_l12m\n",
    ",outbound_call_cnt_min_rat_l6m_l12m\n",
    ",CanceledProtocolCount_min_rat_l6m_l12m\n",
    ",LiveProtocolCount_min_rat_l6m_l12m\n",
    ",Current_UPB_Debt_rat_max_rat_l1m_l3m\n",
    ",Inbound_call_20_cnt_max_rat_l1m_l3m\n",
    ",Manuel_call_20_cnt_max_rat_l1m_l3m\n",
    ",Manuel_call_cnt_max_rat_l1m_l3m\n",
    ",tah_odeme_teklifi_UPB_rat_max_rat_l1m_l3m\n",
    ",LiveProtocolAmount_max_rat_l1m_l3m\n",
    ",Current_UPB_Debt_rat_max_rat_l1m_l6m\n",
    ",BrokeProtocolCount_max_rat_l1m_l6m\n",
    ",outbound_call_20_cnt_max_rat_l3m_l6m\n",
    ",Inbound_call_20_cnt_max_rat_l3m_l6m\n",
    ",Inbound_call_cnt_max_rat_l3m_l6m\n",
    ",Manuel_call_20_cnt_max_rat_l3m_l6m\n",
    ",Manuel_call_cnt_max_rat_l3m_l6m\n",
    ",tah_total_max_rat_l3m_l6m\n",
    ",Current_UPB_Debt_rat_max_rat_l1m_l12m\n",
    ",tah_diger_max_rat_l1m_l12m\n",
    ",ActivatedProtocolCount_max_rat_l1m_l12m\n",
    ",CanceledProtocolCount_max_rat_l1m_l12m\n",
    ",FullyPaidProtocolCount_max_rat_l1m_l12m\n",
    ",LiveProtocolCount_max_rat_l1m_l12m\n",
    ",NewProtocolCount_max_rat_l1m_l12m\n",
    ",BrokeProtocolCount_max_rat_l3m_l12m\n",
    ",FullyPaidProtocolCount_max_rat_l3m_l12m\n",
    ",Current_UPB_Debt_rat_max_rat_l6m_l12m\n",
    ",ExpenseAmount_Debt_rat_max_rat_l6m_l12m\n",
    ",Inbound_call_20_cnt_max_rat_l6m_l12m\n",
    ",Inbound_call_cnt_max_rat_l6m_l12m\n",
    ",Manuel_call_20_cnt_max_rat_l6m_l12m\n",
    ",Manuel_call_cnt_max_rat_l6m_l12m\n",
    ",tah_diger_max_rat_l6m_l12m\n",
    ",ActivatedProtocolCount_max_rat_l6m_l12m\n",
    ",BrokeProtocolCount_max_rat_l6m_l12m\n",
    ",CanceledProtocolCount_max_rat_l6m_l12m\n",
    ",FullyPaidProtocolCount_max_rat_l6m_l12m\n",
    ",LiveProtocolCount_max_rat_l6m_l12m\n",
    ",NewProtocolCount_max_rat_l6m_l12m\n",
    "\n",
    "\n",
    " FROM [Erc_Project].[exp].[ticari_vars_target] a\n",
    "    left join [Erc_Project].[exp].[ticari_ratio_vars_avg] b on a.contactcode=b.contactcode and a.refdate=b.refdate \n",
    "    left join [Erc_Project].[exp].[ticari_ratio_vars_min] c on a.contactcode=c.contactcode and a.refdate=c.refdate\n",
    "    left join [Erc_Project].[exp].[ticari_ratio_vars_max] d on a.contactcode=d.contactcode and a.refdate=d.refdate\n",
    "\"\"\"), conn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_vars=pd.read_excel('ticari_model_variables_'+str(datetime.today().strftime(\"%Y_%m_%d\"))+'.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model_vars.loc[len(model_vars.index)]=['121','refdate']\n",
    "#model_vars.loc[len(model_vars.index)]=['122','target']\n",
    "#model_vars=model_vars.drop(model_vars.index[0])\n",
    "#model_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data=data[model_vars[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(-999,inplace=True)\n",
    "data=data.replace([np.nan],-999)\n",
    "data=data.replace([np.inf,-np.inf],-999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset has 56647 entries and 236 features\n",
      "0 columns eliminated due to missing values\n",
      "0 columns eliminated due to single unique values, namely: []\n",
      "\n",
      "Preprocess is done...\n",
      "\n",
      "Shape of dataframes; train: (35629,), test: (8908,), oot: (12110,)\n",
      "\n",
      "Percentage of dataframes; train: 62.90%, test: 15.73%, oot: 21.38%\n",
      "\n",
      "Shape of dataframes; train: (35629,), test: (8908,), oot: (12110,)\n",
      "\n",
      "Percentage of dataframes; train: 62.90%, test: 15.73%, oot: 21.38%\n",
      "\n",
      "iteration: 1\n",
      "[0]\ttrain-mae:0.49527\teval-mae:0.49529\n",
      "[50]\ttrain-mae:0.30999\teval-mae:0.31112\n",
      "[100]\ttrain-mae:0.19756\teval-mae:0.19958\n",
      "[150]\ttrain-mae:0.12927\teval-mae:0.13202\n",
      "[200]\ttrain-mae:0.08768\teval-mae:0.09111\n",
      "[250]\ttrain-mae:0.06243\teval-mae:0.06629\n",
      "[300]\ttrain-mae:0.04702\teval-mae:0.05133\n",
      "[350]\ttrain-mae:0.03768\teval-mae:0.04227\n",
      "[400]\ttrain-mae:0.03195\teval-mae:0.03684\n",
      "[450]\ttrain-mae:0.02844\teval-mae:0.03359\n",
      "[499]\ttrain-mae:0.02629\teval-mae:0.03166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:105: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration: 2\n",
      "[0]\ttrain-rmse:0.49527\teval-rmse:0.49529\n",
      "[50]\ttrain-rmse:0.31293\teval-rmse:0.31473\n",
      "[100]\ttrain-rmse:0.20858\teval-rmse:0.21321\n",
      "[150]\ttrain-rmse:0.15241\teval-rmse:0.16073\n",
      "[200]\ttrain-rmse:0.12449\teval-rmse:0.13662\n",
      "[250]\ttrain-rmse:0.11186\teval-rmse:0.12645\n",
      "[300]\ttrain-rmse:0.10604\teval-rmse:0.12280\n",
      "[350]\ttrain-rmse:0.10345\teval-rmse:0.12144\n",
      "[400]\ttrain-rmse:0.10203\teval-rmse:0.12099\n",
      "[450]\ttrain-rmse:0.10116\teval-rmse:0.12088\n",
      "[499]\ttrain-rmse:0.10049\teval-rmse:0.12086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:105: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration: 3\n",
      "[0]\ttrain-mae:0.49527\teval-mae:0.49529\n",
      "[50]\ttrain-mae:0.31026\teval-mae:0.31121\n",
      "[100]\ttrain-mae:0.19798\teval-mae:0.19974\n",
      "[150]\ttrain-mae:0.12980\teval-mae:0.13219\n",
      "[200]\ttrain-mae:0.08826\teval-mae:0.09127\n",
      "[250]\ttrain-mae:0.06300\teval-mae:0.06643\n",
      "[300]\ttrain-mae:0.04750\teval-mae:0.05143\n",
      "[350]\ttrain-mae:0.03805\teval-mae:0.04237\n",
      "[400]\ttrain-mae:0.03229\teval-mae:0.03688\n",
      "[450]\ttrain-mae:0.02869\teval-mae:0.03356\n",
      "[499]\ttrain-mae:0.02640\teval-mae:0.03158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:105: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration: 4\n",
      "[0]\ttrain-rmse:0.49528\teval-rmse:0.49529\n",
      "[50]\ttrain-rmse:0.31331\teval-rmse:0.31481\n",
      "[100]\ttrain-rmse:0.20931\teval-rmse:0.21332\n",
      "[150]\ttrain-rmse:0.15359\teval-rmse:0.16075\n",
      "[200]\ttrain-rmse:0.12603\teval-rmse:0.13659\n",
      "[250]\ttrain-rmse:0.11344\teval-rmse:0.12643\n",
      "[300]\ttrain-rmse:0.10744\teval-rmse:0.12272\n",
      "[350]\ttrain-rmse:0.10451\teval-rmse:0.12135\n",
      "[400]\ttrain-rmse:0.10303\teval-rmse:0.12079\n",
      "[450]\ttrain-rmse:0.10185\teval-rmse:0.12057\n",
      "[499]\ttrain-rmse:0.10080\teval-rmse:0.12049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:105: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration: 5\n",
      "[0]\ttrain-mae:0.49528\teval-mae:0.49529\n",
      "[50]\ttrain-mae:0.31045\teval-mae:0.31126\n",
      "[100]\ttrain-mae:0.19833\teval-mae:0.19984\n",
      "[150]\ttrain-mae:0.13022\teval-mae:0.13236\n",
      "[200]\ttrain-mae:0.08877\teval-mae:0.09145\n",
      "[250]\ttrain-mae:0.06349\teval-mae:0.06663\n",
      "[300]\ttrain-mae:0.04787\teval-mae:0.05161\n",
      "[350]\ttrain-mae:0.03832\teval-mae:0.04251\n",
      "[400]\ttrain-mae:0.03250\teval-mae:0.03701\n",
      "[450]\ttrain-mae:0.02888\teval-mae:0.03370\n",
      "[499]\ttrain-mae:0.02660\teval-mae:0.03171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:105: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration: 6\n",
      "[0]\ttrain-rmse:0.49528\teval-rmse:0.49529\n",
      "[50]\ttrain-rmse:0.31355\teval-rmse:0.31484\n",
      "[100]\ttrain-rmse:0.20994\teval-rmse:0.21339\n",
      "[150]\ttrain-rmse:0.15457\teval-rmse:0.16089\n",
      "[200]\ttrain-rmse:0.12736\teval-rmse:0.13669\n",
      "[250]\ttrain-rmse:0.11485\teval-rmse:0.12661\n",
      "[300]\ttrain-rmse:0.10855\teval-rmse:0.12288\n",
      "[350]\ttrain-rmse:0.10533\teval-rmse:0.12155\n",
      "[400]\ttrain-rmse:0.10362\teval-rmse:0.12101\n",
      "[450]\ttrain-rmse:0.10247\teval-rmse:0.12083\n",
      "[499]\ttrain-rmse:0.10142\teval-rmse:0.12078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:105: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration: 7\n",
      "[0]\ttrain-mae:0.49530\teval-mae:0.49532\n",
      "[50]\ttrain-mae:0.31264\teval-mae:0.31262\n",
      "[100]\ttrain-mae:0.20277\teval-mae:0.20271\n",
      "[150]\ttrain-mae:0.13628\teval-mae:0.13621\n",
      "[200]\ttrain-mae:0.09607\teval-mae:0.09598\n",
      "[250]\ttrain-mae:0.07173\teval-mae:0.07163\n",
      "[300]\ttrain-mae:0.05701\teval-mae:0.05691\n",
      "[350]\ttrain-mae:0.04810\teval-mae:0.04800\n",
      "[400]\ttrain-mae:0.04271\teval-mae:0.04261\n",
      "[450]\ttrain-mae:0.03945\teval-mae:0.03934\n",
      "[499]\ttrain-mae:0.03751\teval-mae:0.03740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:105: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration: 8\n",
      "[0]\ttrain-rmse:0.49532\teval-rmse:0.49532\n",
      "[50]\ttrain-rmse:0.31644\teval-rmse:0.31643\n",
      "[100]\ttrain-rmse:0.21754\teval-rmse:0.21747\n",
      "[150]\ttrain-rmse:0.16730\teval-rmse:0.16719\n",
      "[200]\ttrain-rmse:0.14461\teval-rmse:0.14446\n",
      "[250]\ttrain-rmse:0.13536\teval-rmse:0.13519\n",
      "[300]\ttrain-rmse:0.13181\teval-rmse:0.13162\n",
      "[350]\ttrain-rmse:0.13048\teval-rmse:0.13029\n",
      "[400]\ttrain-rmse:0.13000\teval-rmse:0.12980\n",
      "[450]\ttrain-rmse:0.12982\teval-rmse:0.12962\n",
      "[499]\ttrain-rmse:0.12975\teval-rmse:0.12955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:105: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration: 9\n",
      "[0]\ttrain-mae:0.49530\teval-mae:0.49532\n",
      "[50]\ttrain-mae:0.31264\teval-mae:0.31262\n",
      "[100]\ttrain-mae:0.20277\teval-mae:0.20272\n",
      "[150]\ttrain-mae:0.13629\teval-mae:0.13622\n",
      "[200]\ttrain-mae:0.09607\teval-mae:0.09599\n",
      "[250]\ttrain-mae:0.07175\teval-mae:0.07165\n",
      "[300]\ttrain-mae:0.05702\teval-mae:0.05692\n",
      "[350]\ttrain-mae:0.04811\teval-mae:0.04801\n",
      "[400]\ttrain-mae:0.04272\teval-mae:0.04262\n",
      "[450]\ttrain-mae:0.03947\teval-mae:0.03936\n",
      "[499]\ttrain-mae:0.03752\teval-mae:0.03742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:105: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration: 10\n",
      "[0]\ttrain-rmse:0.49532\teval-rmse:0.49532\n",
      "[50]\ttrain-rmse:0.31645\teval-rmse:0.31643\n",
      "[100]\ttrain-rmse:0.21754\teval-rmse:0.21749\n",
      "[150]\ttrain-rmse:0.16731\teval-rmse:0.16720\n",
      "[200]\ttrain-rmse:0.14462\teval-rmse:0.14447\n",
      "[250]\ttrain-rmse:0.13537\teval-rmse:0.13520\n",
      "[300]\ttrain-rmse:0.13182\teval-rmse:0.13164\n",
      "[350]\ttrain-rmse:0.13049\teval-rmse:0.13030\n",
      "[400]\ttrain-rmse:0.13001\teval-rmse:0.12981\n",
      "[450]\ttrain-rmse:0.12983\teval-rmse:0.12963\n",
      "[499]\ttrain-rmse:0.12976\teval-rmse:0.12956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:105: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration: 11\n",
      "[0]\ttrain-mae:0.49530\teval-mae:0.49532\n",
      "[50]\ttrain-mae:0.31266\teval-mae:0.31263\n",
      "[100]\ttrain-mae:0.20279\teval-mae:0.20272\n",
      "[150]\ttrain-mae:0.13631\teval-mae:0.13623\n",
      "[200]\ttrain-mae:0.09609\teval-mae:0.09600\n",
      "[250]\ttrain-mae:0.07176\teval-mae:0.07166\n",
      "[300]\ttrain-mae:0.05703\teval-mae:0.05693\n",
      "[350]\ttrain-mae:0.04813\teval-mae:0.04802\n",
      "[400]\ttrain-mae:0.04274\teval-mae:0.04263\n",
      "[450]\ttrain-mae:0.03948\teval-mae:0.03937\n",
      "[499]\ttrain-mae:0.03754\teval-mae:0.03743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:105: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration: 12\n",
      "[0]\ttrain-rmse:0.49532\teval-rmse:0.49532\n",
      "[50]\ttrain-rmse:0.31646\teval-rmse:0.31644\n",
      "[100]\ttrain-rmse:0.21756\teval-rmse:0.21749\n",
      "[150]\ttrain-rmse:0.16732\teval-rmse:0.16721\n",
      "[200]\ttrain-rmse:0.14463\teval-rmse:0.14449\n",
      "[250]\ttrain-rmse:0.13538\teval-rmse:0.13521\n",
      "[300]\ttrain-rmse:0.13183\teval-rmse:0.13165\n",
      "[350]\ttrain-rmse:0.13050\teval-rmse:0.13031\n",
      "[400]\ttrain-rmse:0.13001\teval-rmse:0.12982\n",
      "[450]\ttrain-rmse:0.12984\teval-rmse:0.12964\n",
      "[499]\ttrain-rmse:0.12977\teval-rmse:0.12957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:105: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration: 13\n",
      "[0]\ttrain-mae:0.49530\teval-mae:0.49532\n",
      "[50]\ttrain-mae:0.31335\teval-mae:0.31330\n",
      "[100]\ttrain-mae:0.20347\teval-mae:0.20339\n",
      "[150]\ttrain-mae:0.13699\teval-mae:0.13689\n",
      "[200]\ttrain-mae:0.09677\teval-mae:0.09666\n",
      "[250]\ttrain-mae:0.07244\teval-mae:0.07232\n",
      "[300]\ttrain-mae:0.05771\teval-mae:0.05759\n",
      "[350]\ttrain-mae:0.04881\teval-mae:0.04868\n",
      "[400]\ttrain-mae:0.04342\teval-mae:0.04329\n",
      "[450]\ttrain-mae:0.04016\teval-mae:0.04003\n",
      "[499]\ttrain-mae:0.03821\teval-mae:0.03809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:105: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration: 14\n",
      "[0]\ttrain-rmse:0.49532\teval-rmse:0.49532\n",
      "[50]\ttrain-rmse:0.31735\teval-rmse:0.31730\n",
      "[100]\ttrain-rmse:0.21886\teval-rmse:0.21871\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"main parameters should be set before each run\"\"\"\n",
    "\n",
    "    grid_search_= True\n",
    "    save_results_= True\n",
    "    random_search = False\n",
    "    \n",
    "    \n",
    "    segment = 'ticari' #kkbsiz,kkbli_yeni,kkbli_mevcut_sm, kkbli_mevcut_ou, kkbli_mevcut_emk,kkbli_mevcut_ku\n",
    "    \n",
    "\n",
    "    # key variables\n",
    "    key_var = [\"contactcode\", \"refdate\", \"target\"] # for management of data issues\n",
    "    key_vars = [\"contactcode\", \"refdate\", \"target\"] # for preparing data.table() for model estimation\n",
    "    \n",
    "    # define maximum acceptable percentage of missing information\n",
    "    max_threshold = 1\n",
    "    \n",
    "    # splitting percentage of development data into train nand test\n",
    "    splitting_perc = 0.2\n",
    "   \n",
    "    min_date = '2021-01-01' #oot date\n",
    "\n",
    "    # specify seed for random selection - to keep track of the chosen seed, for results' reproduction\n",
    "    seed = 1234\n",
    "    \n",
    "    ## diff_threshold for grid search\n",
    "    diff_threshold = 1000\n",
    "\n",
    "\n",
    "    trial = 'v1'\n",
    "    t_string = str(trial)\n",
    "    today = date.today()\n",
    "    path = 'C://Users//p-cemre.kassara//Desktop/DV f2 py template/{}-{}-{}/'\n",
    "    path = path.format(segment, t_string, today)\n",
    "    \n",
    "        ##datayi okutma##\n",
    "    #directory = Path(\"D:/gtp-files/Final/\")\n",
    "\n",
    "    \n",
    "    #data_directory = directory / \"Data_Files\"\n",
    "    \n",
    "    #data okuma:\n",
    "    \n",
    "    #data=pd.read_csv('Ticari.csv')\n",
    "\n",
    "    \n",
    "    df= data.copy()\n",
    "            \n",
    "    \n",
    "    df=df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "    ###############\n",
    "    ###Modelling###\n",
    "    ###############\n",
    "    \n",
    "    \n",
    "    #code execution:\n",
    "    df = execute_preprocess(df, max_threshold)\n",
    "    df = df.reset_index(drop = True)\n",
    "    train_index, test_index, oot_index=set_split_index(df, splitting_perc, seed, min_date)\n",
    "   \n",
    "    if grid_search_:\n",
    "        grid_params = {\n",
    "        # Parameters that we are going to tune.\n",
    "        'eta' : [0.01,0.05, 0.1],  #0.01,0.05, #0.1\n",
    "        'max_depth': [9,12,15,18], #[9,15],#[9,12,15], #np.arange(3, 8, 2).tolist(), #3,5  #6\n",
    "        'min_child_weight': [20,50,100,150,200], #[50,100], #250,500,750],\n",
    "        'subsample': [1],\n",
    "        'colsample_bytree': [1],\n",
    "        'gamma' : [0,40,60,80,100], #40,60,80,100\n",
    "        'reg_alpha' : [0.1,0.5,0.9], #[0.9,0.5,0.1], #0.05,0.1\n",
    "        # Other parameters  \n",
    "        'objective': ['reg:squarederror'],\n",
    "        #eval_metric değiştirilecek:\n",
    "        'eval_metric' : ['mae','rmse'], #mae #mse # Use mean abs error rather than rmse (lower impact of outliers)\n",
    "        'booster' : ['gbtree'] #dart \n",
    "        }\n",
    "        xgb_gs_train, best_gs_iter, params = execute_grid_search(df, key_var, grid_params, splitting_perc, min_date, seed, random_search, diff_threshold, 432)    \n",
    "    else:\n",
    "        params = set_params(seed)\n",
    " \n",
    "    bst, predictions, perf_sum, importance_xgb = execute_final_model(df, key_var, params, splitting_perc, seed, num_boost_round=500, early_stopping_rounds=20, verbose_eval=10)\n",
    "    train_sum_by_pred_score, train_sum_by_pred_row, fig_train_score, fig_train_row, test_sum_by_pred_score, test_sum_by_pred_row, fig_test_score, fig_test_row, oot_sum_by_pred_score, oot_sum_by_pred_row, fig_oot_score, fig_oot_row = performance_measurement()    \n",
    "    #segment_summary_all = evaluate_performance_by_segment()    \n",
    "    #segment_summary_all_train, segment_summary_all_test,segment_summary_all_oot, segment_summary_all = performance_measurement_for_segment()\n",
    "    \n",
    "    print(perf_sum[[\"sample\",\"mae\"]])\n",
    "    if save_results_:\n",
    "        save_results(path)\n",
    "    print('Done with model_building.py')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_out=predictions[predictions['sample']=='train']\n",
    "train_out[\"pred_binary\"]=np.where(train_out['predicted']>0.03,1,0)\n",
    "train_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_out=predictions[predictions['sample']=='test']\n",
    "test_out[\"pred_binary\"]=np.where(test_out['predicted']>0.03,1,0)\n",
    "test_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oot_out=predictions[predictions['sample']=='test']\n",
    "oot_out[\"pred_binary\"]=np.where(oot_out['predicted']>0.03,1,0)\n",
    "oot_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_conf_matrix=confusion_matrix(train_out[\"actual\"],train_out[\"pred_binary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_roc_auc=roc_auc_score(train_out[\"actual\"],train_out[\"pred_binary\"])\n",
    "test_roc_auc=roc_auc_score(test_out[\"actual\"],test_out[\"pred_binary\"])\n",
    "oot_roc_auc=roc_auc_score(oot_out[\"actual\"],oot_out[\"pred_binary\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gini=2*train_roc_auc-1\n",
    "test_gini=2*test_roc_auc-1\n",
    "oot_gini=2*oot_roc_auc-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"gini train: \", train_gini)\n",
    "print(\"gini test: \", test_gini)\n",
    "print(\"gini oot: \", oot_gini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# aşağısı silinecek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#powercurve için:\n",
    "#pmml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, X_oot, y_train, y_test, y_oot, train_keys, test_keys, oot_keys = split_df_by_index_pmml(df, key_var, train_index, test_index, oot_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = X_train.copy()\n",
    "df_y = y_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = XGBRegressor(n_estimators = (bst.best_iteration +1),\n",
    "                              #early_stopping_rounds=early_stopping_rounds,\n",
    "                              learning_rate = params['eta'],\n",
    "                              max_depth = params['max_depth'],\n",
    "                              min_child_weight = params['min_child_weight'],\n",
    "                              subsample = params['subsample'],\n",
    "                              colsample_bytree = params['colsample_bytree'],\n",
    "                              gamma = params['gamma'],\n",
    "                              reg_alpha = params['reg_alpha'],\n",
    "                              objective = params['objective'],\n",
    "                              booster = params['booster'],\n",
    "                              eval_metric = params['eval_metric'],\n",
    "                              seed = params['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = PMMLPipeline([\n",
    "      (\"regressor\", regressor)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(df_X, df_y.ravel())\n",
    "pipeline.verify(df_X,zeroThreshold = 1e-6, precision = 1e-6)\n",
    "pipeline = make_pmml_pipeline(pipeline, active_fields = df_X.columns.values, target_fields = df_y.name )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_pmml_train = pipeline.predict(X_train)\n",
    "predict_pmml_test = pipeline.predict(X_test)\n",
    "predict_pmml_oot = pipeline.predict(X_oot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict_pmml_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions[predictions['sample']=='train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmml_train_df = pd.DataFrame({'sample': 'train', \n",
    "                              'actual': y_train, \n",
    "                              'predicted_pmml' : predict_pmml_train,\n",
    "                              'RT_CUSTOMER_ID': train_keys[\"RT_CUSTOMER_ID\"], \n",
    "                              #'REPORT_DATE': train_keys[\"REPORT_DATE\"],\n",
    "                              'APP_ID': train_keys[\"APP_ID\"]\n",
    "                              })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmml_test_df = pd.DataFrame({'sample': 'test', \n",
    "                             'actual': y_test, \n",
    "                             'predicted_pmml' : predict_pmml_test,\n",
    "                             'RT_CUSTOMER_ID': test_keys[\"RT_CUSTOMER_ID\"], \n",
    "                             #'REPORT_DATE': test_keys[\"REPORT_DATE\"],\n",
    "                             'APP_ID': test_keys[\"APP_ID\"]\n",
    "                              })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmml_oot_df = pd.DataFrame({'sample': 'oot', \n",
    "                            'actual': y_oot, \n",
    "                            'predicted_pmml' : predict_pmml_oot,\n",
    "                            'RT_CUSTOMER_ID': oot_keys[\"RT_CUSTOMER_ID\"], \n",
    "                             #'REPORT_DATE': oot_keys[\"REPORT_DATE\"],\n",
    "                             'APP_ID': oot_keys[\"APP_ID\"]\n",
    "                              })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_pmml = pmml_train_df.append([pmml_test_df, pmml_oot_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_all =pd.merge(predictions , predictions_pmml, left_on=['APP_ID'], right_on=['APP_ID'], how='left' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if save_results_:\n",
    "    save_results(path)\n",
    "    print('Done with model_building.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "###Analizler Özet###\n",
    "####################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = pd.read_csv(r'D:\\gtp-files\\Final\\3.Hedef Değişken Belirleme\\FAZ1\\KKBLI_YENI_v1\\KKBli_Yeni_target_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v1=pd.merge(df,data_all[['APP_ID','KKB_FLAG','ANA_GRUP_FLAG','APP_SH_DOC_INCOME','RT_MODEL_EST_INCOME',\n",
    "                                 'APP_SH_DECL_INCOME','APP_SALARY_AMT_LM','SKY_FLAG_URUN',\n",
    "                                 'AGE_GRUP','YENISALARYCUSTFLAG','income_seg','APP_FINAL_DECISION',\n",
    "                                 'RT_WORKING_TYPE_DESC', 'RT_SECTOR_TYPE_DESC', 'RT_OCCUPATION', \n",
    "                                 'DOC_INCOME_FINAL','target_w_inf','inflation']],\n",
    "               on=['APP_ID'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v1['DOC_INCOME_FLAG'] = np.where(df_v1['APP_SH_DOC_INCOME'] > 0, 1,0)\n",
    "df_v1['DECL_INCOME_FLAG'] = np.where(df_v1['APP_SH_DECL_INCOME'] > 0, 1,0)\n",
    "df_v1['APP_SALARY_AMT_LM_FLAG'] = np.where(df_v1['APP_SALARY_AMT_LM'] > 0, 1,0)\n",
    "df_v1['RT_KKB_CC_O_TOT_LIM_FLAG'] = np.where(df_v1['RT_KKB_CC_O_TOT_LIM'] > 0, 1,0)\n",
    "df_v1['RT_KKB_CC_MAX_O_C_LIM_L1Y_FLAG'] = np.where(df_v1['RT_KKB_CC_MAX_O_C_LIM_L1Y'] > 0, 1,0)\n",
    "df_v1['RT_KKB_INST_O_TOT_INST_AMT_FLAG'] = np.where(df_v1['RT_KKB_INST_O_TOT_INST_AMT'] > 0, 1,0)\n",
    "df_v1['RT_MODEL_EST_INCOME_FLAG']=np.where(df_v1['RT_MODEL_EST_INCOME']>0,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cx_Oracle as cxo \n",
    "import sqlalchemy\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_v=pd.merge(predictions,df_v1,on=['APP_ID','APP_DATE','RT_CUSTOMER_ID'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle_db=sqlalchemy.create_engine(\"oracle+cx_oracle://SASGELIRTAHMIN:DDzVB4597_PHZK239+@dbtexa.vakifbank.intra:1854/?service_name=DWHTEST\")\n",
    "connection=oracle_db.connect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql=\"SELECT APP_ID,S314 FROM SPSSDM.DM_BRY_SEGMENT_KKBLI_YENI_S314\" \n",
    "df_chunk=pd.read_sql(sql,connection,chunksize=1000000) \n",
    "chunklist= [] \n",
    "for chunk in tqdm(df_chunk,total=20):\n",
    "    chunklist.append(chunk) \n",
    "gc.collect()\n",
    "\n",
    "start_time=timeit.default_timer() \n",
    "df_sm=pd.concat(chunklist,ignore_index=True)\n",
    "print(timeit.default_timer()-start_time) \n",
    "\n",
    "del chunklist, df_chunk, chunk \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sm.columns = map(str.upper,df_sm.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sm['S314_flag']=np.where(df_sm['S314']>0,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_v1=pd.merge(predictions_v,df_sm[['APP_ID','S314','S314_flag']],on=['APP_ID'],how='left') #,'APP_DATE','RT_CUSTOMER_ID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_v1['S314'] = predictions_v1['S314']*predictions_v1['inflation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_v1['actual_grp'] = np.where(predictions_v1['actual'] ==0, '0',\n",
    "                                     np.where(predictions_v1['actual'] < 2000, '<2000',\n",
    "                                              np.where(predictions_v1['actual'] < 3000, '<3000',\n",
    "                                                       np.where(predictions_v1['actual'] < 5000, '<5000',\n",
    "                                                                np.where(predictions_v1['actual'] < 7000, '<7000',\n",
    "                                                                         np.where(predictions_v1['actual'] < 10000, '<10000',\n",
    "                                                                                  np.where(predictions_v1['actual'] < 15000, '<15000',\n",
    "                                                                                           np.where(predictions_v1['actual'] < 20000, '<20000',\n",
    "                                                                                                    np.where(predictions_v1['actual'] < 50000, '<50000',\n",
    "                                                                                                             np.where(predictions_v1['actual'] >= 50000, '>=50000','Unknown'))))))))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_v1['pred_grp'] = np.where(predictions_v1['predicted'] ==0, '0',\n",
    "                                     np.where(predictions_v1['predicted'] < 2000, '<2000',\n",
    "                                              np.where(predictions_v1['predicted'] < 3000, '<3000',\n",
    "                                                       np.where(predictions_v1['predicted'] < 5000, '<5000',\n",
    "                                                                np.where(predictions_v1['predicted'] < 7000, '<7000',\n",
    "                                                                         np.where(predictions_v1['predicted'] < 10000, '<10000',\n",
    "                                                                                  np.where(predictions_v1['predicted'] < 15000, '<15000',\n",
    "                                                                                           np.where(predictions_v1['predicted'] < 20000, '<20000',\n",
    "                                                                                                    np.where(predictions_v1['predicted'] < 50000, '<50000',\n",
    "                                                                                                             np.where(predictions_v1['predicted'] >= 50000, '>=50000','Unknown'))))))))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_v1['model_grp'] = np.where(predictions_v1['RT_MODEL_EST_INCOME'] ==0, '0',\n",
    "                                     np.where(predictions_v1['RT_MODEL_EST_INCOME'] < 2000, '<2000',\n",
    "                                              np.where(predictions_v1['RT_MODEL_EST_INCOME'] < 3000, '<3000',\n",
    "                                                       np.where(predictions_v1['RT_MODEL_EST_INCOME'] < 5000, '<5000',\n",
    "                                                                np.where(predictions_v1['RT_MODEL_EST_INCOME'] < 7000, '<7000',\n",
    "                                                                         np.where(predictions_v1['RT_MODEL_EST_INCOME'] < 10000, '<10000',\n",
    "                                                                                  np.where(predictions_v1['RT_MODEL_EST_INCOME'] < 15000, '<15000',\n",
    "                                                                                           np.where(predictions_v1['RT_MODEL_EST_INCOME'] < 20000, '<20000',\n",
    "                                                                                                    np.where(predictions_v1['RT_MODEL_EST_INCOME'] < 50000, '<50000',\n",
    "                                                                                                             np.where(predictions_v1['RT_MODEL_EST_INCOME'] >= 50000, '>=50000','Unknown'))))))))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_v1['S314_grp'] = np.where(predictions_v1['S314'] ==0, '0',\n",
    "                                     np.where(predictions_v1['S314'] < 2000, '<2000',\n",
    "                                              np.where(predictions_v1['S314'] < 3000, '<3000',\n",
    "                                                       np.where(predictions_v1['S314'] < 5000, '<5000',\n",
    "                                                                np.where(predictions_v1['S314'] < 7000, '<7000',\n",
    "                                                                         np.where(predictions_v1['S314'] < 10000, '<10000',\n",
    "                                                                                  np.where(predictions_v1['S314'] < 15000, '<15000',\n",
    "                                                                                           np.where(predictions_v1['S314'] < 20000, '<20000',\n",
    "                                                                                                    np.where(predictions_v1['S314'] < 50000, '<50000',\n",
    "                                                                                                             np.where(predictions_v1['S314'] >= 50000, '>=50000','Unknown'))))))))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_v1['act_pred']=predictions_v1['actual']-predictions_v1['predicted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_v1['act_pred_grp'] = np.where(predictions_v1['act_pred'] <= -20000, '<=-20000',\n",
    "                             np.where(predictions_v1['act_pred'] < -15000, '<-15000',\n",
    "                             np.where(predictions_v1['act_pred'] < -10000, '<-10000',\n",
    "                             np.where(predictions_v1['act_pred'] < -7000, '<-7000',\n",
    "                             np.where(predictions_v1['act_pred'] < -5000, '<-5000',\n",
    "                             np.where(predictions_v1['act_pred'] < -3000, '<-3000',\n",
    "                             np.where(predictions_v1['act_pred'] < -2000, '<-2000',\n",
    "                             np.where(predictions_v1['act_pred'] < -1000, '<-1000',\n",
    "                             np.where(predictions_v1['act_pred'] < -500, '<-500',\n",
    "                             np.where(predictions_v1['act_pred'] < 0, '<0',\n",
    "                             np.where(predictions_v1['act_pred'] ==0, '0',\n",
    "                             np.where(predictions_v1['act_pred'] < 500, '<500', \n",
    "                             np.where(predictions_v1['act_pred'] < 1000, '<1000',         \n",
    "                             np.where(predictions_v1['act_pred'] < 2000, '<2000',\n",
    "                             np.where(predictions_v1['act_pred'] < 3000, '<3000',\n",
    "                             np.where(predictions_v1['act_pred'] < 5000, '<5000',\n",
    "                             np.where(predictions_v1['act_pred'] < 7000, '<7000',\n",
    "                             np.where(predictions_v1['act_pred'] < 10000, '<10000',\n",
    "                             np.where(predictions_v1['act_pred'] < 15000, '<15000',\n",
    "                             np.where(predictions_v1['act_pred'] < 20000, '<20000',\n",
    "                             np.where(predictions_v1['act_pred'] >= 20000, '>=20000',\n",
    "                                      'Unknown')))))))))))))))))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_v1['model_pred']=predictions_v1['RT_MODEL_EST_INCOME']-predictions_v1['predicted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_v1['model_pred_grp'] = np.where(predictions_v1['model_pred'] <= -20000, '<=-20000',\n",
    "                             np.where(predictions_v1['model_pred'] < -15000, '<-15000',\n",
    "                             np.where(predictions_v1['model_pred'] < -10000, '<-10000',\n",
    "                             np.where(predictions_v1['model_pred'] < -7000, '<-7000',\n",
    "                             np.where(predictions_v1['model_pred'] < -5000, '<-5000',\n",
    "                             np.where(predictions_v1['model_pred'] < -3000, '<-3000',\n",
    "                             np.where(predictions_v1['model_pred'] < -2000, '<-2000',\n",
    "                             np.where(predictions_v1['model_pred'] < -1000, '<-1000',\n",
    "                             np.where(predictions_v1['model_pred'] < -500, '<-500',\n",
    "                             np.where(predictions_v1['model_pred'] < 0, '<0',\n",
    "                             np.where(predictions_v1['model_pred'] ==0, '0',\n",
    "                             np.where(predictions_v1['model_pred'] < 500, '<500', \n",
    "                             np.where(predictions_v1['model_pred'] < 1000, '<1000',         \n",
    "                             np.where(predictions_v1['model_pred'] < 2000, '<2000',\n",
    "                             np.where(predictions_v1['model_pred'] < 3000, '<3000',\n",
    "                             np.where(predictions_v1['model_pred'] < 5000, '<5000',\n",
    "                             np.where(predictions_v1['model_pred'] < 7000, '<7000',\n",
    "                             np.where(predictions_v1['model_pred'] < 10000, '<10000',\n",
    "                             np.where(predictions_v1['model_pred'] < 15000, '<15000',\n",
    "                             np.where(predictions_v1['model_pred'] < 20000, '<20000',\n",
    "                             np.where(predictions_v1['model_pred'] >= 20000, '>=20000',\n",
    "                                      'Unknown')))))))))))))))))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_v1['error'] = abs((predictions_v1['predicted'] - predictions_v1['actual'])/predictions_v1['actual'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_v1['error_grp'] = np.where(predictions_v1['error'] < 0.15, '0-15',\n",
    "                              np.where(predictions_v1['error'] < 0.30, '15-30',\n",
    "                              np.where(predictions_v1['error'] < 0.50, '30-50',\n",
    "                              np.where(predictions_v1['error'] < 0.75, '50-75',\n",
    "                              np.where(predictions_v1['error'] >= 0.75, '75+',\n",
    "                              'Unknown')))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_v1['error_model'] = abs((predictions_v1['predicted'] - predictions_v1['RT_MODEL_EST_INCOME'])/predictions_v1['RT_MODEL_EST_INCOME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_v1['error_model_grp'] = np.where(predictions_v1['error_model'] < 0.15, '0-15',\n",
    "                              np.where(predictions_v1['error_model'] < 0.30, '15-30',\n",
    "                              np.where(predictions_v1['error_model'] < 0.50, '30-50',\n",
    "                              np.where(predictions_v1['error_model'] < 0.75, '50-75',\n",
    "                              np.where(predictions_v1['error_model'] >= 0.75, '75+',\n",
    "                              'Unknown')))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_v1['count']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_v1['count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_v1=predictions_v1.fillna(-9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deneme \n",
    "# tablo oluşturma\n",
    "predictions_v2= pd.pivot_table(predictions_v1,index=['sample','actual_grp','pred_grp','model_grp','act_pred_grp','model_pred_grp',\n",
    "                                                     'error_grp','error_model_grp',\n",
    "                                                     'ANA_GRUP_FLAG', 'RT_WORKING_TYPE_DESC', \n",
    "                                                     'RT_SECTOR_TYPE_DESC', 'RT_OCCUPATION',  \n",
    "                       'MESLEK_SEGMENT', 'KKB_FLAG', 'S314_flag','S314_grp',\n",
    "                       'APP_SALARY_AMT_LM_FLAG', 'APP_SALARYCUST_FLAG','RT_MODEL_EST_INCOME_FLAG',\n",
    "                       'RT_KKB_CC_O_TOT_LIM_FLAG','RT_KKB_CC_MAX_O_C_LIM_L1Y_FLAG',\n",
    "                       'M144','DECL_INCOME_FLAG','RT_KKB_INST_O_TOT_INST_AMT_FLAG',\n",
    "                       'SKY_FLAG_URUN','AGE_GRUP','YENISALARYCUSTFLAG','income_seg','APP_FINAL_DECISION',\n",
    "                       'DOC_INCOME_FLAG'],\n",
    "                            aggfunc={'APP_SH_DECL_INCOME' : np.sum,\n",
    "                                     'APP_SH_DOC_INCOME' : np.sum, \n",
    "                                     'RT_MODEL_EST_INCOME' : np.sum,\n",
    "                                     'S314':np.sum,\n",
    "                                     'APP_SALARY_AMT_LM' :np.sum,\n",
    "                                     'DOC_INCOME_FINAL' :np.sum , 'target_w_inf' : np.sum,\n",
    "                                     'count' : np.sum, \n",
    "                                     'RT_KKB_CC_O_TOT_LIM' : np.sum, 'RT_KKB_CC_MAX_O_C_LIM_L1Y' : np.sum,\n",
    "                                     'RT_KKB_INST_O_TOT_INST_AMT': np.sum,\n",
    "                                     'actual':np.sum,\n",
    "                                     'predicted':np.sum\n",
    "                                     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_v2.reset_index().to_excel(path + str(\"Analysis_perf.xlsx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_v1.to_csv(path + str(\"AllData_KKBli_Yeni.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##BURADAN SONRASI ÇALIŞTIRILMAYACAK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_matrix['variable'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_enc = pd.merge(predictions_v1,data[['APP_ID','MESLEK_SEGMENT', 'RT_CUST_SEGMENT', 'APP_BANK_REGION_CODE',\n",
    "       'M144']],on='APP_ID',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_enc=predictions_enc.fillna('-9999')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = ExcelWriter(path + str('Encoding_List.xlsx'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ('MESLEK_SEGMENT','RT_CUST_SEGMENT','APP_BANK_REGION_CODE','M144'):\n",
    "    predictions_enc[[str(i)+str('_x'),str(i)+str('_y'),'APP_ID']].groupby([str(i)+str('_x'),str(i)+str('_y')]).count().reset_index().to_excel(writer, sheet_name = str(i),index=False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
*****

import pandas as pd
import catboost as cat
from sklearn.preprocessing import LabelEncoder
import csv
import os
import pickle
from sklearn.metrics import roc_auc_score
import pyodbc


####READ AND PICKLE DATASETS####

def sqlreader(query,datasetname,sqldriver,server,path,toPickle=True):
    print( "\nReading data from database...")
    cnxn = pyodbc.connect('DRIVER={'+sqldriver+'};SERVER='+server+';Trusted_Connection=yes')
    print ("Executing query: ", query)
    queryresult = pd.read_sql(query, cnxn)
    print ("Query succesful. Dataframe created.")
    if toPickle == True:
        print ("Pickling dataset")
        queryresult.to_pickle(os.path.join(path,datasetname))
    return queryresult

# =============================================================================
# 
# 
# print ("Reading dataset for 2018-01-31")
# 
# Df_0131 = sqlreader(query = "select * from DM_DS.model.HV_ProtocolPredictionModel_RealEstate_Trainset_20180131 where LiabilityUnresolvedLoanF = 1"
#                     ,datasetname = "Df_0131"
#                     ,sqldriver = "ODBC Driver 13 for SQL Server" 
#                     ,server = "development-01"
#                     ,path = "D:\\Machine_Learning_Models\\Real_Estate_Protocol_Prediction_Model\\Datasets"
#                     ,toPickle=True)
# 
# print ("Reading dataset for 2018-04-30")
# 
# 
# Df_0430 = sqlreader(query = "select * from DM_DS.model.HV_ProtocolPredictionModel_RealEstate_Trainset_20180430 where LiabilityUnresolvedLoanF = 1"
#                     ,datasetname = "Df_0430"
#                     ,sqldriver = "ODBC Driver 13 for SQL Server" 
#                     ,server = "development-01"
#                     ,path = "D:\\Machine_Learning_Models\\Real_Estate_Protocol_Prediction_Model\\Datasets"
#                     ,toPickle=True)
# 
# print ("Reading dataset for 2018-07-31")
# 
# 
# Df_0731 = sqlreader(query = "select * from DM_DS.model.HV_ProtocolPredictionModel_RealEstate_Trainset_20180731 where LiabilityUnresolvedLoanF = 1"
#                     ,datasetname = "Df_0731"
#                     ,sqldriver = "ODBC Driver 13 for SQL Server" 
#                     ,server = "development-01"
#                     ,path = "D:\\Machine_Learning_Models\\Real_Estate_Protocol_Prediction_Model\\Datasets"
#                     ,toPickle=True)
# 
# 
# print ("Reading dataset for 2018-10-31")
# 
# 
# Df_1031 = sqlreader(query = "select * from DM_DS.model.HV_ProtocolPredictionModel_RealEstate_Trainset_20181031 where LiabilityUnresolvedLoanF = 1"
#                     ,datasetname = "Df_1031"
#                     ,sqldriver = "ODBC Driver 13 for SQL Server" 
#                     ,server = "development-01"
#                     ,path = "D:\\Machine_Learning_Models\\Real_Estate_Protocol_Prediction_Model\\Datasets"
#                     ,toPickle=True)
# 
# 
# print ("Reading dataset for 2018-12-31")
# 
# 
# Df_1231 = sqlreader(query = "select * from DM_DS.model.HV_ProtocolPredictionModel_RealEstate_Trainset_20181231 where LiabilityUnresolvedLoanF = 1"
#                     ,datasetname = "Df_1231"
#                     ,sqldriver = "ODBC Driver 13 for SQL Server" 
#                     ,server = "development-01"
#                     ,path = "D:\\Machine_Learning_Models\\Real_Estate_Protocol_Prediction_Model\\Datasets"
#                     ,toPickle=True)
# =============================================================================


Df_0131 =pd.read_pickle('//file-04/FOLDER-REDIRECTION-MALI-ISLER/yunus.karadag/Desktop/RE_Pro_ml/datasets/Df_0131')
Df_0430 =pd.read_pickle('//file-04/FOLDER-REDIRECTION-MALI-ISLER/yunus.karadag/Desktop/RE_Pro_ml/datasets/Df_0430')
Df_0731 =pd.read_pickle('//file-04/FOLDER-REDIRECTION-MALI-ISLER/yunus.karadag/Desktop/RE_Pro_ml/datasets/Df_0731')
Df_1031 =pd.read_pickle('//file-04/FOLDER-REDIRECTION-MALI-ISLER/yunus.karadag/Desktop/RE_Pro_ml/datasets/Df_1031')
Df_1231 =pd.read_pickle('//file-04/FOLDER-REDIRECTION-MALI-ISLER/yunus.karadag/Desktop/RE_Pro_ml/datasets/Df_1231')

Df_0131.AsOfDate.value_counts()
Df_0430.AsOfDate.value_counts()
Df_0731.AsOfDate.value_counts()
Df_1031.AsOfDate.value_counts()
Df_1231.AsOfDate.value_counts()

####PREPROCESS DATASETS####

#For removing nonunique columns
def removenonuniquecol(dataset):
    dropcols = [col for col in dataset.columns if dataset[col].nunique(dropna=True)==1]
    print ('Removing columns: ',dropcols)
    dataset.drop(dropcols,axis= 1,inplace= True,errors= 'ignore')
    return dropcols

#For converting csv file to dictionary
def csvloader(path):
    csv_file_object = csv.reader(open(path, 'r' )) #r is for read b is for binary
    listname = [] 
    for row in csv_file_object:
        row = row[0]
        listname.append(row) 
    return listname

#For label encoding
def labelencoder(dataset,featurelist):
    objectlist = dataset.select_dtypes(include=['object']).copy()
    cat_col = [col for col in dataset.columns if col in objectlist and col in featurelist]
    for col in cat_col:
        print("Encoding ",col)
        lbl = LabelEncoder()
        dataset[col].fillna(-999)
        lbl.fit(list(dataset[col].values.astype('str')))
        dataset[col] = lbl.transform(list(dataset[col].values.astype('str')))
    return cat_col

def TrainPrep(DataPath,PredictorPath,Datasetname,Labelname,Id):
    picklePath = os.path.join(DataPath,Datasetname)
    Df = pd.read_pickle(picklePath)
    removedCols = removenonuniquecol(Df)    
    Predictors = csvloader(PredictorPath)
    Predictors = [col for col in Predictors if col not in removedCols]
    Predictors = [col for col in Predictors if col not in ["ContactCode",'TrainsetContactF']]
    DfLabel = Df[Labelname]
    encodedList = labelencoder(Df, Predictors)
    return Predictors,Df, DfLabel, removedCols, encodedList

def TestPrep(DataPath,PredictorPath,Datasetname, dropcols, encodedList,Labelname):
    picklePath = os.path.join(DataPath,Datasetname)
    Df = pd.read_pickle(picklePath)
    Df.drop(dropcols,axis= 1,inplace= True,errors= 'ignore')
    Predictors = csvloader(PredictorPath)
    Predictors = [col for col in Predictors if col not in dropcols]
    Predictors = [col for col in Predictors if col not in ['ContactCode','TrainsetContactF']]
    DfLabel = Df[Labelname]
    for col in encodedList:
        print("Encoding ",col)
        lbl = LabelEncoder()
        Df[col].fillna(-999)
        lbl.fit(list(Df[col].values.astype('str')))
        Df[col] = lbl.transform(list(Df[col].values.astype('str')))
    return Df, DfLabel

print ("Loading and preparing datasets...")

FeaturePath = '//file-04/FOLDER-REDIRECTION-MALI-ISLER/yunus.karadag/Desktop/RE_Pro_ml/Predictors/Predictors.csv'
DataPath = '//file-04/FOLDER-REDIRECTION-MALI-ISLER/yunus.karadag/Desktop/RE_Pro_ml/datasets'

Predictors,Df_0131, Label_0131, removedCols, encodedList = TrainPrep(DataPath,FeaturePath,'Df_0131',"LiveProtocolF_3Month","ContactCode")

Df_0430, Label_0430 = TestPrep(DataPath,FeaturePath,"Df_0430",removedCols, encodedList,"LiveProtocolF_3Month")

Df_0731, Label_0731 = TestPrep(DataPath,FeaturePath,"Df_0731",removedCols, encodedList,"LiveProtocolF_3Month")

Df_1031, Label_1031 = TestPrep(DataPath,FeaturePath,"Df_1031",removedCols, encodedList,"LiveProtocolF_3Month")

Df_1231, Label_1231 = TestPrep(DataPath,FeaturePath,"Df_1231",removedCols, encodedList,"LiveProtocolF_3Month")


####TRAINING CLASSIFIERS####


def catboosttrainer(X,y,features,initparam,modelname,modelpath,docpath,cvfold = 5):
    print ("searching for optimal iteration count...")
    trainpool = cat.Pool(X[features],y)
    cvresult = cat.cv(params= initparam, fold_count=cvfold, pool=trainpool,stratified = True)
    initparam['iterations'] = (len(cvresult)) - (initparam['od_wait']+1)   
    del initparam['od_wait'] 
    del initparam['od_type']
    #Update sonrası kontrol et
    print ("optimal iteration count is ", initparam['iterations'])
    print ("fitting model...")
    clf = cat.CatBoostClassifier(** initparam)
    clf.fit(trainpool)
    imp = clf.get_feature_importance(trainpool,fstr_type='FeatureImportance')
    dfimp = pd.DataFrame(imp,columns = ['CatBoostImportance'])
    dfimp.insert(0,column='Feature', value=features) 
    dfimp = dfimp.sort_values(['CatBoostImportance','Feature'], ascending= False)
    xlsxpath = os.path.join(docpath,modelname+".xlsx")
    dfimp.to_excel(xlsxpath)
    print ("pickling model...")
    picklepath = os.path.join(modelpath,modelname)
    with open(picklepath,'wb') as fout:
        pickle.dump(clf, fout)
    return cvresult,clf,initparam,dfimp



print ("Setting paths...")

modelpath = '//file-04/FOLDER-REDIRECTION-MALI-ISLER/yunus.karadag/Desktop/models'
docpath = '//file-04/FOLDER-REDIRECTION-MALI-ISLER/yunus.karadag/Desktop/features'


print ("Training with 0131 dataset...")

CatBoostParam = { 'iterations': 10000, 'od_type': 'Iter', 'od_wait': 100,'loss_function': 'Logloss','eval_metric': 'AUC' }

cvresult_0131,clf_0131,initparam,dfimp_0131 = catboosttrainer(Df_0131,Label_0131,Predictors,CatBoostParam,'CB_Model_0131',modelpath,docpath,cvfold = 5)


print ("Training with 0430 dataset...")

CatBoostParam = { 'iterations': 10000, 'od_type': 'Iter', 'od_wait': 100,'loss_function': 'Logloss','eval_metric': 'AUC' }

cvresult_0430,clf_0430,initparam,dfimp_0430 = catboosttrainer(Df_0430,Label_0430,Predictors,CatBoostParam,'CB_Model_0430',modelpath,docpath,cvfold = 5)


print ("Training with 0731 dataset...")

CatBoostParam = { 'iterations': 10000, 'od_type': 'Iter', 'od_wait': 100,'loss_function': 'Logloss','eval_metric': 'AUC' }

cvresult_0731,clf_0731,initparam,dfimp_0731 = catboosttrainer(Df_0731,Label_0731,Predictors,CatBoostParam,'CB_Model_0731',modelpath,docpath,cvfold = 5)

print ("Predicting 1031 dataset")

proba_0131 = clf_0131.predict_proba(Df_1031[Predictors])[:,1]
proba_0430 = clf_0430.predict_proba(Df_1031[Predictors])[:,1]
proba_0731 = clf_0731.predict_proba(Df_1031[Predictors])[:,1]

Df_1031.insert(len(Df_1031.columns),"Proba_0131",proba_0131)
Df_1031.insert(len(Df_1031.columns),"Proba_0430",proba_0430)
Df_1031.insert(len(Df_1031.columns),"Proba_0731",proba_0731)

for proba in ["Proba_0131","Proba_0430","Proba_0731"]:
    print ("auc for ",proba," is: ",roc_auc_score(Label_1031,Df_1031[proba]))


print ("Searching optimal weights...")

proba_0131_v2 = clf_0131.predict_proba(Df_1231[Predictors])[:,1]
proba_0430_v2 = clf_0430.predict_proba(Df_1231[Predictors])[:,1]
proba_0731_v2 = clf_0731.predict_proba(Df_1231[Predictors])[:,1]

Df_1231.insert(len(Df_1231.columns),"Proba_0131",proba_0131_v2)
Df_1231.insert(len(Df_1231.columns),"Proba_0430",proba_0430_v2)
Df_1231.insert(len(Df_1231.columns),"Proba_0731",proba_0731_v2)



space = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]


optdict = dict()
iteration = 0
for w1 in space:
    iteration +=1
    print (iteration)
    for w2 in space:
        w3 = 1-w1-w2
        if w3 < 0:
            pass
        else:
           weightdict = dict() 
           auc = 0 
           Df_1031["WeightedProba"] = w1*Df_1031["Proba_0131"]+w2*Df_1031["Proba_0430"]+w3*Df_1031["Proba_0731"] 
           weightdict["w1"] = w1
           weightdict["w2"] = w2
           weightdict["w3"] = w3
           auc += roc_auc_score(Label_1031, Df_1031["WeightedProba"])
           print (auc)
           key = str(auc)
           optdict[key] = weightdict

maxlist = list()
for key in optdict.keys():
    maxlist.append(key)
maxauc = max(maxlist)
print (maxauc)
print (optdict[maxauc])    

optdict[maxauc]["w1"]

Df_1231["WeightedProba"] = optdict[maxauc]["w1"]*Df_1231["Proba_0131"]+optdict[maxauc]["w2"]*Df_1231["Proba_0430"]+optdict[maxauc]["w3"]*Df_1231["Proba_0731"]

for proba in ["Proba_0131","Proba_0430","Proba_0731","WeightedProba"]:
    print ("auc for ",proba," is: ",roc_auc_score(Label_1231,Df_1231[proba]))

print ("Write results to excel...")

perffeatures = csvloader('//file-04/FOLDER-REDIRECTION-MALI-ISLER/yunus.karadag/Desktop/features/PerfFeatures.csv')

DfTest = Df_1231[perffeatures]

DfTest.to_excel('//file-04/FOLDER-REDIRECTION-MALI-ISLER/yunus.karadag/Desktop/result/Df_Test_v1.xlsx')
 
